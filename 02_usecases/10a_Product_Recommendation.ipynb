{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommender System with Amazon SageMaker Factorization Machines\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "- Recommender systems were a catalyst for ML's popularity (Amazon, Netflix Prize)\n",
    "- User item matrix factorization is a core methodology\n",
    "- Factorization machines combine linear prediction with a factorized representation of pairwise feature interaction\n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "- SageMaker has a highly scalable factorization machines algorithm built-in\n",
    "- To learn more about the math behind _factorization machines_, [this paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) is a great resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'recommender'\n",
    "prefix = 'sagemaker' + base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset\n",
    "\n",
    "[Amazon Reviews AWS Public Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "- 1 to 5 star ratings\n",
    "- 2M+ Amazon customers\n",
    "- 160K+ digital videos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz to ../../../../../tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12190288</td>\n",
       "      <td>R3FU16928EP5TC</td>\n",
       "      <td>B00AYB1482</td>\n",
       "      <td>668895143</td>\n",
       "      <td>Enlightened: Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I loved it and I wish there was a season 3</td>\n",
       "      <td>I loved it and I wish there was a season 3... ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30549954</td>\n",
       "      <td>R1IZHHS1MH3AQ4</td>\n",
       "      <td>B00KQD28OM</td>\n",
       "      <td>246219280</td>\n",
       "      <td>Vicious</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>52895410</td>\n",
       "      <td>R52R85WC6TIAH</td>\n",
       "      <td>B01489L5LQ</td>\n",
       "      <td>534732318</td>\n",
       "      <td>After Words</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Charming movie</td>\n",
       "      <td>This movie isn't perfect, but it gets a lot of...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>27072354</td>\n",
       "      <td>R7HOOYTVIB0DS</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>239012694</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellant this is what tv should be</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26939022</td>\n",
       "      <td>R1XQ2N5CDOZGNX</td>\n",
       "      <td>B0094LZMT0</td>\n",
       "      <td>535858974</td>\n",
       "      <td>On The Waterfront</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Brilliant film from beginning to end</td>\n",
       "      <td>Brilliant film from beginning to end. All of t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n",
       "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n",
       "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n",
       "3          US     27072354   R7HOOYTVIB0DS  B008LOVIIK       239012694   \n",
       "4          US     26939022  R1XQ2N5CDOZGNX  B0094LZMT0       535858974   \n",
       "\n",
       "                           product_title        product_category  star_rating  \\\n",
       "0                  Enlightened: Season 1  Digital_Video_Download            5   \n",
       "1                                Vicious  Digital_Video_Download            5   \n",
       "2                            After Words  Digital_Video_Download            4   \n",
       "3  Masterpiece: Inspector Lewis Season 5  Digital_Video_Download            5   \n",
       "4                      On The Waterfront  Digital_Video_Download            5   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0              0            0    N                 Y   \n",
       "1              0            0    N                 Y   \n",
       "2             17           18    N                 Y   \n",
       "3              0            0    N                 Y   \n",
       "4              0            0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0         I loved it and I wish there was a season 3   \n",
       "1  As always it seems that the best shows come fr...   \n",
       "2                                     Charming movie   \n",
       "3                                         Five Stars   \n",
       "4               Brilliant film from beginning to end   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n",
       "1  As always it seems that the best shows come fr...  2015-08-31  \n",
       "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  \n",
       "3                excellant this is what tv should be  2015-08-31  \n",
       "4  Brilliant film from beginning to end. All of t...  2015-08-31  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop some fields that won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'product_title', 'star_rating', 'review_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most users don't rate most movies - Check our long tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       1.0\n",
      "0.50       1.0\n",
      "0.75       2.0\n",
      "0.90       4.0\n",
      "0.95       5.0\n",
      "0.96       6.0\n",
      "0.97       7.0\n",
      "0.98       9.0\n",
      "0.99      13.0\n",
      "1.00    2704.0\n",
      "Name: customer_id, dtype: float64\n",
      "products\n",
      " 0.00        1.00\n",
      "0.01        1.00\n",
      "0.02        1.00\n",
      "0.03        1.00\n",
      "0.04        1.00\n",
      "0.05        1.00\n",
      "0.10        1.00\n",
      "0.25        1.00\n",
      "0.50        3.00\n",
      "0.75        9.00\n",
      "0.90       31.00\n",
      "0.95       73.00\n",
      "0.96       95.00\n",
      "0.97      130.00\n",
      "0.98      199.00\n",
      "0.99      386.67\n",
      "1.00    32790.00\n",
      "Name: product_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out customers who haven't rated many movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a sequential index for customers and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>10463</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>489</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44025160</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>32100</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18602179</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-12-23</td>\n",
       "      <td>2237</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14424972</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>32340</td>\n",
       "      <td>140450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id                          product_title  \\\n",
       "0     27072354  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "1     16030865  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "2     44025160  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "3     18602179  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "4     14424972  B008LOVIIK  Masterpiece: Inspector Lewis Season 5   \n",
       "\n",
       "   star_rating review_date   user    item  \n",
       "0            5  2015-08-31  10463  140450  \n",
       "1            5  2014-06-20    489  140450  \n",
       "2            5  2014-05-27  32100  140450  \n",
       "3            5  2014-12-23   2237  140450  \n",
       "4            5  2015-08-31  32340  140450  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0]) + customer_index.shape[0]})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count days since first review (included as a feature to capture trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['review_date'] = pd.to_datetime(reduced_df['review_date'])\n",
    "customer_first_date = reduced_df.groupby('customer_id')['review_date'].min().reset_index()\n",
    "customer_first_date.columns = ['customer_id', 'first_review_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduced_df.merge(customer_first_date)\n",
    "reduced_df['days_since_first'] = (reduced_df['review_date'] - reduced_df['first_review_date']).dt.days\n",
    "reduced_df['days_since_first'] = reduced_df['days_since_first'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Factorization machines expects data to look something like:\n",
    "  - Sparse matrix\n",
    "  - Target variable is that user's rating for a movie\n",
    "  - One-hot encoding for users ($N$ features)\n",
    "  - One-hot encoding for movies ($M$ features)\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|Feature1|Feature2|...|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|20|2.2|...|\n",
    "|5|1|0|...|0|0|1|0|...|0|17|9.1|...|\n",
    "|3|0|1|...|0|1|0|0|...|0|3|11.0|...|\n",
    "|4|0|1|...|0|0|0|1|...|0|15|6.4|...|\n",
    "\n",
    "\n",
    "- Wouldn't want to hold this full matrix in memory\n",
    "  - Create a sparse matrix\n",
    "  - Designed to work efficiently with CPUs. Some parts of training for more dense matrices can be parallelized with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csr_matrix(df, num_users, num_items):\n",
    "    feature_dim = num_users + num_items + 1\n",
    "    data = np.concatenate([np.array([1] * df.shape[0]),\n",
    "                           np.array([1] * df.shape[0]),\n",
    "                           df['days_since_first'].values])\n",
    "    row = np.concatenate([np.arange(df.shape[0])] * 3)\n",
    "    col = np.concatenate([df['user'].values,\n",
    "                          df['item'].values,\n",
    "                          np.array([feature_dim - 1] * df.shape[0])])\n",
    "    return csr_matrix((data, (row, col)), \n",
    "                      shape=(df.shape[0], feature_dim), \n",
    "                      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3_protobuf(csr, label, bucket, prefix, channel, splits):\n",
    "    indices = np.array_split(np.arange(csr.shape[0]), splits)\n",
    "    for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, csr[index, ], label[index])\n",
    "        buf.seek(0)\n",
    "        boto3.client('s3').upload_fileobj(buf, bucket, '{}/{}/data-{}'.format(prefix, channel, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to sparse recordIO-wrapped protobuf that SageMaker factorization machines expects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = to_csr_matrix(train_df, customer_index.shape[0], product_index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_s3_protobuf(train_csr, train_df['star_rating'].values.astype(np.float32), bucket, prefix, channel='train', splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csr = to_csr_matrix(test_df, customer_index.shape[0], product_index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_s3_protobuf(test_csr, test_df['star_rating'].values.astype(np.float32), bucket, prefix, channel='test', splits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "- Create a [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) estimator to run a training jobs and specify:\n",
    "  - Algorithm container image\n",
    "  - IAM role\n",
    "  - Hardware setup\n",
    "  - S3 output location\n",
    "  - Algorithm hyperparameters\n",
    "    - `feature_dim`: $N + M + 1$ (additional feature is `days_since_first` to capture trend)\n",
    "    - `num_factors`: number of factor dimensions (increasing too much can lead to overfitting)\n",
    "    - `epochs`: number of full passes through the dataset\n",
    "- `.fit()` points to training and test data in S3 and begins the training job\n",
    "\n",
    "**Note**: For AWS accounts registered in conjunction with a workshop, default instance limits may prevent the use of `ml.c5.2xlarge` (and other equally powerful instances), and may require a lower value for `train_instance_count` depending on the instance type chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "# # https://github.com/aws/sagemaker-python-sdk/issues/1985\n",
    "# # container = sagemaker.image_uris.retrieve(region_name, \"blazingtext\", \"latest\")\n",
    "\n",
    "# image_uri = ''\n",
    "\n",
    "# if region_name == 'us-west-1':\n",
    "#     image_uri = '632365934929.dkr.ecr.us-west-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'us-west-2':\n",
    "#     image_uri = '433757028032.dkr.ecr.us-west-2.amazonaws.com'\n",
    "    \n",
    "# if region_name =='us-east-1':\n",
    "#     image_uri = '811284229777.dkr.ecr.us-east-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'us-east-2':\n",
    "#     image_uri = '825641698319.dkr.ecr.us-east-2.amazonaws.com'\n",
    "\n",
    "# if region_name =='ap-east-1':\n",
    "#     image_uri = '286214385809.dkr.ecr.ap-east-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'ap-northeast-1':\n",
    "#     image_uri = '501404015308.dkr.ecr.ap-northeast-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'ap-northeast-2':\n",
    "#     image_uri = '306986355934.dkr.ecr.ap-northeast-2.amazonaws.com'\n",
    "\n",
    "# if region_name == 'ap-south-1':\n",
    "#     image_uri = '991648021394.dkr.ecr.ap-south-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'ap-southeast-1':\n",
    "#     image_uri = '475088953585.dkr.ecr.ap-southeast-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'ap-southeast-2':\n",
    "#     image_uri = '544295431143.dkr.ecr.ap-southeast-2.amazonaws.com'\n",
    "\n",
    "# if region_name == 'ca-central-1':\n",
    "#     image_uri = '469771592824.dkr.ecr.ca-central-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'cn-north-1':\n",
    "#     image_uri = '390948362332.dkr.ecr.cn-north-1.amazonaws.com.cn'\n",
    "\n",
    "# if region_name == 'cn-northwest-1':\n",
    "#     image_uri = '387376663083.dkr.ecr.cn-northwest-1.amazonaws.com.cn'\n",
    "\n",
    "# if region_name == 'eu-central-1': \n",
    "#     image_uri = '813361260812.dkr.ecr.eu-central-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'eu-north-1':\n",
    "#     image_uri = '669576153137.dkr.ecr.eu-north-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'eu-west-1':\n",
    "#     image_uri = '685385470294.dkr.ecr.eu-west-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'eu-west-2':\n",
    "#     image_uri = '644912444149.dkr.ecr.eu-west-2.amazonaws.com'\n",
    "\n",
    "# if region_name == 'eu-west-3':\n",
    "#     image_uri = '749696950732.dkr.ecr.eu-west-3.amazonaws.com'\n",
    "\n",
    "# if region_name == 'me-south-1':\n",
    "#     image_uri = '249704162688.dkr.ecr.me-south-1.amazonaws.com'\n",
    "    \n",
    "# if region_name == 'sa-east-1':\n",
    "#     image_uri = '855470959533.dkr.ecr.sa-east-1.amazonaws.com'\n",
    "\n",
    "# if region_name == 'us-gov-west-1':\n",
    "#     image_uri = '226302683700.dkr.ecr.us-gov-west-1.amazonaws.com'\n",
    "    \n",
    "# # https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "# image_uri = '{}/factorization-machines:1'.format(image_uri)\n",
    "\n",
    "# print('Using SageMaker container: {} ({})'.format(image_uri, region_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:1\n"
     ]
    }
   ],
   "source": [
    "# from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# image_uri = get_image_uri(region_name, \"factorization-machines\")\n",
    "# print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role, \n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    base_job_name=base,\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=customer_index.shape[0] + product_index.shape[0] + 1,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=256,\n",
    "    epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(s3_data='s3://{}/{}/train/'.format(bucket, prefix), \n",
    "                            distribution='ShardedByS3Key')\n",
    "\n",
    "test_input = TrainingInput(s3_data='s3://{}/{}/test/'.format(bucket, prefix), \n",
    "                            distribution='ShardedByS3Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-08 21:10:15 Starting - Starting the training job...\n",
      "2020-11-08 21:10:20 Starting - Launching requested ML instances.........\n",
      "2020-11-08 21:11:50 Starting - Preparing the instances for training...............\n",
      "2020-11-08 21:14:34 Downloading - Downloading input data...\n",
      "2020-11-08 21:15:16 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'3', u'feature_dim': u'178730', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'256'}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'3', u'feature_dim': u'178730', u'num_factors': u'256', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 WARNING 139620961433408] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:15:17.857] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:15:17.869] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 16, \"num_examples\": 1, \"num_bytes\": 72000}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] nvidia-smi took: 0.0251278877258 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:17 INFO 139620961433408] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 39.392948150634766, \"sum\": 39.392948150634766, \"min\": 39.392948150634766}}, \"EndTime\": 1604870117.90457, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870117.8533}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1604870117.904744, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870117.9047}\n",
      "\u001b[0m\n",
      "\u001b[34m[21:15:17] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.203343.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[21:15:17] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.203343.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:19 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=6.84764044672\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:19 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=46.8901796875\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:19 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=6.46359130859\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:27 INFO 139620961433408] Iter[0] Batch [500]#011Speed: 68417.91 samples/sec\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:27 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=500 train rmse <loss>=1.42406051474\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:27 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=500 train mse <loss>=2.02794834964\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:27 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=500 train absolute_loss <loss>=1.03810093339\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] Iter[0] Batch [1000]#011Speed: 70479.66 samples/sec\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=1000 train rmse <loss>=1.30794400863\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=1000 train mse <loss>=1.71071752972\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, batch=1000 train absolute_loss <loss>=0.988528015807\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:15:34.585] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 15501, \"num_examples\": 1029, \"num_bytes\": 74059632}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.30752996317\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, train mse <loss>=1.70963460458\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=0.988993563431\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"update.time\": {\"count\": 1, \"max\": 16680.63187599182, \"sum\": 16680.63187599182, \"min\": 16680.63187599182}}, \"EndTime\": 1604870134.58555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870117.904642}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1030, \"sum\": 1030.0, \"min\": 1030}, \"Total Records Seen\": {\"count\": 1, \"max\": 1029606, \"sum\": 1029606.0, \"min\": 1029606}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1604870134.585736, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1604870117.904892}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #throughput_metric: host=algo-1, train throughput=61663.5807415 records/second\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.00093904274\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.00187896729\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:34 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.834227050781\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[11/08/2020 21:15:41 INFO 139620961433408] Iter[1] Batch [500]#011Speed: 72688.85 samples/sec\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:41 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=500 train rmse <loss>=1.0475453634\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:41 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=500 train mse <loss>=1.09735128839\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:41 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=500 train absolute_loss <loss>=0.820659577939\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] Iter[1] Batch [1000]#011Speed: 71265.85 samples/sec\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=1000 train rmse <loss>=1.08773311719\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=1000 train mse <loss>=1.18316333422\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, batch=1000 train absolute_loss <loss>=0.856207611602\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:15:48.919] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 14331, \"num_examples\": 1029, \"num_bytes\": 74059632}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.0920837789\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.19264698013\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.858851410416\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14333.210945129395, \"sum\": 14333.210945129395, \"min\": 14333.210945129395}}, \"EndTime\": 1604870148.919843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870134.585611}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2059, \"sum\": 2059.0, \"min\": 2059}, \"Total Records Seen\": {\"count\": 1, \"max\": 2058212, \"sum\": 2058212.0, \"min\": 2058212}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1604870148.920004, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1604870134.586606}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #throughput_metric: host=algo-1, train throughput=71762.4194803 records/second\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=0.942474918588\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=0.888258972168\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:48 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.748994567871\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:55 INFO 139620961433408] Iter[2] Batch [500]#011Speed: 74698.17 samples/sec\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:55 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=500 train rmse <loss>=1.00437658676\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:55 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=500 train mse <loss>=1.00877232802\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:15:55 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=500 train absolute_loss <loss>=0.781038339459\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] Iter[2] Batch [1000]#011Speed: 71792.47 samples/sec\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=1000 train rmse <loss>=1.0440308942\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=1000 train mse <loss>=1.09000050804\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, batch=1000 train absolute_loss <loss>=0.816252434211\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:16:02.997] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 14074, \"num_examples\": 1029, \"num_bytes\": 74059632}\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.04820445047\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.09873256998\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.818623317707\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, train rmse <loss>=1.04820445047\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, train mse <loss>=1.09873256998\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #quality_metric: host=algo-1, train absolute_loss <loss>=0.818623317707\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 14076.62296295166, \"sum\": 14076.62296295166, \"min\": 14076.62296295166}}, \"EndTime\": 1604870162.997631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870148.919908}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1029, \"sum\": 1029.0, \"min\": 1029}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3088, \"sum\": 3088.0, \"min\": 3088}, \"Total Records Seen\": {\"count\": 1, \"max\": 3086818, \"sum\": 3086818.0, \"min\": 3086818}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1028606, \"sum\": 1028606.0, \"min\": 1028606}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1604870162.997783, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1604870148.920983}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] #throughput_metric: host=algo-1, train throughput=73070.496295 records/second\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 WARNING 139620961433408] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:02 INFO 139620961433408] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 22.927045822143555, \"sum\": 22.927045822143555, \"min\": 22.927045822143555}}, \"EndTime\": 1604870163.020927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870162.997693}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:03 INFO 139620961433408] Saved checkpoint to \"/tmp/tmpBCW3yW/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:16:04.031] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 46173, \"num_examples\": 1, \"num_bytes\": 72000}\u001b[0m\n",
      "\u001b[34m[2020-11-08 21:16:24.222] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 20190, \"num_examples\": 141, \"num_bytes\": 10104768}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Total Batches Seen\": {\"count\": 1, \"max\": 141, \"sum\": 141.0, \"min\": 141}, \"Total Records Seen\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 140344, \"sum\": 140344.0, \"min\": 140344}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1604870184.222612, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870164.031384}\n",
      "\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:24 INFO 139620961433408] #test_score (algo-1) : ('rmse', 1.228081429613048)\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:24 INFO 139620961433408] #test_score (algo-1) : ('mse', 1.5081839977604277)\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:24 INFO 139620961433408] #test_score (algo-1) : ('absolute_loss', 0.9666407492239214)\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:24 INFO 139620961433408] #quality_metric: host=algo-1, test rmse <loss>=1.22808142961\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:24 INFO 139620961433408] #quality_metric: host=algo-1, test mse <loss>=1.50818399776\u001b[0m\n",
      "\u001b[34m[11/08/2020 21:16:24 INFO 139620961433408] #quality_metric: host=algo-1, test absolute_loss <loss>=0.966640749224\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 66409.75904464722, \"sum\": 66409.75904464722, \"min\": 66409.75904464722}, \"setuptime\": {\"count\": 1, \"max\": 36.242008209228516, \"sum\": 36.242008209228516, \"min\": 36.242008209228516}}, \"EndTime\": 1604870184.223661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1604870163.020992}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-08 21:16:27 Uploading - Uploading generated training model\n",
      "2020-11-08 21:16:54 Completed - Training job completed\n",
      "Training seconds: 140\n",
      "Billable seconds: 140\n"
     ]
    }
   ],
   "source": [
    "fm.fit({'train': train_input, \n",
    "        'test': test_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host the Endpoint\n",
    "\n",
    "Deploy trained model to a real-time production endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(df):\n",
    "    feature_dim = customer_index.shape[0] + product_index.shape[0] + 1\n",
    "    js = {'instances': []}\n",
    "    for index, data in df.iterrows():\n",
    "        js['instances'].append({'data': {'features': {'values': [1, 1, data['days_since_first']],\n",
    "                                                      'keys': [data['user'], data['item'], feature_dim - 1],\n",
    "                                                      'shape': [feature_dim]}}})\n",
    "    return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----"
     ]
    }
   ],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.m4.xlarge', \n",
    "                         initial_instance_count=1,\n",
    "                         serializer=fm_serializer,\n",
    "                         deserializer=sagemaker.deserializers.JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the predictor request handler\n",
    "Serialize the request data to match what the model is expecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fm_predictor.content_type = 'application/json'\n",
    "#fm_predictor.serializer = fm_serializer\n",
    "#fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a single customer from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_customer = test_df.iloc[[20]]\n",
    "test_df.iloc[[20]] # peek at the data to confirm it's the one we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass `test_customer` to predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm_predictor.predict(test_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a dataframe for an arbitrary customer and movie pair and test it out!\n",
    "\n",
    "Our `fm_serializer` requires 3 inputs to perform a prediction:\n",
    " - `user` id for a customer (type = num)\n",
    " - `item` id for a movie (type = num)\n",
    " - `days_since_first` review (type = double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_customer = test_customer # make a copy of the test_customer we pulled out before to modify\n",
    "desired_user_id = 65884 # person who rated Dexter with 5 stars\n",
    "desired_item_id = 140461 # Code for True Blood: Season 1\n",
    "desired_review_days = 28.0 # arbitrary number of days since first review\n",
    "\n",
    "#fake_customer_data = {'user' : desired_user_id, 'item' : desired_item_id, 'days_since_first' : desired_review_days}\n",
    "#fake_customer = pd.DataFrame(fake_customer_data, index=[0])\n",
    "fake_customer['user'] = desired_user_id\n",
    "fake_customer['item'] = desired_item_id\n",
    "fake_customer['days_since_first'] = desired_review_days\n",
    "\n",
    "# print the details for this fake customer\n",
    "fake_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.predict(fake_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
