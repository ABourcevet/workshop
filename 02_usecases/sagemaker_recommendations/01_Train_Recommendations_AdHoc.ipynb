{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sT8AyHRMNh41"
   },
   "source": [
    "# TensorFlow Recommenders: Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8f-reQ11gbLB"
   },
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TFRS. We can use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import TensorFlow Recommender System (TFRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker==2.9.2\n",
    "!pip install -q sagemaker-experiments==0.1.24\n",
    "!pip install -q tensorflow==2.3.0\n",
    "!pip install -q tensorflow-recommenders==0.2.0\n",
    "!pip install -q tensorflow-datasets==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3oYt3R6Nr9l"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCxQ1CZcO2wh"
   },
   "source": [
    "# Load Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-mxBYjdO5m7"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset movielens: could not find data in ./tensorflow_datasets/. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-77fbf55989b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./tensorflow_datasets/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     split='train')\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    356\u001b[0m   \u001b[0mas_dataset_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"read_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mas_dataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    515\u001b[0m            \u001b[0;34m\"dataset_builder.download_and_prepare(), or pass download=True to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m            \u001b[0;34m\"tfds.load() before trying to access the tf.data.Dataset object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m           ) % (self.name, self._data_dir_root))\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;31m# By default, return all splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset movielens: could not find data in ./tensorflow_datasets/. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object."
     ]
    }
   ],
   "source": [
    "ratings = tfds.load('movielens/100k-ratings',                     \n",
    "                    download=False,\n",
    "                    data_dir='./tensorflow_datasets/',\n",
    "                    split='train')\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"]\n",
    "})\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = tfds.load('movielens/100k-movies', \n",
    "                   download=False,                   \n",
    "                   data_dir='./tensorflow_datasets/',\n",
    "                   split='train')\n",
    "print('Movies BEFORE', movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "print('Movies AFTER', movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0HSfmSNCWm"
   },
   "source": [
    "# Create Vocabularies\n",
    "Build vocabularies to convert user ids and movie titles into integer indices for embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I1VTEjHzpfX"
   },
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_ids_vocabulary.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lrch6rVBOB9Q"
   },
   "source": [
    "# Create the Model\n",
    "\n",
    "We can define a TFRS model by inheriting from `tfrs.Model` and implementing the `compute_loss` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5dNbDZwOIHR"
   },
   "outputs": [],
   "source": [
    "class MovieLensModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_embedding: tf.keras.Model,\n",
    "      movie_embeddings: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie representations.\n",
    "    self.user_embeddings = user_embeddings\n",
    "    self.movie_embeddings = movie_embeddings\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed using the retrieval task\n",
    "    user_embeddings = self.user_embeddings(features['user_id'])\n",
    "    movie_embeddings = self.movie_embeddings(features['movie_title'])\n",
    "\n",
    "    return self.task(user_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdwtgUCEOI8y"
   },
   "source": [
    "# Define User and Movie Models\n",
    "Define the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvtnUN6aUY4U"
   },
   "outputs": [],
   "source": [
    "user_embeddings = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 128)\n",
    "])\n",
    "\n",
    "movie_embeddings = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 128)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Retrieval Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    movies.batch(128).map(movie_embeddings)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMV0HpzmJGWk"
   },
   "source": [
    "# Train the Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2tQDhqkOKf1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MovieLensModel(user_embeddings, movie_embeddings, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.\n",
    "model.fit(ratings.batch(4096), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions\n",
    "Use brute-force search to set up retrieval using the trained representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_embeddings)\n",
    "index.index(movies.batch(100).map(model.movie_embeddings), movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_model_dir_bruteforce_model = './exported_models/bruteforce_model/'\n",
    "\n",
    "tensorflow_saved_model_path_bruteforce_model = os.path.join(local_model_dir_bruteforce_model, 'tensorflow/saved_model/0')\n",
    "\n",
    "os.makedirs(tensorflow_saved_model_path_bruteforce_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "user_id = \"42\"\n",
    "\n",
    "_, titles = index(np.array([user_id]))\n",
    "\n",
    "print(f\"Top {k} recommendations for user {user_id}: {titles[0, :k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Compiled model {}'.format(index))          \n",
    "print(index.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save(tensorflow_saved_model_path_bruteforce_model, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir $tensorflow_saved_model_path_bruteforce_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following works!\n",
    "\n",
    "\n",
    "```\n",
    "!saved_model_cli run --input_exprs 'input_1=[str(42)]' --tag_set serve --signature_def serving_default --dir $tensorflow_saved_model_path_bruteforce_model\n",
    "```\n",
    "\n",
    "```\n",
    "Result for output key output_1:\n",
    "[[3.9453535 2.9104383 2.7285933 2.6687438 2.5107827 2.4038887 2.3974428\n",
    "  2.386409  2.321732  2.3211298]]\n",
    "Result for output key output_2:\n",
    "[[b'Rent-a-Kid (1995)' b'Last Dance (1996)'\n",
    "  b'Adventures of Pinocchio, The (1996)'\n",
    "  b'Winnie the Pooh and the Blustery Day (1968)'\n",
    "  b'Aristocats, The (1970)' b'Celtic Pride (1996)'\n",
    "  b'Conan the Barbarian (1981)' b'House Arrest (1996)'\n",
    "  b'Just Cause (1995)' b'Johnny 100 Pesos (1993)']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!saved_model_cli run --input_exprs 'input_1=[\"$user_id\"]' --tag_set serve --signature_def serving_default --dir $tensorflow_saved_model_path_bruteforce_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pprint\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the directory you want to start from\n",
    "#for dirName, subdirList, fileList in os.walk('/etc/ssl/certs/'):\n",
    "#    print('Found directory: %s' % dirName)\n",
    "#    for fname in fileList:\n",
    "#        print('\\t%s' % fname)        \n",
    "        \n",
    "from typing import Dict, Text\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_embedding: tf.keras.Model,\n",
    "      movie_embeddings: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie representations.\n",
    "    self.user_embeddings = user_embeddings\n",
    "    self.movie_embeddings = movie_embeddings\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed using the retrieval task\n",
    "    user_embeddings = self.user_embeddings(features['user_id'])\n",
    "    movie_embeddings = self.movie_embeddings(features['movie_title'])\n",
    "\n",
    "    return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "     \n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Args:\") \n",
    "    print(args)\n",
    "    \n",
    "    env_var = os.environ \n",
    "    print(\"Environment Variables:\") \n",
    "    pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "    local_model_dir = './model' # os.environ['SM_MODEL_DIR']\n",
    "    output_dir = './output' # args.output_dir\n",
    "    print('output_dir {}'.format(output_dir))    \n",
    "\n",
    "    train_data = './tensorflow_datasets/' # args.train_data\n",
    "    print('train_data {}'.format(train_data))\n",
    "    epochs = 1 # args.epochs\n",
    "    print('epochs {}'.format(epochs))    \n",
    "    learning_rate = 0.5 # args.learning_rate\n",
    "    print('learning_rate {}'.format(learning_rate))    \n",
    "    enable_tensorboard = False # args.enable_tensorboard\n",
    "    print('enable_tensorboard {}'.format(enable_tensorboard))       \n",
    "    dataset_variant = '100k' # args.dataset_variant\n",
    "    print('dataset_variant {}'.format(dataset_variant))\n",
    "    embedding_dimension = int(256) # int(args.embedding_dimension)\n",
    "    print('embedding_dimension {}'.format(embedding_dimension))       \n",
    "\n",
    "    # Load the ratings data to use for training\n",
    "    ratings = tfds.load('movielens/{}-ratings'.format(dataset_variant), \n",
    "                        download=False,\n",
    "                        data_dir=train_data,\n",
    "                        split='train')\n",
    "    print('Ratings raw', ratings)\n",
    "\n",
    "    # Transform the ratings data specific to our training task\n",
    "    ratings = ratings.map(lambda x: {\n",
    "        'movie_title': x['movie_title'],\n",
    "        'user_id': x['user_id']\n",
    "    })\n",
    "    print('Ratings transformed', ratings)    \n",
    "\n",
    "    # Load the movies data to use for training\n",
    "    movies = tfds.load('movielens/{}-movies'.format(dataset_variant),\n",
    "                       download=False,\n",
    "                       data_dir=train_data,\n",
    "                       split='train')\n",
    "    print('Movies raw', movies)\n",
    "    \n",
    "    # Transform the movies data specific to our training task\n",
    "    movies = movies.map(lambda x: x['movie_title'])\n",
    "    print('Movies transformed', movies)\n",
    "\n",
    "    # Create the user vocabulary and user embeddings\n",
    "    user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "    user_ids_vocabulary.adapt(ratings.map(lambda x: x['user_id']))\n",
    "\n",
    "    user_embeddings = tf.keras.Sequential([\n",
    "        user_ids_vocabulary,\n",
    "        tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(),\n",
    "                                  embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Create the movie vocabulary and movie embeddings\n",
    "    movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "    movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "    movie_embeddings = tf.keras.Sequential([\n",
    "        movie_titles_vocabulary,\n",
    "        tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(),\n",
    "                                  embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Specify the task and the top-k metric to optimize during model training\n",
    "    task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "        movies.batch(128).map(movie_embeddings)\n",
    "    ))\n",
    "\n",
    "    # Define the optimizer and hyper-parameters\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate)\n",
    "    print('Optimizer:  {}'.format(optimizer))\n",
    "\n",
    "    # Setup the callbacks to use during training\n",
    "    callbacks = []\n",
    "\n",
    "    # Setup the Tensorboard callback if Tensorboard is enabled\n",
    "    if enable_tensorboard: \n",
    "        # Tensorboard Logs \n",
    "        tensorboard_logs_path = os.path.join(local_model_dir, 'tensorboard/')\n",
    "        os.makedirs(tensorboard_logs_path, exist_ok=True)\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n",
    "        print('Adding Tensorboard callback {}'.format(tensorboard_callback))\n",
    "        callbacks.append(tensorboard_callback)\n",
    "    print('Callbacks: {}'.format(callbacks))\n",
    "\n",
    "    # Create a custom Keras model with the user embeddings, movie embeddings, and optimization task\n",
    "    model = MovieLensModel(user_embeddings, movie_embeddings, task)\n",
    "    \n",
    "    # Compile the model and prepare for training\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(ratings.batch(4096), epochs=epochs)\n",
    "\n",
    "    # Make some sample predictions to test our model\n",
    "    # Note:  This is required to save and server our model with TensorFlow Serving\n",
    "    #        See https://github.com/tensorflow/tensorflow/issues/31057 for more  details.\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(query_model=model.user_embeddings)\n",
    "    index.index(movies.batch(100).map(model.movie_embeddings), movies)\n",
    "\n",
    "    user_id = '42'\n",
    "    _, titles = index(np.array([user_id]))\n",
    "\n",
    "    k = 10\n",
    "    print(f'Top {k} recommendations for user {user_id}: {titles[0, :k]}')\n",
    "\n",
    "    # Print a summary of our recommender model\n",
    "    print('Trained index {}'.format(index))\n",
    "    print(index.summary())\n",
    "\n",
    "    # Save the TensorFlow SavedModel for Serving Predictions\n",
    "    # SavedModel Output\n",
    "    tensorflow_saved_model_path = os.path.join(local_model_dir,\n",
    "                                               'tensorflow/saved_model/0')\n",
    "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n",
    "    \n",
    "    print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))\n",
    "    index.save(tensorflow_saved_model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"42\"\n",
    "\n",
    "!saved_model_cli run --input_exprs 'input_1=np.array([\"$user_id\"])' --tag_set serve --signature_def serving_default --dir ./model/tensorflow/saved_model/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "quickstart.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
