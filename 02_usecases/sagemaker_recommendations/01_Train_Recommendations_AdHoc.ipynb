{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sT8AyHRMNh41"
   },
   "source": [
    "# TensorFlow Recommenders: Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8f-reQ11gbLB"
   },
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TFRS. We can use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import TensorFlow Recommender System (TFRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.3.0\n",
    "!pip install -q tensorflow-recommenders==0.2.0\n",
    "!pip install -q tensorflow-datasets==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3oYt3R6Nr9l"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCxQ1CZcO2wh"
   },
   "source": [
    "# Load Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-mxBYjdO5m7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset movielens/25m-ratings/0.1.0 (download: 249.84 MiB, generated: 3.89 GiB, total: 4.13 GiB) to ./tensorflow_datasets/movielens/25m-ratings/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3949757e894e38a78638060697c0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f6164cf9b84810853e9a8477af76f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a4ea78167844518a742b2e020b9303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b916dc660cd42f79282b4e2465865a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0a77a7f6306f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./tensorflow_datasets/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     split='train')\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;31m# Update the info object with the splits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, max_examples_per_split)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                      hash_salt=split_generator.name)\n\u001b[1;32m   1038\u001b[0m     for key, record in utils.tqdm(generator, unit=\" examples\",\n\u001b[0;32m-> 1039\u001b[0;31m                                   total=split_info.num_examples, leave=False):\n\u001b[0m\u001b[1;32m   1040\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/structured/movielens.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, dir_path)\u001b[0m\n\u001b[1;32m    440\u001b[0m   ) -> Iterator[Tuple[int, Dict[str, Any]]]:\n\u001b[1;32m    441\u001b[0m     \u001b[0;34m\"\"\"Yields examples by calling the corresponding parsing function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_datasets/structured/movielens_parsing.py\u001b[0m in \u001b[0;36mparse_current_ratings_data\u001b[0;34m(dir_path)\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0;34m'user_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0;34m'user_rating'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m           \u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m       }\n\u001b[1;32m     58\u001b[0m       \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_info_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# /home/ec2-user/tensorflow_datasets/train/movielens/100k-ratings/0.1.0/\n",
    "\n",
    "# /opt/ml/input/data/train/          movielens/100k-ratings/0.1.0\n",
    "\n",
    "ratings = tfds.load('movielens/100k-ratings',                     \n",
    "                    download=True,\n",
    "                    data_dir='./tensorflow_datasets/',\n",
    "                    split='train')\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings = tf.data.TFRecordDataset(\n",
    "#     './data/movielens/100k-ratings/0.1.0/movielens-train.tfrecord-00000-of-00001', \n",
    "# #    './data/movielens/100k-ratings/0.1.0/', \n",
    "#     compression_type=None, \n",
    "#     buffer_size=None, \n",
    "#     num_parallel_reads=None\n",
    "# )\n",
    "\n",
    "# list(ratings.batch(10).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: {movie_title: (), user_id: ()}, types: {movie_title: tf.string, user_id: tf.string}>\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"]\n",
    "})\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies BEFORE <PrefetchDataset shapes: {movie_genres: (None,), movie_id: (), movie_title: ()}, types: {movie_genres: tf.int64, movie_id: tf.string, movie_title: tf.string}>\n"
     ]
    }
   ],
   "source": [
    "movies = tfds.load('movielens/100k-movies', \n",
    "                   download=True,                   \n",
    "                   data_dir='./tensorflow_datasets/',\n",
    "                   split='train')\n",
    "print('Movies BEFORE', movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:  Shuffle:  https://github.com/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "# train = shuffled.take(80_000)\n",
    "# test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies = tf.data.TFRecordDataset(\n",
    "#     './data/movielens/100k-movies/0.1.0/movielens-train.tfrecord-00000-of-00001', \n",
    "#     compression_type=None, \n",
    "#     buffer_size=None, \n",
    "#     num_parallel_reads=None\n",
    "# )\n",
    "\n",
    "# list(movies.batch(10).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies AFTER <MapDataset shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "print('Movies AFTER', movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(ratings))\n",
    "#print(type(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0HSfmSNCWm"
   },
   "source": [
    "# Create Vocabularies\n",
    "Build vocabularies to convert user ids and movie titles into integer indices for embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I1VTEjHzpfX"
   },
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(user_ids_vocabulary.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lrch6rVBOB9Q"
   },
   "source": [
    "# Create the Model\n",
    "\n",
    "We can define a TFRS model by inheriting from `tfrs.Model` and implementing the `compute_loss` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5dNbDZwOIHR"
   },
   "outputs": [],
   "source": [
    "class MovieLensModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      movie_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie representations.\n",
    "    self.user_model = user_model\n",
    "    self.movie_model = movie_model\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "#    https://github.com/tensorflow/tensorflow/issues/31057\n",
    "#     @tf.function(input_signature=[tf.TensorSpec([1], tf.float32)])\n",
    "#     def call(self, x, training=True, mask=None):\n",
    "#         return self.d(x)\n",
    "    \n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed.\n",
    "\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    return self.task(user_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdwtgUCEOI8y"
   },
   "source": [
    "# Define User and Movie Models\n",
    "Define the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvtnUN6aUY4U"
   },
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 128)\n",
    "])\n",
    "\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 128)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Retrieval Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    movies.batch(128).map(movie_model)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMV0HpzmJGWk"
   },
   "source": [
    "# Train the Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2tQDhqkOKf1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 6s 256ms/step - factorized_top_k/top_1_categorical_accuracy: 8.0000e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0012 - factorized_top_k/top_10_categorical_accuracy: 0.0035 - factorized_top_k/top_50_categorical_accuracy: 0.0380 - factorized_top_k/top_100_categorical_accuracy: 0.0876 - loss: 35698.4820 - regularization_loss: 0.0000e+00 - total_loss: 35698.4820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f14d82d9a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train.\n",
    "model.fit(ratings.batch(4096), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions\n",
    "Use brute-force search to set up retrieval using the trained representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7f1534e632b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index(movies.batch(100).map(model.movie_model), movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_model_dir_bruteforce_model = './exported_models/bruteforce_model/'\n",
    "\n",
    "tensorflow_saved_model_path_bruteforce_model = os.path.join(local_model_dir_bruteforce_model, 'tensorflow/saved_model/0')\n",
    "\n",
    "os.makedirs(tensorflow_saved_model_path_bruteforce_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommendations for user 42: [b'Just Cause (1995)' b'House Arrest (1996)' b'Aristocats, The (1970)'\n",
      " b'Far From Home: The Adventures of Yellow Dog (1995)' b'Nell (1994)']\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "user_id = \"42\"\n",
    "\n",
    "_, titles = index(np.array([user_id]))\n",
    "\n",
    "print(f\"Top {k} recommendations for user {user_id}: {titles[0, :k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled model <tensorflow_recommenders.layers.factorized_top_k.BruteForce object at 0x7f1534e632b0>\n",
      "Model: \"brute_force\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 128)               120832    \n",
      "=================================================================\n",
      "Total params: 337,810\n",
      "Trainable params: 120,832\n",
      "Non-trainable params: 216,978\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Compiled model {}'.format(index))          \n",
    "print(index.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./exported_models/bruteforce_model/tensorflow/saved_model/0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./exported_models/bruteforce_model/tensorflow/saved_model/0/assets\n"
     ]
    }
   ],
   "source": [
    "index.save(tensorflow_saved_model_path_bruteforce_model, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir $tensorflow_saved_model_path_bruteforce_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following works!\n",
    "\n",
    "\n",
    "```\n",
    "!saved_model_cli run --input_exprs 'input_1=[str(42)]' --tag_set serve --signature_def serving_default --dir $tensorflow_saved_model_path_bruteforce_model\n",
    "```\n",
    "\n",
    "```\n",
    "Result for output key output_1:\n",
    "[[3.9453535 2.9104383 2.7285933 2.6687438 2.5107827 2.4038887 2.3974428\n",
    "  2.386409  2.321732  2.3211298]]\n",
    "Result for output key output_2:\n",
    "[[b'Rent-a-Kid (1995)' b'Last Dance (1996)'\n",
    "  b'Adventures of Pinocchio, The (1996)'\n",
    "  b'Winnie the Pooh and the Blustery Day (1968)'\n",
    "  b'Aristocats, The (1970)' b'Celtic Pride (1996)'\n",
    "  b'Conan the Barbarian (1981)' b'House Arrest (1996)'\n",
    "  b'Just Cause (1995)' b'Johnny 100 Pesos (1993)']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!saved_model_cli run --input_exprs 'input_1=[\"$user_id\"]' --tag_set serve --signature_def serving_default --dir $tensorflow_saved_model_path_bruteforce_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pprint\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.3.0.1.0.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow-recommenders==0.2.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow-datasets==4.0.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'matplotlib==3.2.1'])\n",
    "#subprocess.check_call([sys.executable, '-m', 'ls', '-al', '/etc/ssl/certs/'])\n",
    "#subprocess.check_call([sys.executable, '-m', 'ln', '-s', '/etc/ssl/certs/ca-bundle.crt', '/etc/ssl/certs/ca-certificates.crt'])\n",
    "\n",
    "# Set the directory you want to start from\n",
    "#for dirName, subdirList, fileList in os.walk('/etc/ssl/certs/'):\n",
    "#    print('Found directory: %s' % dirName)\n",
    "#    for fname in fileList:\n",
    "#        print('\\t%s' % fname)        \n",
    "        \n",
    "from typing import Dict, Text\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "import numpy as np\n",
    "\n",
    "class MovieLensModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      movie_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie representations.\n",
    "    self.user_model = user_model\n",
    "    self.movie_model = movie_model\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed.\n",
    "\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    return self.task(user_embeddings, movie_embeddings)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "#     parser.add_argument('--train_data', \n",
    "#                         type=str, \n",
    "#                         default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "#     parser.add_argument('--validation_data', \n",
    "#                         type=str, \n",
    "#                         default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "#     parser.add_argument('--test_data',\n",
    "#                         type=str,\n",
    "#                         default=os.environ['SM_CHANNEL_TEST'])\n",
    "#     parser.add_argument('--output_dir',\n",
    "#                         type=str,\n",
    "#                         default=os.environ['SM_OUTPUT_DIR'])\n",
    "#     parser.add_argument('--hosts', \n",
    "#                         type=list, \n",
    "#                         default=json.loads(os.environ['SM_HOSTS']))\n",
    "#     parser.add_argument('--current_host', \n",
    "#                         type=str, \n",
    "#                         default=os.environ['SM_CURRENT_HOST'])    \n",
    "#     parser.add_argument('--num_gpus', \n",
    "#                         type=int, \n",
    "#                         default=os.environ['SM_NUM_GPUS'])\n",
    "#     parser.add_argument('--use_xla',\n",
    "#                         type=eval,\n",
    "#                         default=False)\n",
    "#     parser.add_argument('--use_amp',\n",
    "#                         type=eval,\n",
    "#                         default=False)\n",
    "#     parser.add_argument('--epochs',\n",
    "#                         type=int,\n",
    "#                         default=100)\n",
    "#     parser.add_argument('--learning_rate',\n",
    "#                         type=float,\n",
    "#                         default=0.5)\n",
    "#     parser.add_argument('--enable_tensorboard',\n",
    "#                         type=eval,\n",
    "#                         default=False)        \n",
    "#     parser.add_argument('--output_data_dir', # This is unused\n",
    "#                         type=str,\n",
    "#                         default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    \n",
    "    # This points to the S3 location - this should not be used by our code\n",
    "    # We should use /opt/ml/model/ instead\n",
    "    # parser.add_argument('--model_dir', \n",
    "    #                     type=str, \n",
    "    #                     default=os.environ['SM_MODEL_DIR'])\n",
    "     \n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Args:\") \n",
    "    print(args)\n",
    "    \n",
    "    env_var = os.environ \n",
    "    print(\"Environment Variables:\") \n",
    "    pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "#    print('SM_TRAINING_ENV {}'.format(env_var['SM_TRAINING_ENV']))\n",
    "#    sm_training_env_json = json.loads(env_var['SM_TRAINING_ENV'])\n",
    "#    is_master = sm_training_env_json['is_master']\n",
    "#    print('is_master {}'.format(is_master))\n",
    "    \n",
    "#     train_data = args.train_data\n",
    "#     print('train_data {}'.format(train_data))\n",
    "#     validation_data = args.validation_data\n",
    "#     print('validation_data {}'.format(validation_data))\n",
    "#     test_data = args.test_data\n",
    "#     print('test_data {}'.format(test_data))    \n",
    "\n",
    "    local_model_dir = './model' # os.environ['SM_MODEL_DIR']\n",
    "    output_dir = './output' # args.output_dir\n",
    "    print('output_dir {}'.format(output_dir))    \n",
    "#    hosts = args.hosts\n",
    "#    print('hosts {}'.format(hosts))    \n",
    "#    current_host = args.current_host\n",
    "#    print('current_host {}'.format(current_host))    \n",
    "#    num_gpus = args.num_gpus\n",
    "#    print('num_gpus {}'.format(num_gpus))\n",
    "#    job_name = 'job' # os.environ['SAGEMAKER_JOB_NAME']\n",
    "#    print('job_name {}'.format(job_name))    \n",
    "\n",
    "    use_xla = True # args.use_xla\n",
    "    print('use_xla {}'.format(use_xla))    \n",
    "    use_amp = True # args.use_amp\n",
    "    print('use_amp {}'.format(use_amp))    \n",
    "    epochs = 1 # args.epochs\n",
    "    print('epochs {}'.format(epochs))    \n",
    "    learning_rate = 0.5 # args.learning_rate\n",
    "    print('learning_rate {}'.format(learning_rate))    \n",
    "    enable_tensorboard = False # args.enable_tensorboard\n",
    "    print('enable_tensorboard {}'.format(enable_tensorboard))       \n",
    "\n",
    "    # Determine if PipeMode is enabled \n",
    "#     pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\n",
    "#     pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\n",
    "#     print('Using pipe_mode: {}'.format(pipe_mode))\n",
    " \n",
    "    # SavedModel Output\n",
    "    tensorflow_saved_model_path = os.path.join(local_model_dir, 'tensorflow/saved_model/0')\n",
    "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n",
    "\n",
    "    # Tensorboard Logs \n",
    "    tensorboard_logs_path = os.path.join(local_model_dir, 'tensorboard/')\n",
    "    os.makedirs(tensorboard_logs_path, exist_ok=True)\n",
    "\n",
    "    # Commented out due to incompatibility with transformers library (possibly)\n",
    "    # Set the global precision mixed_precision policy to \"mixed_float16\"    \n",
    "#    mixed_precision_policy = 'mixed_float16'\n",
    "#    print('Mixed precision policy {}'.format(mixed_precision_policy))\n",
    "#    policy = mixed_precision.Policy(mixed_precision_policy)\n",
    "#    mixed_precision.set_policy(policy)    \n",
    "    \n",
    "    from typing import Dict, Text\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow_recommenders as tfrs\n",
    "\n",
    "    distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    with distributed_strategy.scope():\n",
    "        tf.config.optimizer.set_jit(use_xla)\n",
    "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\n",
    "\n",
    "        for dirName, subdirList, fileList in os.walk('/opt/ml/input/data/train/'):\n",
    "            print('Found directory BEFORE: %s' % dirName)\n",
    "            for fname in fileList:\n",
    "                print('\\t%s' % fname)         \n",
    "\n",
    "        ratings = tfds.load('movielens/100k-ratings', \n",
    "                            download=True,                            \n",
    "                            data_dir='./tensorflow_datasets/',\n",
    "                            split=\"train\")\n",
    "        print('Ratings BEFORE', ratings)\n",
    "        \n",
    "        for dirName, subdirList, fileList in os.walk('/opt/ml/input/data/train/'):\n",
    "            print('Found directory AFTER: %s' % dirName)\n",
    "            for fname in fileList:\n",
    "                print('\\t%s' % fname)         \n",
    "\n",
    "        movies = tfds.load('movielens/100k-movies',                           \n",
    "                           download=True,                           \n",
    "                           data_dir='./tensorflow_datasets/',\n",
    "                           split=\"train\")\n",
    "        print('Movies BEFORE', movies)\n",
    "\n",
    "        ratings = ratings.map(lambda x: {\n",
    "            \"movie_title\": x[\"movie_title\"],\n",
    "            \"user_id\": x[\"user_id\"]\n",
    "        })\n",
    "        print('Ratings BEFORE', ratings)\n",
    "\n",
    "        movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "        print('Movies AFTER', movies)\n",
    "\n",
    "        user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "        user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "        movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "        movie_titles_vocabulary.adapt(movies)\n",
    "\n",
    "        user_model = tf.keras.Sequential([\n",
    "            user_ids_vocabulary,\n",
    "            tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 128)\n",
    "        ])\n",
    "\n",
    "        movie_model = tf.keras.Sequential([\n",
    "            movie_titles_vocabulary,\n",
    "            tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 128)\n",
    "        ])\n",
    "\n",
    "        task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "            movies.batch(128).map(movie_model)\n",
    "          )\n",
    "        )        \n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adagrad(learning_rate)\n",
    "        print('** use_amp {}'.format(use_amp))        \n",
    "        if use_amp:\n",
    "            # loss scaling is currently required when using mixed precision\n",
    "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\n",
    "\n",
    "        callbacks = []\n",
    "        \n",
    "        if enable_tensorboard:            \n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "                                                        log_dir=tensorboard_logs_path)\n",
    "            print('*** TENSORBOARD CALLBACK {} ***'.format(tensorboard_callback))\n",
    "            callbacks.append(tensorboard_callback)\n",
    "  \n",
    "        print('*** OPTIMIZER {} ***'.format(optimizer))\n",
    "        \n",
    "        model = MovieLensModel(user_model, movie_model, task)          \n",
    "        model.compile(optimizer=optimizer)\n",
    "        \n",
    "        model.fit(ratings.batch(4096), epochs=epochs)\n",
    "\n",
    "        index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "        index.index(movies.batch(100).map(model.movie_model), movies)\n",
    "\n",
    "        # Must make a prediction before we can save the model.\n",
    "        _, titles = index(np.array([\"42\"]))\n",
    "        print(f\"Top 10 recommendations for user 42: {titles[0, :10]}\")\n",
    "\n",
    "        print('Compiled model {}'.format(index))\n",
    "        print(index.summary())\n",
    "\n",
    "        # Save the TensorFlow SavedModel for Serving Predictions\n",
    "        # Note:  We must call index() above before we save().\n",
    "        #        See https://github.com/tensorflow/tensorflow/issues/31057 for more details.\n",
    "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))   \n",
    "        index.save(tensorflow_saved_model_path, save_format='tf')\n",
    "                \n",
    "        # Copy inference.py and requirements.txt to the code/ directory\n",
    "        #   Note: This is required for the SageMaker Endpoint to pick them up.\n",
    "        #         This appears to be hard-coded and must be called code/\n",
    "        inference_path = os.path.join(local_model_dir, 'code/')\n",
    "        print('Copying inference source files to {}'.format(inference_path))\n",
    "        os.makedirs(inference_path, exist_ok=True)               \n",
    "        os.system('cp inference.py {}'.format(inference_path))\n",
    "        print(glob(inference_path))        \n",
    "#        os.system('cp requirements.txt {}/code'.format(inference_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"42\"\n",
    "\n",
    "!saved_model_cli run --input_exprs 'input_1=np.array([\"$user_id\"])' --tag_set serve --signature_def serving_default --dir ./model/tensorflow/saved_model/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "quickstart.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
