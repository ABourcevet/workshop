{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TensorFlow Recommender System (TFRS) using Amazon SageMaker. \n",
    "\n",
    "We will use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/pip\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/cli/main.py\", line 73, in main\n",
      "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/commands/__init__.py\", line 96, in create_command\n",
      "    module = importlib.import_module(module_path)\n",
      "  File \"/opt/conda/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 24, in <module>\n",
      "    from pip._internal.cli.req_command import RequirementCommand\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/cli/req_command.py\", line 19, in <module>\n",
      "    from pip._internal.network.session import PipSession\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/network/session.py\", line 26, in <module>\n",
      "    from pip._internal.network.auth import MultiDomainBasicAuth\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 589, in module_from_spec\n",
      "  File \"<frozen importlib._bootstrap>\", line 568, in _init_module_attrs\n",
      "  File \"<frozen importlib._bootstrap>\", line 409, in cached\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 372, in _get_cached\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 297, in cache_from_source\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 65, in _path_split\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/pip\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/cli/main.py\", line 73, in main\n",
      "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/commands/__init__.py\", line 96, in create_command\n",
      "    module = importlib.import_module(module_path)\n",
      "  File \"/opt/conda/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 24, in <module>\n",
      "    from pip._internal.cli.req_command import RequirementCommand\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/cli/req_command.py\", line 19, in <module>\n",
      "    from pip._internal.network.session import PipSession\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/network/session.py\", line 26, in <module>\n",
      "    from pip._internal.network.auth import MultiDomainBasicAuth\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pip/_internal/network/auth.py\", line 36, in <module>\n",
      "    import keyring  # noqa\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keyring/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keyring/core.py\", line 192, in <module>\n",
      "    init_backend()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keyring/core.py\", line 96, in init_backend\n",
      "    filter(limit, backend.get_all_keyring()),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keyring/util/__init__.py\", line 22, in wrapper\n",
      "    func.always_returns = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keyring/backend.py\", line 216, in get_all_keyring\n",
      "    _load_plugins()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/keyring/backend.py\", line 199, in _load_plugins\n",
      "    entry_points = metadata.entry_points()['keyring.backends']\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/importlib_metadata/__init__.py\", line 564, in entry_points\n",
      "    ordered = sorted(eps, key=by_group)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/importlib_metadata/__init__.py\", line 562, in <genexpr>\n",
      "    dist.entry_points for dist in distributions())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/importlib_metadata/__init__.py\", line 257, in entry_points\n",
      "    return EntryPoint._from_text(self.read_text('entry_points.txt'))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/importlib_metadata/__init__.py\", line 113, in _from_text\n",
      "    config = ConfigParser(delimiters='=')\n",
      "  File \"/opt/conda/lib/python3.7/configparser.py\", line 610, in __init__\n",
      "    self._converters = ConverterMapping(self)\n",
      "  File \"/opt/conda/lib/python3.7/configparser.py\", line 1317, in __init__\n",
      "    for getter in dir(self._parser):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sagemaker==2.9.2\n",
    "!pip install -q sagemaker-experiments==0.1.24\n",
    "!pip install -q tensorflow==2.3.0\n",
    "!pip install -q tensorflow-recommenders==0.2.0\n",
    "!pip install -q tensorflow-datasets==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Input Data S3 URI and `Distribution Strategy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "input_train_data_s3_uri ='s3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/'\n",
    "\n",
    "s3_input_train_data = TrainingInput(s3_data=input_train_data_s3_uri,\n",
    "                                    distribution='ShardedByS3Key')\n",
    "print(s3_input_train_data.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance\n",
    "\n",
    "These sample log lines...\n",
    "```\n",
    "499/500 [=====>..] - ETA: 3s - root_mean_squared_error: 1.194 - factorized_top_k/top_10_categorical_accuracy: 0.481 - factorized_top_k/top_50_categorical_accuracy: 0.607 - factorized_top_k/top_100_categorical_accuracy: 0.885\n",
    "```\n",
    "...will produce the following metrics in CloudWatch:\n",
    "\n",
    "`root_mean_squared_error` = 1.194\n",
    "\n",
    "`factorized_top_k/top_10_categorical_accuracy` = 0.481\n",
    "\n",
    "`factorized_top_k/top_50_categorical_accuracy` = 0.607\n",
    "\n",
    "`factorized_top_k/top_100_categorical_accuracy` = 0.885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'root_mean_squared_error', 'Regex': 'root_mean_squared_error: ([0-9\\\\.]+)'},    \n",
    "     {'Name': 'top_10_categorical_accuracy', 'Regex': 'factorized_top_k/top_10_categorical_accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_50_categorical_accuracy', 'Regex': 'factorized_top_k/top_50_categorical_accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_100_categorical_accuracy', 'Regex': 'factorized_top_k/top_100_categorical_accuracy: ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters for Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "learning_rate=0.5\n",
    "dataset_variant='100k' # movielens 100k, 1m, 20m, 25m, etc\n",
    "embedding_dimension=32 # dimension (k) of our user and item embeddings\n",
    "enable_tensorboard=True\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Our TensorFlow Script to Run on SageMaker\n",
    "Prepare our TensorFlow model to run on the managed SageMaker service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-tensorflow==2.3.0.1.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow-recommenders==0.2.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow-datasets==4.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict, Text\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_datasets\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfds\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_recommenders\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfrs\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRankingModel\u001b[39;49;00m(tf.keras.Model):\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dimension, unique_user_ids, unique_movie_titles):\n",
      "    \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m()\n",
      "\n",
      "    \u001b[37m# Compute embeddings for users.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.user_model = tf.keras.Sequential([\n",
      "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
      "        vocabulary=unique_user_ids, mask_token=\u001b[34mNone\u001b[39;49;00m),\n",
      "      tf.keras.layers.Embedding(\u001b[36mlen\u001b[39;49;00m(unique_user_ids) + \u001b[34m1\u001b[39;49;00m, embedding_dimension)\n",
      "    ])\n",
      "\n",
      "    \u001b[37m# Compute embeddings for movies.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.movie_model = tf.keras.Sequential([\n",
      "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
      "        vocabulary=unique_movie_titles, mask_token=\u001b[34mNone\u001b[39;49;00m),\n",
      "      tf.keras.layers.Embedding(\u001b[36mlen\u001b[39;49;00m(unique_movie_titles) + \u001b[34m1\u001b[39;49;00m, embedding_dimension)\n",
      "    ])\n",
      "\n",
      "    \u001b[37m# Compute predictions.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.ratings = tf.keras.Sequential([\n",
      "      \u001b[37m# Learn multiple dense layers.\u001b[39;49;00m\n",
      "      tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "      tf.keras.layers.Dense(\u001b[34m64\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "      \u001b[37m# Make rating predictions in the final layer.\u001b[39;49;00m\n",
      "      tf.keras.layers.Dense(\u001b[34m1\u001b[39;49;00m)\n",
      "  ])\n",
      "    \n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mcall\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, inputs):\n",
      "\n",
      "    user_id, movie_title = inputs\n",
      "\n",
      "    user_model = \u001b[36mself\u001b[39;49;00m.user_model(user_id)\n",
      "    movie_model = \u001b[36mself\u001b[39;49;00m.movie_model(movie_title)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.ratings(tf.concat([user_model, movie_model], axis=\u001b[34m1\u001b[39;49;00m))\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMovielensModel\u001b[39;49;00m(tfrs.models.Model):\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dimension, unique_user_ids, unique_movie_titles):\n",
      "    \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m()\n",
      "    \u001b[36mself\u001b[39;49;00m.ranking_model: tf.keras.Model = RankingModel(embedding_dimension, unique_user_ids, unique_movie_titles)\n",
      "    \u001b[36mself\u001b[39;49;00m.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
      "      loss = tf.keras.losses.MeanSquaredError(),\n",
      "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
      "    )\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_loss\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: Dict[Text, tf.Tensor], training=\u001b[34mFalse\u001b[39;49;00m) -> tf.Tensor:\n",
      "    rating_predictions = \u001b[36mself\u001b[39;49;00m.ranking_model(\n",
      "        (features[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], features[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "\n",
      "    \u001b[37m# The task computes the loss and the metrics.\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.task(labels=features[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], predictions=rating_predictions)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:    \n",
      "    env_var = os.environ \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)        \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# This is unused\u001b[39;49;00m\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_variant\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m100k\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--embedding_dimension\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcommand line args:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlocal_model_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(local_model_dir))\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))\n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_tensorboard))\n",
      "    dataset_variant = args.dataset_variant\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdataset_variant \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant))\n",
      "    embedding_dimension = \u001b[36mint\u001b[39;49;00m(args.embedding_dimension)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dimension \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(embedding_dimension))    \n",
      "             \n",
      "    \u001b[37m# Load the ratings data to use for training\u001b[39;49;00m\n",
      "    ratings = tfds.load(\u001b[33m'\u001b[39;49;00m\u001b[33mmovielens/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-ratings\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant), \n",
      "                        download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        data_dir=train_data,\n",
      "                        split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRatings raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ratings)\n",
      "    \n",
      "    \u001b[37m# Transform the ratings data specific to our training task\u001b[39;49;00m\n",
      "    ratings = ratings.map(\u001b[34mlambda\u001b[39;49;00m x: {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: x[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    })\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRatings transformed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ratings)    \n",
      "\n",
      "    movies = tfds.load(\u001b[33m'\u001b[39;49;00m\u001b[33mmovielens/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-movies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant),\n",
      "                       download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                       data_dir=train_data,\n",
      "                       split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mMovies raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, movies)\n",
      "\n",
      "    movies = movies.map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mMovies transformed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, movies)\n",
      "\n",
      "    tf.random.set_seed(\u001b[34m42\u001b[39;49;00m)\n",
      "    shuffled = ratings.shuffle(\u001b[34m100_000\u001b[39;49;00m, seed=\u001b[34m42\u001b[39;49;00m, reshuffle_each_iteration=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    train = shuffled.take(\u001b[34m80_000\u001b[39;49;00m)\n",
      "    test = shuffled.skip(\u001b[34m80_000\u001b[39;49;00m).take(\u001b[34m20_000\u001b[39;49;00m)\n",
      "\n",
      "    movie_titles = ratings.batch(\u001b[34m100_000\u001b[39;49;00m).map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    user_ids = ratings.batch(\u001b[34m100_000\u001b[39;49;00m).map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    unique_movie_titles = np.unique(np.concatenate(\u001b[36mlist\u001b[39;49;00m(movie_titles)))\n",
      "    unique_user_ids = np.unique(np.concatenate(\u001b[36mlist\u001b[39;49;00m(user_ids)))\n",
      "    \n",
      "    \u001b[37m# Define the optimizer and hyper-parameters\u001b[39;49;00m\n",
      "    optimizer = tf.keras.optimizers.Adagrad(learning_rate)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\n",
      "\n",
      "    \u001b[37m# Setup the callbacks to use during training\u001b[39;49;00m\n",
      "    callbacks = []\n",
      "\n",
      "    \u001b[37m# Setup the Tensorboard callback if Tensorboard is enabled\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m enable_tensorboard: \n",
      "        \u001b[37m# Tensorboard Logs \u001b[39;49;00m\n",
      "        tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAdding Tensorboard callback \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorboard_callback))\n",
      "        callbacks.append(tensorboard_callback)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCallbacks: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(callbacks))\n",
      "\n",
      "    \u001b[37m# Create and compile a custom Keras model specialized for rating\u001b[39;49;00m\n",
      "    model = MovielensModel(embedding_dimension, unique_user_ids, unique_movie_titles)\n",
      "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=learning_rate))\n",
      "\n",
      "    cached_train = train.shuffle(\u001b[34m100_000\u001b[39;49;00m).batch(\u001b[34m8192\u001b[39;49;00m).cache()\n",
      "    cached_test = test.batch(\u001b[34m4096\u001b[39;49;00m).cache()\n",
      "\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mroot_mean_squared_error: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Make some sample predictions to test our model\u001b[39;49;00m\n",
      "    \u001b[37m# Note:  This is required to save and server our model with TensorFlow Serving\u001b[39;49;00m\n",
      "    \u001b[37m#        See https://github.com/tensorflow/tensorflow/issues/31057 for more  details.\u001b[39;49;00m\n",
      "    \u001b[37m# Create a model that takes in raw query features, and returns the predicted movie titles\u001b[39;49;00m\n",
      "    index = tfrs.layers.factorized_top_k.BruteForce(model.ranking_model.user_model)\n",
      "    index.index(movies.batch(\u001b[34m100\u001b[39;49;00m).map(model.ranking_model.movie_model), movies)\n",
      "\n",
      "    k = \u001b[34m5\u001b[39;49;00m\n",
      "    user_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m42\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "    _, titles = index(np.array([user_id]))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTop \u001b[39;49;00m\u001b[33m{k}\u001b[39;49;00m\u001b[33m recommendations for user \u001b[39;49;00m\u001b[33m{user_id}\u001b[39;49;00m\u001b[33m: \u001b[39;49;00m\u001b[33m{titles[0, :k]}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Print a summary of our recommender model\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrained index \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(index))\n",
      "    \u001b[36mprint\u001b[39;49;00m(index.summary())\n",
      "\n",
      "    \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir,\n",
      "                                               \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))\n",
      "    index.save(tensorflow_saved_model_path, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "    \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "    \u001b[37m#         This directory must be named `code/`\u001b[39;49;00m\n",
      "    inference_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference.py to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "    os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)               \n",
      "    os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "    \u001b[36mprint\u001b[39;49;00m(glob(inference_path))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize /root/workshop/02_usecases/sagemaker_recommendations/src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='/root/workshop/02_usecases/sagemaker_recommendations/src',\n",
    "                       role=role,\n",
    "                       instance_count=train_instance_count,\n",
    "                       instance_type=train_instance_type,\n",
    "                       py_version='py37',\n",
    "                       framework_version='2.3.0',\n",
    "                       hyperparameters={\n",
    "                           'epochs': epochs,\n",
    "                           'learning_rate': learning_rate,\n",
    "                           'dataset_variant': dataset_variant,\n",
    "                           'embedding_dimension': embedding_dimension,                           \n",
    "                           'enable_tensorboard': enable_tensorboard\n",
    "                       },\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                       debugger_hook_config=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: MovieLens-Recommender-1604389575\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "recommender_experiment = Experiment.create(\n",
    "                         experiment_name='MovieLens-Recommender-{}'.format(timestamp),\n",
    "                         description='MovieLens Recommender', \n",
    "                         sagemaker_boto_client=sm)\n",
    "\n",
    "recommender_experiment_name = recommender_experiment.experiment_name\n",
    "print('Experiment name: {}'.format(recommender_experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1604389575-1000-100k-256\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial_name = 'trial-{}-{}-{}-{}'.format(timestamp, epochs, dataset_variant, embedding_dimension)\n",
    "\n",
    "trial = Trial.create(trial_name=trial_name,\n",
    "                     experiment_name=recommender_experiment_name,\n",
    "                     sagemaker_boto_client=sm)\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print('Trial name: {}'.format(trial_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_experiment_config = {\n",
    "    'ExperimentName': recommender_experiment_name,\n",
    "    'TrialName': trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2020-11-03-07-46-15-771\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "              inputs={\n",
    "                  'train': s3_input_train_data, \n",
    "              },              \n",
    "              experiment_config=recommender_experiment_config,                   \n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job Name:  tensorflow-training-2020-11-03-07-46-15-771\n"
     ]
    }
   ],
   "source": [
    "recommender_training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(recommender_training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2020-11-03-07-46-15-771\">Training Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a></b>'.format(region, recommender_training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-11-03-07-46-15-771;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'.format(region, recommender_training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-03-07-46-15-771/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, recommender_training_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Training Job to Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-03 07:46:17 Starting - Starting the training job\n",
      "2020-11-03 07:46:22 Starting - Launching requested ML instances"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the Trained Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$recommender_training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir ./model/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli run --input_exprs 'input_1=np.array([\"$user_id\"])' --tag_set serve --signature_def serving_default --dir ./model/tensorflow/saved_model/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=recommender_experiment_name,\n",
    "    metric_names=[\n",
    "        'root_mean_squared_error',        \n",
    "        'top_10_categorical_accuracy',\n",
    "        'top_50_categorical_accuracy',\n",
    "        'top_100_categorical_accuracy'\n",
    "    ],\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "\n",
    "lineage_df = lineage_table.dataframe()\n",
    "lineage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.describe_trial_component(TrialComponentName=lineage_df.TrialComponentName[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store recommender_training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
