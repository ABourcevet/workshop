{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Recommender with Apache Spark ML\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mevaluation\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m RegressionEvaluator\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mrecommendation\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ALS\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Row\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\n",
      "    \n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\n",
      "    s3_output_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_data)\n",
      "    \n",
      "    spark = SparkSession.builder \\\n",
      "        .appName(\u001b[33m\"\u001b[39;49;00m\u001b[33mSpark_ALS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \\\n",
      "        .getOrCreate()\n",
      "\n",
      "    lines = spark.read.text(s3_input_data).rdd\n",
      "    parts = lines.map(\u001b[34mlambda\u001b[39;49;00m row: row.value.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m::\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    ratingsRDD = parts.map(\u001b[34mlambda\u001b[39;49;00m p: Row(userId=\u001b[36mint\u001b[39;49;00m(p[\u001b[34m0\u001b[39;49;00m]), movieId=\u001b[36mint\u001b[39;49;00m(p[\u001b[34m1\u001b[39;49;00m]),\n",
      "                                         rating=\u001b[36mfloat\u001b[39;49;00m(p[\u001b[34m2\u001b[39;49;00m]), timestamp=\u001b[36mint\u001b[39;49;00m(p[\u001b[34m3\u001b[39;49;00m])))\n",
      "    ratings = spark.createDataFrame(ratingsRDD)\n",
      "    (training, test) = ratings.randomSplit([\u001b[34m0.8\u001b[39;49;00m, \u001b[34m0.2\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Build the recommendation model using ALS on the training data\u001b[39;49;00m\n",
      "    \u001b[37m# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\u001b[39;49;00m\n",
      "    als = ALS(maxIter=\u001b[34m5\u001b[39;49;00m, regParam=\u001b[34m0.01\u001b[39;49;00m, userCol=\u001b[33m\"\u001b[39;49;00m\u001b[33muserId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, itemCol=\u001b[33m\"\u001b[39;49;00m\u001b[33mmovieId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ratingCol=\u001b[33m\"\u001b[39;49;00m\u001b[33mrating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "              coldStartStrategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mdrop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = als.fit(training)\n",
      "\n",
      "    \u001b[37m# Evaluate the model by computing the RMSE on the test data\u001b[39;49;00m\n",
      "    predictions = model.transform(test)\n",
      "    evaluator = RegressionEvaluator(metricName=\u001b[33m\"\u001b[39;49;00m\u001b[33mrmse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                                    labelCol=\u001b[33m\"\u001b[39;49;00m\u001b[33mrating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                                    predictionCol=\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    rmse = evaluator.evaluate(predictions)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mrmse: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(rmse))\n",
      "\n",
      "    \u001b[37m# Generate top 10 movie recommendations for each user\u001b[39;49;00m\n",
      "    userRecs = model.recommendForAllUsers(\u001b[34m10\u001b[39;49;00m)\n",
      "    userRecs.show()\n",
      "    \u001b[37m# Write top 10 movie recommendations for each user\u001b[39;49;00m\n",
      "    userRecs.repartition(\u001b[34m1\u001b[39;49;00m).write.mode(SaveMode.Overwrite).option(\u001b[33m\"\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, true).option(\u001b[33m\"\u001b[39;49;00m\u001b[33mdelimiter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).csv(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{s3_output_data}\u001b[39;49;00m\u001b[33m/recommendations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[37m# Generate top 10 movie recommendations for a specified set of 3 users\u001b[39;49;00m\n",
      "    \u001b[37m# TODO:  Just select user_id \"42\"    \u001b[39;49;00m\n",
      "    users = ratings.select(als.getUserCol()).distinct().limit(\u001b[34m3\u001b[39;49;00m)\n",
      "    userSubsetRecs = model.recommendForUserSubset(users, \u001b[34m10\u001b[39;49;00m)\n",
      "    userSubsetRecs.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "\u001b[37m#     # Generate top 10 user recommendations for each movie\u001b[39;49;00m\n",
      "\u001b[37m#     movieRecs = model.recommendForAllItems(10)\u001b[39;49;00m\n",
      "\u001b[37m#     movieRecs.show()\u001b[39;49;00m\n",
      "\u001b[37m#     # Write top 10 user recommendations for each movie\u001b[39;49;00m\n",
      "\u001b[37m#     movieRecs.\u001b[39;49;00m\n",
      "\u001b[37m#       .repartition(1)\u001b[39;49;00m\n",
      "\u001b[37m#       .write\u001b[39;49;00m\n",
      "\u001b[37m#       .mode(SaveMode.Overwrite)\u001b[39;49;00m\n",
      "\u001b[37m#       .option(\"header\", true)      \u001b[39;49;00m\n",
      "\u001b[37m#       .option(\"delimiter\", \"\\t\")\u001b[39;49;00m\n",
      "\u001b[37m#       .csv(f\"{s3_output_data}/movies\")\u001b[39;49;00m\n",
      "  \n",
      "\u001b[37m#     # Generate top 10 user recommendations for a specified set of movies\u001b[39;49;00m\n",
      "\u001b[37m#     # TODO:  Just select user_id \"42\"\u001b[39;49;00m\n",
      "\u001b[37m#     movies = ratings.select(als.getItemCol()).distinct().limit(3)\u001b[39;49;00m\n",
      "\u001b[37m#     movieSubSetRecs = model.recommendForItemSubset(movies, 10)\u001b[39;49;00m\n",
      "\u001b[37m#     movieSubSetRecs.show(truncate=False)\u001b[39;49;00m\n",
      "        \n",
      "    spark.stop()\n",
      "    \n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train_spark_als.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "processor = PySparkProcessor(base_job_name='spark-als',\n",
    "                             role=role,\n",
    "                             instance_count=1,\n",
    "                             instance_type='ml.r5.2xlarge',\n",
    "                             max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/spark_datasets/train/movielens/sample_movielens_ratings.txt\n"
     ]
    }
   ],
   "source": [
    "s3_input_data = 's3://sagemaker-us-east-1-835319576252/spark_datasets/train/movielens/sample_movielens_ratings.txt'\n",
    "\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 20:04:44      32363 sample_movielens_ratings.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  spark-als-2020-11-02-21-12-42\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'spark-als-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'spark-als-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/spark-als-2020-11-02-21-12-42/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-als-2020-11-02-21-12-42-419\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-als-2020-11-02-21-12-42-419/input/code/train_spark_als.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(submit_app='src/train_spark_als.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_data', s3_output_data,\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/spark-als-2020-11-02-21-12-42-419\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-als-2020-11-02-21-12-42-419;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/spark-als-2020-11-02-21-12-42/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor the Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-als-2020-11-02-21-12-42-419/input/code/train_spark_als.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingJobName': 'spark-als-2020-11-02-21-12-42-419', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.r5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}, 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '/opt/ml/processing/input/code/train_spark_als.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-835319576252/spark_datasets/train/movielens/sample_movielens_ratings.txt', 's3_output_data', 's3://sagemaker-us-east-1-835319576252/spark-als-2020-11-02-21-12-42/output']}, 'Environment': {}, 'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:835319576252:processing-job/spark-als-2020-11-02-21-12-42-419', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 11, 2, 21, 12, 43, 193000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 11, 2, 21, 12, 42, 849000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'a49dd88f-b5d9-4d23-952d-cecbe31a2e6e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'a49dd88f-b5d9-4d23-952d-cecbe31a2e6e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1508', 'date': 'Mon, 02 Nov 2020 21:12:42 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........."
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $s3_output_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Copy the Output from S3 to Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $s3_output_data ./spark-als-output/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recommendations = load_dataset(path='./spark-als-output/recommendations/', sep='\\t', header=0)\n",
    "df_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# Jupyter.notebook.save_checkpoint();\n",
    "# Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
