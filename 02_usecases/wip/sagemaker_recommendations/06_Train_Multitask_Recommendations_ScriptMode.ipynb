{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TensorFlow Recommender System (TFRS) using Amazon SageMaker. \n",
    "\n",
    "We will use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker==2.9.2\n",
    "!pip install -q sagemaker-experiments==0.1.24\n",
    "!pip install -q tensorflow==2.3.0\n",
    "!pip install -q tensorflow-recommenders==0.2.0\n",
    "!pip install -q tensorflow-datasets==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64\n",
      "drwx------ 13 root nogroup 6144 Nov  2 21:33 .\n",
      "drwxr-xr-x  1 root root      39 Nov  2 21:24 ..\n",
      "-rw-------  1 root root    2230 Nov  2 21:14 .bash_history\n",
      "drwxr-xr-x  7 root root    6144 Nov  2 20:59 .cache\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 15:44 .config\n",
      "-rw-r--r--  1 root root      54 Nov  2 15:39 .gitconfig\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:31 .ipynb_checkpoints\n",
      "drwxr-xr-x  5 root root    6144 Oct 10 23:13 .ipython\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:24 .jupyter\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:59 .keras\n",
      "drwxr-xr-x  3 root root    6144 Oct 10 22:41 .local\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:33 .ssh\n",
      "-rw-r--r--  1 root root     111 Oct 10 22:41 .yarnrc\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:12 exported_models\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:33 model\n",
      "-rw-r--r--  1 root root     764 Nov  2 21:33 model.tar.gz\n",
      "drwxr-xr-x 19 root root    6144 Nov  1 19:35 workshop\n"
     ]
    }
   ],
   "source": [
    "!ls -al ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Input Data S3 URI and `Distribution Strategy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "input_train_data_s3_uri ='s3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/'\n",
    "\n",
    "s3_input_train_data = TrainingInput(s3_data=input_train_data_s3_uri,\n",
    "                                    distribution='ShardedByS3Key')\n",
    "print(s3_input_train_data.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance\n",
    "\n",
    "These sample log lines...\n",
    "```\n",
    "499/500 [=====>..] - ETA: 3s - root_mean_squared_error: 1.1198 - factorized_top_k/top_10_categorical_accuracy: 0.481 - factorized_top_k/top_50_categorical_accuracy: 0.607 - factorized_top_k/top_100_categorical_accuracy: 0.885\n",
    "```\n",
    "...will produce the following metrics in CloudWatch:\n",
    "\n",
    "`root_mean_squared_error` = 1.1198\n",
    "\n",
    "`factorized_top_k/top_10_categorical_accuracy` = 0.481\n",
    "\n",
    "`factorized_top_k/top_50_categorical_accuracy` = 0.607\n",
    "\n",
    "`factorized_top_k/top_100_categorical_accuracy` = 0.885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [    \n",
    "     {'Name': 'root_mean_squared_error', 'Regex': 'root_mean_squared_error: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_10_categorical_accuracy', 'Regex': 'factorized_top_k/top_10_categorical_accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_50_categorical_accuracy', 'Regex': 'factorized_top_k/top_50_categorical_accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_100_categorical_accuracy', 'Regex': 'factorized_top_k/top_100_categorical_accuracy: ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters for Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "learning_rate=0.5\n",
    "dataset_variant='100k' # movielens 100k, 1m, 20m, 25m, etc\n",
    "embedding_dimension=32 # dimension (k) of our user and item embeddings\n",
    "enable_tensorboard=True\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Our TensorFlow Script to Run on SageMaker\n",
    "Prepare our TensorFlow model to run on the managed SageMaker service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64\n",
      "drwx------ 13 root nogroup 6144 Nov  2 21:33 .\n",
      "drwxr-xr-x  1 root root      39 Nov  2 21:24 ..\n",
      "-rw-------  1 root root    2230 Nov  2 21:14 .bash_history\n",
      "drwxr-xr-x  7 root root    6144 Nov  2 20:59 .cache\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 15:44 .config\n",
      "-rw-r--r--  1 root root      54 Nov  2 15:39 .gitconfig\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:31 .ipynb_checkpoints\n",
      "drwxr-xr-x  5 root root    6144 Oct 10 23:13 .ipython\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:24 .jupyter\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:59 .keras\n",
      "drwxr-xr-x  3 root root    6144 Oct 10 22:41 .local\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:33 .ssh\n",
      "-rw-r--r--  1 root root     111 Oct 10 22:41 .yarnrc\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:12 exported_models\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:33 model\n",
      "-rw-r--r--  1 root root     764 Nov  2 21:33 model.tar.gz\n",
      "drwxr-xr-x 19 root root    6144 Nov  1 19:35 workshop\n"
     ]
    }
   ],
   "source": [
    "!ls -al ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-tensorflow==2.3.0.1.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow-recommenders==0.2.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow-datasets==4.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict, Text\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_datasets\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfds\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_recommenders\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfrs\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMovielensModel\u001b[39;49;00m(tfrs.models.Model):\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dimension: \u001b[36mint\u001b[39;49;00m, rating_weight: \u001b[36mfloat\u001b[39;49;00m, retrieval_weight: \u001b[36mfloat\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "    \u001b[37m# We take the loss weights in the constructor: this allows us to instantiate\u001b[39;49;00m\n",
      "    \u001b[37m# several model objects with different loss weights.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m()\n",
      "\n",
      "\u001b[37m#    embedding_dimension = 32\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# User and movie models.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
      "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
      "        vocabulary=unique_movie_titles, mask_token=\u001b[34mNone\u001b[39;49;00m),\n",
      "      tf.keras.layers.Embedding(\u001b[36mlen\u001b[39;49;00m(unique_movie_titles) + \u001b[34m1\u001b[39;49;00m, embedding_dimension)\n",
      "    ])\n",
      "    \u001b[36mself\u001b[39;49;00m.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
      "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
      "        vocabulary=unique_user_ids, mask_token=\u001b[34mNone\u001b[39;49;00m),\n",
      "      tf.keras.layers.Embedding(\u001b[36mlen\u001b[39;49;00m(unique_user_ids) + \u001b[34m1\u001b[39;49;00m, embedding_dimension)\n",
      "    ])\n",
      "\n",
      "    \u001b[37m# A small model to take in user and movie embeddings and predict ratings.\u001b[39;49;00m\n",
      "    \u001b[37m# We can make this as complicated as we want as long as we output a scalar\u001b[39;49;00m\n",
      "    \u001b[37m# as our prediction.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.rating_model = tf.keras.Sequential([\n",
      "        tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        tf.keras.layers.Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        tf.keras.layers.Dense(\u001b[34m1\u001b[39;49;00m),\n",
      "    ])\n",
      "\n",
      "    \u001b[37m# The tasks.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
      "        loss=tf.keras.losses.MeanSquaredError(),\n",
      "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
      "    )\n",
      "    \u001b[36mself\u001b[39;49;00m.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
      "        metrics=tfrs.metrics.FactorizedTopK(\n",
      "            candidates=movies.batch(\u001b[34m128\u001b[39;49;00m).map(\u001b[36mself\u001b[39;49;00m.movie_model)\n",
      "        )\n",
      "    )\n",
      "\n",
      "    \u001b[37m# The loss weights.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.rating_weight = rating_weight\n",
      "    \u001b[36mself\u001b[39;49;00m.retrieval_weight = retrieval_weight\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mcall\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
      "    \u001b[37m# We pick out the user features and pass them into the user model.\u001b[39;49;00m\n",
      "    user_embeddings = \u001b[36mself\u001b[39;49;00m.user_model(features[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[37m# And pick out the movie features and pass them into the movie model.\u001b[39;49;00m\n",
      "    movie_embeddings = \u001b[36mself\u001b[39;49;00m.movie_model(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m (\n",
      "        user_embeddings,\n",
      "        movie_embeddings,\n",
      "        \u001b[37m# We apply the multi-layered rating model to a concatentation of\u001b[39;49;00m\n",
      "        \u001b[37m# user and movie embeddings.\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.rating_model(\n",
      "            tf.concat([user_embeddings, movie_embeddings], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        ),\n",
      "    )\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_loss\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: Dict[Text, tf.Tensor], training=\u001b[34mFalse\u001b[39;49;00m) -> tf.Tensor:\n",
      "\n",
      "    user_embeddings, movie_embeddings, rating_predictions = \u001b[36mself\u001b[39;49;00m(features)\n",
      "\n",
      "    \u001b[37m# We compute the loss for each task.\u001b[39;49;00m\n",
      "    rating_loss = \u001b[36mself\u001b[39;49;00m.rating_task(\n",
      "        labels=features[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        predictions=rating_predictions,\n",
      "    )\n",
      "    retrieval_loss = \u001b[36mself\u001b[39;49;00m.retrieval_task(user_embeddings, movie_embeddings)\n",
      "\n",
      "    \u001b[37m# And combine them using the loss weights.\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.rating_weight * rating_loss\n",
      "            + \u001b[36mself\u001b[39;49;00m.retrieval_weight * retrieval_loss)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:    \n",
      "    env_var = os.environ \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)        \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# This is unused\u001b[39;49;00m\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_variant\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m100k\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--embedding_dimension\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcommand line args:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlocal_model_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(local_model_dir))\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))\n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_tensorboard))\n",
      "    dataset_variant = args.dataset_variant\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdataset_variant \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant))\n",
      "    embedding_dimension = \u001b[36mint\u001b[39;49;00m(args.embedding_dimension)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dimension \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(embedding_dimension))    \n",
      "             \n",
      "    \u001b[37m# Load the ratings data to use for training\u001b[39;49;00m\n",
      "    ratings = tfds.load(\u001b[33m'\u001b[39;49;00m\u001b[33mmovielens/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-ratings\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant), \n",
      "                        download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        data_dir=train_data,\n",
      "                        split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRatings raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ratings)\n",
      "\n",
      "    \u001b[37m# Transform the ratings data specific to our training task\u001b[39;49;00m\n",
      "    ratings = ratings.map(\u001b[34mlambda\u001b[39;49;00m x: {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: x[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "    })\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRatings transformed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ratings)    \n",
      "\n",
      "    \u001b[37m# Load the movies data to use for training\u001b[39;49;00m\n",
      "    movies = tfds.load(\u001b[33m'\u001b[39;49;00m\u001b[33mmovielens/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-movies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant),\n",
      "                       download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                       data_dir=train_data,\n",
      "                       split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mMovies raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, movies)\n",
      "    \n",
      "    \u001b[37m# Transform the movies data specific to our training task\u001b[39;49;00m\n",
      "    movies = movies.map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m'\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mMovies transformed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, movies)\n",
      "\n",
      "    \u001b[37m# Randomly shuffle data and split between train and test.\u001b[39;49;00m\n",
      "    tf.random.set_seed(\u001b[34m42\u001b[39;49;00m)\n",
      "    shuffled = ratings.shuffle(\u001b[34m100_000\u001b[39;49;00m, seed=\u001b[34m42\u001b[39;49;00m, reshuffle_each_iteration=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    train = shuffled.take(\u001b[34m80_000\u001b[39;49;00m)\n",
      "    test = shuffled.skip(\u001b[34m80_000\u001b[39;49;00m).take(\u001b[34m20_000\u001b[39;49;00m)\n",
      "\n",
      "    cached_train = train.shuffle(\u001b[34m100_000\u001b[39;49;00m).batch(\u001b[34m8192\u001b[39;49;00m).cache()\n",
      "    cached_test = test.batch(\u001b[34m4096\u001b[39;49;00m).cache()\n",
      "    \n",
      "    movie_titles = movies.batch(\u001b[34m1_000\u001b[39;49;00m)\n",
      "    user_ids = ratings.batch(\u001b[34m100_000\u001b[39;49;00m).map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    unique_movie_titles = np.unique(np.concatenate(\u001b[36mlist\u001b[39;49;00m(movie_titles)))\n",
      "    unique_user_ids = np.unique(np.concatenate(\u001b[36mlist\u001b[39;49;00m(user_ids)))\n",
      "\n",
      "    \u001b[37m# Define the optimizer and hyper-parameters\u001b[39;49;00m\n",
      "    optimizer = tf.keras.optimizers.Adagrad(learning_rate)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\n",
      "\n",
      "    \u001b[37m# Setup the callbacks to use during training\u001b[39;49;00m\n",
      "    callbacks = []\n",
      "\n",
      "    \u001b[37m# Setup the Tensorboard callback if Tensorboard is enabled\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m enable_tensorboard: \n",
      "        \u001b[37m# Tensorboard Logs \u001b[39;49;00m\n",
      "        tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAdding Tensorboard callback \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorboard_callback))\n",
      "        callbacks.append(tensorboard_callback)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCallbacks: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(callbacks))\n",
      "\n",
      "    \u001b[37m# Create and compile a custom Keras model specialized for rating\u001b[39;49;00m\n",
      "    model = MovielensModel(embedding_dimension=embedding_dimension, \n",
      "                           rating_weight=\u001b[34m1.0\u001b[39;49;00m, \n",
      "                           retrieval_weight=\u001b[34m0.0\u001b[39;49;00m)\n",
      "    model.compile(optimizer=optimizer)\n",
      "\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mretrieval-top-100-accuracy: \u001b[39;49;00m\u001b[33m{metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mranking-rmse: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Create and compile a custom Keras model specialized for retrieval\u001b[39;49;00m\n",
      "    model = MovielensModel(embedding_dimension=embedding_dimension, \n",
      "                           rating_weight=\u001b[34m0.0\u001b[39;49;00m, \n",
      "                           retrieval_weight=\u001b[34m1.0\u001b[39;49;00m)   \n",
      "    model.compile(optimizer=optimizer)\n",
      "\n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mretrieval-top-100-accuracy: \u001b[39;49;00m\u001b[33m{metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mranking-rmse: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Create and compile a custom Keras model for both rating and retrieval\u001b[39;49;00m\n",
      "    model = MovielensModel(embedding_dimension=embedding_dimension, \n",
      "                           rating_weight=\u001b[34m1.0\u001b[39;49;00m, \n",
      "                           retrieval_weight=\u001b[34m1.0\u001b[39;49;00m)\n",
      "    model.compile(optimizer=optimizer)\n",
      "    \n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mRetrieval top-100 accuracy: \u001b[39;49;00m\u001b[33m{metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mRanking RMSE: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    \n",
      "    \u001b[37m# Make some sample predictions to test our model\u001b[39;49;00m\n",
      "    \u001b[37m# Note:  This is required to save and server our model with TensorFlow Serving\u001b[39;49;00m\n",
      "    \u001b[37m#        See https://github.com/tensorflow/tensorflow/issues/31057 for more  details.\u001b[39;49;00m\n",
      "\u001b[37m# Create a model that takes in raw query features, and returns the predicted movie titles\u001b[39;49;00m\n",
      "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
      "    index.index(movies.batch(\u001b[34m100\u001b[39;49;00m).map(model.movie_model), movies)\n",
      "\n",
      "    k = \u001b[34m5\u001b[39;49;00m\n",
      "    user_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m42\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "    _, titles = index(np.array([user_id]))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTop \u001b[39;49;00m\u001b[33m{k}\u001b[39;49;00m\u001b[33m recommendations for user \u001b[39;49;00m\u001b[33m{user_id}\u001b[39;49;00m\u001b[33m: \u001b[39;49;00m\u001b[33m{titles[0, :k]}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Print a summary of our recommender model\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrained index \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(index))\n",
      "    \u001b[36mprint\u001b[39;49;00m(index.summary())\n",
      "\n",
      "    \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir,\n",
      "                                               \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))\n",
      "    index.save(tensorflow_saved_model_path, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "    \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "    \u001b[37m#         This directory must be named `code/`\u001b[39;49;00m\n",
      "    inference_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference.py to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "    os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)               \n",
      "    os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "    \u001b[36mprint\u001b[39;49;00m(glob(inference_path))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize /root/workshop/02_usecases/sagemaker_recommendations/src/train_multitask.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='train_multitask.py',\n",
    "                       source_dir='/root/workshop/02_usecases/sagemaker_recommendations/src',\n",
    "                       role=role,\n",
    "                       instance_count=train_instance_count,\n",
    "                       instance_type=train_instance_type,\n",
    "                       py_version='py37',\n",
    "                       framework_version='2.3.0',\n",
    "                       hyperparameters={\n",
    "                           'epochs': epochs,\n",
    "                           'learning_rate': learning_rate,\n",
    "                           'dataset_variant': dataset_variant,\n",
    "                           'embedding_dimension': embedding_dimension,                           \n",
    "                           'enable_tensorboard': enable_tensorboard\n",
    "                       },\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                       debugger_hook_config=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: MovieLens-Recommender-1604386585\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "recommender_experiment = Experiment.create(\n",
    "                         experiment_name='MovieLens-Recommender-{}'.format(timestamp),\n",
    "                         description='MovieLens Recommender', \n",
    "                         sagemaker_boto_client=sm)\n",
    "\n",
    "recommender_experiment_name = recommender_experiment.experiment_name\n",
    "print('Experiment name: {}'.format(recommender_experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1604386585-10-100k-32\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial_name = 'trial-{}-{}-{}-{}'.format(timestamp, epochs, dataset_variant, embedding_dimension)\n",
    "\n",
    "trial = Trial.create(trial_name=trial_name,\n",
    "                     experiment_name=recommender_experiment_name,\n",
    "                     sagemaker_boto_client=sm)\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print('Trial name: {}'.format(trial_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_experiment_config = {\n",
    "    'ExperimentName': recommender_experiment_name,\n",
    "    'TrialName': trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2020-11-03-06-56-26-165\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "              inputs={\n",
    "                  'train': s3_input_train_data, \n",
    "              },              \n",
    "              experiment_config=recommender_experiment_config,                   \n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job Name:  tensorflow-training-2020-11-03-06-56-26-165\n"
     ]
    }
   ],
   "source": [
    "recommender_training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(recommender_training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2020-11-03-06-56-26-165\">Training Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a></b>'.format(region, recommender_training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-11-03-06-56-26-165;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'.format(region, recommender_training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-03-06-56-26-165/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, recommender_training_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Training Job to Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-03 06:56:29 Starting - Starting the training job\n",
      "2020-11-03 06:56:32 Starting - Launching requested ML instances.............\n",
      "2020-11-03 06:57:43 Starting - Preparing the instances for training.............\n",
      "2020-11-03 06:58:55 Downloading - Downloading input data...............\n",
      "2020-11-03 07:00:14 Training - Downloading the training image.........\n",
      "2020-11-03 07:01:02 Training - Training image download completed. Training in progress.............................\n",
      "2020-11-03 07:03:30 Uploading - Uploading generated training model.\n",
      "2020-11-03 07:03:37 Completed - Training job completed\n",
      "CPU times: user 339 ms, sys: 35.6 ms, total: 375 ms\n",
      "Wall time: 7min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the Trained Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-03-06-56-26-165/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$recommender_training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard/\n",
      "tensorboard/train/\n",
      "tensorboard/train/plugins/\n",
      "tensorboard/train/plugins/profile/\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.tensorflow_stats.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.xplane.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.trace.json.gz\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.overview_page.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.memory_profile.json.gz\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.kernel_stats.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_01_40/ip-10-0-203-59.ec2.internal.input_pipeline.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.tensorflow_stats.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.xplane.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.trace.json.gz\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.overview_page.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.memory_profile.json.gz\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.kernel_stats.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_50/ip-10-0-203-59.ec2.internal.input_pipeline.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.tensorflow_stats.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.xplane.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.trace.json.gz\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.overview_page.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.memory_profile.json.gz\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.kernel_stats.pb\n",
      "tensorboard/train/plugins/profile/2020_11_03_07_02_15/ip-10-0-203-59.ec2.internal.input_pipeline.pb\n",
      "tensorboard/train/events.out.tfevents.1604386900.ip-10-0-203-59.ec2.internal.profile-empty\n",
      "tensorboard/train/events.out.tfevents.1604386934.ip-10-0-203-59.ec2.internal.34.4185.v2\n",
      "tensorboard/train/events.out.tfevents.1604386968.ip-10-0-203-59.ec2.internal.34.7808.v2\n",
      "tensorboard/train/events.out.tfevents.1604386896.ip-10-0-203-59.ec2.internal.34.545.v2\n",
      "code/\n",
      "code/inference.py\n",
      "tensorflow/\n",
      "tensorflow/saved_model/\n",
      "tensorflow/saved_model/0/\n",
      "tensorflow/saved_model/0/saved_model.pb\n",
      "tensorflow/saved_model/0/variables/\n",
      "tensorflow/saved_model/0/variables/variables.index\n",
      "tensorflow/saved_model/0/variables/variables.data-00000-of-00001\n",
      "tensorflow/saved_model/0/assets/\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-03 07:03:46.490162: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2020-11-03 07:03:46.491743: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_1'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: serving_default_input_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "    outputs['output_2'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:1\n",
      "  Method name is: tensorflow/serving/predict\n",
      "2020-11-03 07:03:49.914150: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-11-03 07:03:49.917094: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-11-03 07:03:49.917139: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395): /proc/driver/nvidia/version does not exist\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/saved_model_cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1185, in main\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 715, in show\n",
      "    _show_all(args.dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 307, in _show_all\n",
      "    _show_defined_functions(saved_model_dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 187, in _show_defined_functions\n",
      "    trackable_object = load.load(saved_model_dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 603, in load\n",
      "    return load_internal(export_dir, tags, options)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal\n",
      "    ckpt_options)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 131, in __init__\n",
      "    self._restore_checkpoint()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 358, in _restore_checkpoint\n",
      "    \"%r from the checkpoint.\" % obj))\n",
      "NotImplementedError: Missing functionality to restore state of object <tensorflow.python.saved_model.load._RestoredResource object at 0x7fbe95fa7510> from the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir ./model/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-03 07:03:52.056564: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2020-11-03 07:03:52.056881: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-11-03 07:03:54.370234: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-11-03 07:03:54.373056: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-11-03 07:03:54.373332: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395): /proc/driver/nvidia/version does not exist\n",
      "2020-11-03 07:03:54.376840: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-03 07:03:54.413078: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499995000 Hz\n",
      "2020-11-03 07:03:54.415230: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c2486da380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-03 07:03:54.415271: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py:444: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from ./model/tensorflow/saved_model/0/variables/variables\n",
      "Result for output key output_1:\n",
      "[[0.3783565  0.34440356 0.32317945 0.32249567 0.30873933 0.26953703\n",
      "  0.2685387  0.26478744 0.2595122  0.25869507]]\n",
      "Result for output key output_2:\n",
      "[[b'Bastard Out of Carolina (1996)' b'Scarlet Letter, The (1926)'\n",
      "  b'Maya Lin: A Strong Clear Vision (1994)'\n",
      "  b'Amityville Horror, The (1979)' b'Secret Agent, The (1996)'\n",
      "  b'Color of Night (1994)' b'Love Jones (1997)'\n",
      "  b'Angel and the Badman (1947)' b'My Favorite Season (1993)'\n",
      "  b'Hurricane Streets (1998)']]\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli run --input_exprs 'input_1=np.array([\"$user_id\"])' --tag_set serve --signature_def serving_default --dir ./model/tensorflow/saved_model/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 48)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=recommender_experiment_name,\n",
    "    metric_names=[\n",
    "        'root_mean_squared_error',\n",
    "        'top_10_categorical_accuracy',\n",
    "        'top_50_categorical_accuracy',\n",
    "        'top_100_categorical_accuracy'\n",
    "    ],\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "\n",
    "lineage_df = lineage_table.dataframe()\n",
    "lineage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TrialComponentName', 'DisplayName', 'SourceArn', 'SageMaker.ImageUri',\n",
       "       'SageMaker.InstanceCount', 'SageMaker.InstanceType',\n",
       "       'SageMaker.VolumeSizeInGB', 'dataset_variant', 'embedding_dimension',\n",
       "       'enable_tensorboard', 'epochs', 'learning_rate', 'model_dir',\n",
       "       'sagemaker_container_log_level', 'sagemaker_job_name',\n",
       "       'sagemaker_program', 'sagemaker_region', 'sagemaker_submit_directory',\n",
       "       'top_100_categorical_accuracy - Min',\n",
       "       'top_100_categorical_accuracy - Max',\n",
       "       'top_100_categorical_accuracy - Avg',\n",
       "       'top_100_categorical_accuracy - StdDev',\n",
       "       'top_100_categorical_accuracy - Last',\n",
       "       'top_100_categorical_accuracy - Count', 'root_mean_squared_error - Min',\n",
       "       'root_mean_squared_error - Max', 'root_mean_squared_error - Avg',\n",
       "       'root_mean_squared_error - StdDev', 'root_mean_squared_error - Last',\n",
       "       'root_mean_squared_error - Count', 'top_10_categorical_accuracy - Min',\n",
       "       'top_10_categorical_accuracy - Max',\n",
       "       'top_10_categorical_accuracy - Avg',\n",
       "       'top_10_categorical_accuracy - StdDev',\n",
       "       'top_10_categorical_accuracy - Last',\n",
       "       'top_10_categorical_accuracy - Count',\n",
       "       'top_50_categorical_accuracy - Min',\n",
       "       'top_50_categorical_accuracy - Max',\n",
       "       'top_50_categorical_accuracy - Avg',\n",
       "       'top_50_categorical_accuracy - StdDev',\n",
       "       'top_50_categorical_accuracy - Last',\n",
       "       'top_50_categorical_accuracy - Count', 'train - MediaType',\n",
       "       'train - Value', 'SageMaker.ModelArtifact - MediaType',\n",
       "       'SageMaker.ModelArtifact - Value', 'Trials', 'Experiments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineage_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>dataset_variant</th>\n",
       "      <th>embedding_dimension</th>\n",
       "      <th>enable_tensorboard</th>\n",
       "      <th>...</th>\n",
       "      <th>top_50_categorical_accuracy - Avg</th>\n",
       "      <th>top_50_categorical_accuracy - StdDev</th>\n",
       "      <th>top_50_categorical_accuracy - Last</th>\n",
       "      <th>top_50_categorical_accuracy - Count</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow-training-2020-11-03-06-56-26-165-aw...</td>\n",
       "      <td>train</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:835319576252:train...</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.p3.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>\"100k\"</td>\n",
       "      <td>32.0</td>\n",
       "      <td>true</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119324</td>\n",
       "      <td>0.077991</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>33</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-835319576252/tensorfl...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-835319576252/tensorfl...</td>\n",
       "      <td>[trial-1604386585-10-100k-32]</td>\n",
       "      <td>[MovieLens-Recommender-1604386585]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  tensorflow-training-2020-11-03-06-56-26-165-aw...       train   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:835319576252:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0  763104351884.dkr.ecr.us-east-1.amazonaws.com/t...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB dataset_variant  \\\n",
       "0          ml.p3.2xlarge                      30.0          \"100k\"   \n",
       "\n",
       "   embedding_dimension enable_tensorboard  ...  \\\n",
       "0                 32.0               true  ...   \n",
       "\n",
       "   top_50_categorical_accuracy - Avg  top_50_categorical_accuracy - StdDev  \\\n",
       "0                           0.119324                              0.077991   \n",
       "\n",
       "  top_50_categorical_accuracy - Last  top_50_categorical_accuracy - Count  \\\n",
       "0                             0.0762                                   33   \n",
       "\n",
       "  train - MediaType                                      train - Value  \\\n",
       "0              None  s3://sagemaker-us-east-1-835319576252/tensorfl...   \n",
       "\n",
       "  SageMaker.ModelArtifact - MediaType  \\\n",
       "0                                None   \n",
       "\n",
       "                     SageMaker.ModelArtifact - Value  \\\n",
       "0  s3://sagemaker-us-east-1-835319576252/tensorfl...   \n",
       "\n",
       "                          Trials                         Experiments  \n",
       "0  [trial-1604386585-10-100k-32]  [MovieLens-Recommender-1604386585]  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrialComponentName': 'tensorflow-training-2020-11-03-06-56-26-165-aws-training-job',\n",
       " 'TrialComponentArn': 'arn:aws:sagemaker:us-east-1:835319576252:experiment-trial-component/tensorflow-training-2020-11-03-06-56-26-165-aws-training-job',\n",
       " 'DisplayName': 'train',\n",
       " 'Source': {'SourceArn': 'arn:aws:sagemaker:us-east-1:835319576252:training-job/tensorflow-training-2020-11-03-06-56-26-165',\n",
       "  'SourceType': 'SageMakerTrainingJob'},\n",
       " 'Status': {'PrimaryStatus': 'Completed',\n",
       "  'Message': 'Status: Completed, secondary status: Completed, failure reason: .'},\n",
       " 'StartTime': datetime.datetime(2020, 11, 3, 6, 58, 55, tzinfo=tzlocal()),\n",
       " 'EndTime': datetime.datetime(2020, 11, 3, 7, 3, 37, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2020, 11, 3, 6, 56, 29, 880000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:835319576252:user-profile/d-dsxoghy6ztwy/default-1602368497083',\n",
       "  'UserProfileName': 'default-1602368497083',\n",
       "  'DomainId': 'd-dsxoghy6ztwy'},\n",
       " 'LastModifiedTime': datetime.datetime(2020, 11, 3, 7, 3, 37, 917000, tzinfo=tzlocal()),\n",
       " 'LastModifiedBy': {},\n",
       " 'Parameters': {'SageMaker.ImageUri': {'StringValue': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.0-gpu-py37'},\n",
       "  'SageMaker.InstanceCount': {'NumberValue': 1.0},\n",
       "  'SageMaker.InstanceType': {'StringValue': 'ml.p3.2xlarge'},\n",
       "  'SageMaker.VolumeSizeInGB': {'NumberValue': 30.0},\n",
       "  'dataset_variant': {'StringValue': '\"100k\"'},\n",
       "  'embedding_dimension': {'StringValue': '32', 'NumberValue': 32.0},\n",
       "  'enable_tensorboard': {'StringValue': 'true'},\n",
       "  'epochs': {'StringValue': '10', 'NumberValue': 10.0},\n",
       "  'learning_rate': {'StringValue': '0.5', 'NumberValue': 0.5},\n",
       "  'model_dir': {'StringValue': '\"s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-03-06-56-26-165/model\"'},\n",
       "  'sagemaker_container_log_level': {'StringValue': '20', 'NumberValue': 20.0},\n",
       "  'sagemaker_job_name': {'StringValue': '\"tensorflow-training-2020-11-03-06-56-26-165\"'},\n",
       "  'sagemaker_program': {'StringValue': '\"train_multitask.py\"'},\n",
       "  'sagemaker_region': {'StringValue': '\"us-east-1\"'},\n",
       "  'sagemaker_submit_directory': {'StringValue': '\"s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-03-06-56-26-165/source/sourcedir.tar.gz\"'}},\n",
       " 'InputArtifacts': {'train': {'Value': 's3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/'}},\n",
       " 'OutputArtifacts': {'SageMaker.ModelArtifact': {'Value': 's3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-03-06-56-26-165/output/model.tar.gz'}},\n",
       " 'Metrics': [{'MetricName': 'top_100_categorical_accuracy',\n",
       "   'SourceArn': 'arn:aws:sagemaker:us-east-1:835319576252:training-job/tensorflow-training-2020-11-03-06-56-26-165',\n",
       "   'TimeStamp': datetime.datetime(2020, 11, 3, 7, 3, 23, 630000, tzinfo=tzlocal()),\n",
       "   'Max': 0.3813,\n",
       "   'Min': 0.0,\n",
       "   'Last': 0.186,\n",
       "   'Count': 33,\n",
       "   'Avg': 0.2096666666666667,\n",
       "   'StdDev': 0.1231064774561707},\n",
       "  {'MetricName': 'root_mean_squared_error',\n",
       "   'SourceArn': 'arn:aws:sagemaker:us-east-1:835319576252:training-job/tensorflow-training-2020-11-03-06-56-26-165',\n",
       "   'TimeStamp': datetime.datetime(2020, 11, 3, 7, 3, 23, 630000, tzinfo=tzlocal()),\n",
       "   'Max': 3.8706,\n",
       "   'Min': 0.9196,\n",
       "   'Last': 1.0577,\n",
       "   'Count': 33,\n",
       "   'Avg': 2.191915151515152,\n",
       "   'StdDev': 1.3661874711494604},\n",
       "  {'MetricName': 'top_10_categorical_accuracy',\n",
       "   'SourceArn': 'arn:aws:sagemaker:us-east-1:835319576252:training-job/tensorflow-training-2020-11-03-06-56-26-165',\n",
       "   'TimeStamp': datetime.datetime(2020, 11, 3, 7, 3, 23, 630000, tzinfo=tzlocal()),\n",
       "   'Max': 0.0531,\n",
       "   'Min': 0.0,\n",
       "   'Last': 0.0037,\n",
       "   'Count': 33,\n",
       "   'Avg': 0.02422424242424242,\n",
       "   'StdDev': 0.018709874236332906},\n",
       "  {'MetricName': 'top_50_categorical_accuracy',\n",
       "   'SourceArn': 'arn:aws:sagemaker:us-east-1:835319576252:training-job/tensorflow-training-2020-11-03-06-56-26-165',\n",
       "   'TimeStamp': datetime.datetime(2020, 11, 3, 7, 3, 23, 630000, tzinfo=tzlocal()),\n",
       "   'Max': 0.2385,\n",
       "   'Min': 0.0,\n",
       "   'Last': 0.0762,\n",
       "   'Count': 33,\n",
       "   'Avg': 0.11932424242424242,\n",
       "   'StdDev': 0.07799118952766007}],\n",
       " 'ResponseMetadata': {'RequestId': 'ea0afc84-2784-41c0-bf56-9989cf311569',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ea0afc84-2784-41c0-bf56-9989cf311569',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3379',\n",
       "   'date': 'Tue, 03 Nov 2020 07:03:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.describe_trial_component(TrialComponentName=lineage_df.TrialComponentName[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'recommender_training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store recommender_training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
