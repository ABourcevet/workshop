{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Entity detection with Textract and Comprehend\n",
    "\n",
    "## Contents\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data Prep](#Data-Prep)\n",
    "1. [Textract OCR++](#Textract-OCR++)\n",
    "1. [Amazon GroundTruth Labeling](#Amazon-GroundTruth-Labeling)\n",
    "1. [Comprehend Custom Entity Training](#Comprehend-Custom-Entity-Training)\n",
    "1. [Model Performance](#Model-Performance)\n",
    "1. [Inference](#Inference)\n",
    "1. [Results](#Results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In this notebook, we will cover how to extract and build a custom entity recognizer using Amazon Textract and Comprehend. We will be using Amazon Textract to perform OCR++ on scanned document, GroundTruth to label the interested entities, then passing the extracted documents to Amazon Comprehend to build and train a custom entity recognition model. No prior machine learning knowledge is required. \n",
    "\n",
    "In this example, We are using a public dataset from Kaggle: [Resume Entities for NER](https://www.kaggle.com/dataturks/resume-entities-for-ner?select=Entity+Recognition+in+Resumes.json). The dataset comprised 220 samples of candidate resumes in JSON format. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "_This Notebook was created on ml.t2.medium notebook instances._\n",
    "\n",
    "Let's start by install and import all neccessary libaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.42.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Installing tqdm Python Library\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'textract_comprehend_NER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep <a class=\"anchor\" id=\"Data-Prep\"></a>\n",
    "\n",
    "PDF and PNG are most common format for scanned documents within enterprises. We already converted these resumes into PDF format to emulate this. Let's upload all these PDF resumes onto S3 for Textract processing. Please note, there are only 220 samples of resume inside the dataset. By modern standards, this is a very small dataset. This dataset also come with few labeled custom entities. However, we will be running this dataset through Amazon GroundTruth to obtain a fresh copy of entity list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/220 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/220 [00:01<04:02,  1.11s/it]\u001b[A\n",
      "  1%|          | 2/220 [00:02<04:19,  1.19s/it]\u001b[A\n",
      "  1%|▏         | 3/220 [00:03<04:15,  1.18s/it]\u001b[A\n",
      "  2%|▏         | 4/220 [00:04<04:09,  1.16s/it]\u001b[A\n",
      "  2%|▏         | 5/220 [00:05<04:07,  1.15s/it]\u001b[A\n",
      "  3%|▎         | 6/220 [00:07<04:08,  1.16s/it]\u001b[A\n",
      "  3%|▎         | 7/220 [00:08<04:07,  1.16s/it]\u001b[A\n",
      "  4%|▎         | 8/220 [00:09<04:04,  1.15s/it]\u001b[A\n",
      "  4%|▍         | 9/220 [00:10<04:01,  1.14s/it]\u001b[A\n",
      "  5%|▍         | 10/220 [00:11<04:00,  1.15s/it]\u001b[A\n",
      "  5%|▌         | 11/220 [00:12<03:57,  1.14s/it]\u001b[A\n",
      "  5%|▌         | 12/220 [00:13<03:55,  1.13s/it]\u001b[A\n",
      "  6%|▌         | 13/220 [00:15<03:55,  1.14s/it]\u001b[A\n",
      "  6%|▋         | 14/220 [00:16<03:58,  1.16s/it]\u001b[A\n",
      "  7%|▋         | 15/220 [00:17<03:58,  1.17s/it]\u001b[A\n",
      "  7%|▋         | 16/220 [00:18<03:57,  1.17s/it]\u001b[A\n",
      "  8%|▊         | 17/220 [00:19<03:54,  1.15s/it]\u001b[A\n",
      "  8%|▊         | 18/220 [00:20<03:50,  1.14s/it]\u001b[A\n",
      "  9%|▊         | 19/220 [00:21<03:49,  1.14s/it]\u001b[A\n",
      "  9%|▉         | 20/220 [00:23<03:54,  1.17s/it]\u001b[A\n",
      " 10%|▉         | 21/220 [00:24<03:54,  1.18s/it]\u001b[A\n",
      " 10%|█         | 22/220 [00:25<03:50,  1.17s/it]\u001b[A\n",
      " 10%|█         | 23/220 [00:26<03:48,  1.16s/it]\u001b[A\n",
      " 11%|█         | 24/220 [00:27<03:44,  1.15s/it]\u001b[A\n",
      " 11%|█▏        | 25/220 [00:28<03:41,  1.14s/it]\u001b[A\n",
      " 12%|█▏        | 26/220 [00:30<03:39,  1.13s/it]\u001b[A\n",
      " 12%|█▏        | 27/220 [00:31<03:38,  1.13s/it]\u001b[A\n",
      " 13%|█▎        | 28/220 [00:32<03:42,  1.16s/it]\u001b[A\n",
      " 13%|█▎        | 29/220 [00:33<03:38,  1.14s/it]\u001b[A\n",
      " 14%|█▎        | 30/220 [00:34<03:35,  1.14s/it]\u001b[A\n",
      " 14%|█▍        | 31/220 [00:35<03:32,  1.13s/it]\u001b[A\n",
      " 15%|█▍        | 32/220 [00:36<03:30,  1.12s/it]\u001b[A\n",
      " 15%|█▌        | 33/220 [00:37<03:31,  1.13s/it]\u001b[A\n",
      " 15%|█▌        | 34/220 [00:39<03:32,  1.14s/it]\u001b[A\n",
      " 16%|█▌        | 35/220 [00:40<03:35,  1.17s/it]\u001b[A\n",
      " 16%|█▋        | 36/220 [00:41<03:36,  1.18s/it]\u001b[A\n",
      " 17%|█▋        | 37/220 [00:42<03:32,  1.16s/it]\u001b[A\n",
      " 17%|█▋        | 38/220 [00:43<03:30,  1.15s/it]\u001b[A\n",
      " 18%|█▊        | 39/220 [00:44<03:26,  1.14s/it]\u001b[A\n",
      " 18%|█▊        | 40/220 [00:46<03:23,  1.13s/it]\u001b[A\n",
      " 19%|█▊        | 41/220 [00:47<03:22,  1.13s/it]\u001b[A\n",
      " 19%|█▉        | 42/220 [00:48<03:25,  1.15s/it]\u001b[A\n",
      " 20%|█▉        | 43/220 [00:49<03:26,  1.17s/it]\u001b[A\n",
      " 20%|██        | 44/220 [00:50<03:22,  1.15s/it]\u001b[A\n",
      " 20%|██        | 45/220 [00:51<03:22,  1.16s/it]\u001b[A\n",
      " 21%|██        | 46/220 [00:52<03:20,  1.15s/it]\u001b[A\n",
      " 21%|██▏       | 47/220 [00:54<03:16,  1.14s/it]\u001b[A\n",
      " 22%|██▏       | 48/220 [00:55<03:14,  1.13s/it]\u001b[A\n",
      " 22%|██▏       | 49/220 [00:56<03:11,  1.12s/it]\u001b[A\n",
      " 23%|██▎       | 50/220 [00:57<03:14,  1.15s/it]\u001b[A\n",
      " 23%|██▎       | 51/220 [00:58<03:11,  1.13s/it]\u001b[A\n",
      " 24%|██▎       | 52/220 [00:59<03:11,  1.14s/it]\u001b[A\n",
      " 24%|██▍       | 53/220 [01:00<03:09,  1.13s/it]\u001b[A\n",
      " 25%|██▍       | 54/220 [01:02<03:07,  1.13s/it]\u001b[A\n",
      " 25%|██▌       | 55/220 [01:03<03:05,  1.12s/it]\u001b[A\n",
      " 25%|██▌       | 56/220 [01:04<03:03,  1.12s/it]\u001b[A\n",
      " 26%|██▌       | 57/220 [01:05<03:02,  1.12s/it]\u001b[A\n",
      " 26%|██▋       | 58/220 [01:06<03:04,  1.14s/it]\u001b[A\n",
      " 27%|██▋       | 59/220 [01:07<03:03,  1.14s/it]\u001b[A\n",
      " 27%|██▋       | 60/220 [01:08<03:02,  1.14s/it]\u001b[A\n",
      " 28%|██▊       | 61/220 [01:09<03:00,  1.13s/it]\u001b[A\n",
      " 28%|██▊       | 62/220 [01:11<02:58,  1.13s/it]\u001b[A\n",
      " 29%|██▊       | 63/220 [01:12<02:56,  1.13s/it]\u001b[A\n",
      " 29%|██▉       | 64/220 [01:13<02:55,  1.13s/it]\u001b[A\n",
      " 30%|██▉       | 65/220 [01:14<02:58,  1.15s/it]\u001b[A\n",
      " 30%|███       | 66/220 [01:15<02:55,  1.14s/it]\u001b[A\n",
      " 30%|███       | 67/220 [01:16<02:52,  1.13s/it]\u001b[A\n",
      " 31%|███       | 68/220 [01:17<02:52,  1.13s/it]\u001b[A\n",
      " 31%|███▏      | 69/220 [01:19<02:52,  1.14s/it]\u001b[A\n",
      " 32%|███▏      | 70/220 [01:20<02:50,  1.14s/it]\u001b[A\n",
      " 32%|███▏      | 71/220 [01:21<02:48,  1.13s/it]\u001b[A\n",
      " 33%|███▎      | 72/220 [01:22<02:50,  1.15s/it]\u001b[A\n",
      " 33%|███▎      | 73/220 [01:23<02:48,  1.15s/it]\u001b[A\n",
      " 34%|███▎      | 74/220 [01:24<02:47,  1.15s/it]\u001b[A\n",
      " 34%|███▍      | 75/220 [01:25<02:47,  1.16s/it]\u001b[A\n",
      " 35%|███▍      | 76/220 [01:27<02:46,  1.15s/it]\u001b[A\n",
      " 35%|███▌      | 77/220 [01:28<02:43,  1.14s/it]\u001b[A\n",
      " 35%|███▌      | 78/220 [01:29<02:40,  1.13s/it]\u001b[A\n",
      " 36%|███▌      | 79/220 [01:30<02:38,  1.13s/it]\u001b[A\n",
      " 36%|███▋      | 80/220 [01:31<02:41,  1.15s/it]\u001b[A\n",
      " 37%|███▋      | 81/220 [01:32<02:40,  1.15s/it]\u001b[A\n",
      " 37%|███▋      | 82/220 [01:34<02:44,  1.19s/it]\u001b[A\n",
      " 38%|███▊      | 83/220 [01:35<02:39,  1.17s/it]\u001b[A\n",
      " 38%|███▊      | 84/220 [01:36<02:36,  1.15s/it]\u001b[A\n",
      " 39%|███▊      | 85/220 [01:37<02:33,  1.14s/it]\u001b[A\n",
      " 39%|███▉      | 86/220 [01:38<02:31,  1.13s/it]\u001b[A\n",
      " 40%|███▉      | 87/220 [01:39<02:32,  1.15s/it]\u001b[A\n",
      " 40%|████      | 88/220 [01:40<02:30,  1.14s/it]\u001b[A\n",
      " 40%|████      | 89/220 [01:41<02:28,  1.13s/it]\u001b[A\n",
      " 41%|████      | 90/220 [01:43<02:27,  1.14s/it]\u001b[A\n",
      " 41%|████▏     | 91/220 [01:44<02:26,  1.14s/it]\u001b[A\n",
      " 42%|████▏     | 92/220 [01:45<02:26,  1.15s/it]\u001b[A\n",
      " 42%|████▏     | 93/220 [01:46<02:24,  1.14s/it]\u001b[A\n",
      " 43%|████▎     | 94/220 [01:47<02:26,  1.16s/it]\u001b[A\n",
      " 43%|████▎     | 95/220 [01:48<02:23,  1.15s/it]\u001b[A\n",
      " 44%|████▎     | 96/220 [01:49<02:20,  1.14s/it]\u001b[A\n",
      " 44%|████▍     | 97/220 [01:51<02:18,  1.13s/it]\u001b[A\n",
      " 45%|████▍     | 98/220 [01:52<02:18,  1.13s/it]\u001b[A\n",
      " 45%|████▌     | 99/220 [01:53<02:17,  1.14s/it]\u001b[A\n",
      " 45%|████▌     | 100/220 [01:54<02:16,  1.14s/it]\u001b[A\n",
      " 46%|████▌     | 101/220 [01:55<02:16,  1.14s/it]\u001b[A\n",
      " 46%|████▋     | 102/220 [01:56<02:17,  1.17s/it]\u001b[A\n",
      " 47%|████▋     | 103/220 [01:58<02:15,  1.15s/it]\u001b[A\n",
      " 47%|████▋     | 104/220 [01:59<02:12,  1.14s/it]\u001b[A\n",
      " 48%|████▊     | 105/220 [02:00<02:10,  1.14s/it]\u001b[A\n",
      " 48%|████▊     | 106/220 [02:01<02:09,  1.13s/it]\u001b[A\n",
      " 49%|████▊     | 107/220 [02:02<02:08,  1.14s/it]\u001b[A\n",
      " 49%|████▉     | 108/220 [02:03<02:06,  1.13s/it]\u001b[A\n",
      " 50%|████▉     | 109/220 [02:04<02:08,  1.15s/it]\u001b[A\n",
      " 50%|█████     | 110/220 [02:05<02:05,  1.14s/it]\u001b[A\n",
      " 50%|█████     | 111/220 [02:07<02:04,  1.14s/it]\u001b[A\n",
      " 51%|█████     | 112/220 [02:08<02:01,  1.13s/it]\u001b[A\n",
      " 51%|█████▏    | 113/220 [02:09<02:00,  1.13s/it]\u001b[A\n",
      " 52%|█████▏    | 114/220 [02:10<01:59,  1.12s/it]\u001b[A\n",
      " 52%|█████▏    | 115/220 [02:11<01:57,  1.12s/it]\u001b[A\n",
      " 53%|█████▎    | 116/220 [02:12<01:59,  1.15s/it]\u001b[A\n",
      " 53%|█████▎    | 117/220 [02:13<01:57,  1.14s/it]\u001b[A\n",
      " 54%|█████▎    | 118/220 [02:15<01:57,  1.16s/it]\u001b[A\n",
      " 54%|█████▍    | 119/220 [02:16<01:55,  1.15s/it]\u001b[A\n",
      " 55%|█████▍    | 120/220 [02:17<01:53,  1.13s/it]\u001b[A\n",
      " 55%|█████▌    | 121/220 [02:18<01:52,  1.14s/it]\u001b[A\n",
      " 55%|█████▌    | 122/220 [02:19<01:51,  1.14s/it]\u001b[A\n",
      " 56%|█████▌    | 123/220 [02:20<01:50,  1.14s/it]\u001b[A\n",
      " 56%|█████▋    | 124/220 [02:21<01:51,  1.16s/it]\u001b[A\n",
      " 57%|█████▋    | 125/220 [02:23<01:48,  1.14s/it]\u001b[A\n",
      " 57%|█████▋    | 126/220 [02:24<01:48,  1.16s/it]\u001b[A\n",
      " 58%|█████▊    | 127/220 [02:25<01:46,  1.14s/it]\u001b[A\n",
      " 58%|█████▊    | 128/220 [02:26<01:44,  1.14s/it]\u001b[A\n",
      " 59%|█████▊    | 129/220 [02:27<01:42,  1.12s/it]\u001b[A\n",
      " 59%|█████▉    | 130/220 [02:28<01:40,  1.12s/it]\u001b[A\n",
      " 60%|█████▉    | 131/220 [02:29<01:41,  1.14s/it]\u001b[A\n",
      " 60%|██████    | 132/220 [02:30<01:39,  1.13s/it]\u001b[A\n",
      " 60%|██████    | 133/220 [02:32<01:37,  1.13s/it]\u001b[A\n",
      " 61%|██████    | 134/220 [02:33<01:36,  1.12s/it]\u001b[A\n",
      " 61%|██████▏   | 135/220 [02:34<01:35,  1.12s/it]\u001b[A\n",
      " 62%|██████▏   | 136/220 [02:35<01:34,  1.12s/it]\u001b[A\n",
      " 62%|██████▏   | 137/220 [02:36<01:34,  1.13s/it]\u001b[A\n",
      " 63%|██████▎   | 138/220 [02:37<01:38,  1.21s/it]\u001b[A\n",
      " 63%|██████▎   | 139/220 [02:39<01:35,  1.18s/it]\u001b[A\n",
      " 64%|██████▎   | 140/220 [02:40<01:32,  1.16s/it]\u001b[A\n",
      " 64%|██████▍   | 141/220 [02:41<01:30,  1.15s/it]\u001b[A\n",
      " 65%|██████▍   | 142/220 [02:42<01:29,  1.15s/it]\u001b[A\n",
      " 65%|██████▌   | 143/220 [02:43<01:27,  1.13s/it]\u001b[A\n",
      " 65%|██████▌   | 144/220 [02:44<01:25,  1.13s/it]\u001b[A\n",
      " 66%|██████▌   | 145/220 [02:45<01:24,  1.12s/it]\u001b[A\n",
      " 66%|██████▋   | 146/220 [02:47<01:24,  1.15s/it]\u001b[A\n",
      " 67%|██████▋   | 147/220 [02:48<01:23,  1.14s/it]\u001b[A\n",
      " 67%|██████▋   | 148/220 [02:49<01:22,  1.15s/it]\u001b[A\n",
      " 68%|██████▊   | 149/220 [02:50<01:20,  1.14s/it]\u001b[A\n",
      " 68%|██████▊   | 150/220 [02:51<01:19,  1.13s/it]\u001b[A\n",
      " 69%|██████▊   | 151/220 [02:52<01:17,  1.13s/it]\u001b[A\n",
      " 69%|██████▉   | 152/220 [02:53<01:16,  1.12s/it]\u001b[A\n",
      " 70%|██████▉   | 153/220 [02:54<01:16,  1.14s/it]\u001b[A\n",
      " 70%|███████   | 154/220 [02:56<01:14,  1.14s/it]\u001b[A\n",
      " 70%|███████   | 155/220 [02:57<01:13,  1.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 156/220 [02:58<01:11,  1.12s/it]\u001b[A\n",
      " 71%|███████▏  | 157/220 [02:59<01:10,  1.12s/it]\u001b[A\n",
      " 72%|███████▏  | 158/220 [03:00<01:09,  1.12s/it]\u001b[A\n",
      " 72%|███████▏  | 159/220 [03:01<01:08,  1.12s/it]\u001b[A\n",
      " 73%|███████▎  | 160/220 [03:02<01:08,  1.14s/it]\u001b[A\n",
      " 73%|███████▎  | 161/220 [03:03<01:06,  1.13s/it]\u001b[A\n",
      " 74%|███████▎  | 162/220 [03:05<01:05,  1.12s/it]\u001b[A\n",
      " 74%|███████▍  | 163/220 [03:06<01:03,  1.12s/it]\u001b[A\n",
      " 75%|███████▍  | 164/220 [03:07<01:08,  1.23s/it]\u001b[A\n",
      " 75%|███████▌  | 165/220 [03:08<01:07,  1.23s/it]\u001b[A\n",
      " 75%|███████▌  | 166/220 [03:10<01:05,  1.20s/it]\u001b[A\n",
      " 76%|███████▌  | 167/220 [03:11<01:03,  1.19s/it]\u001b[A\n",
      " 76%|███████▋  | 168/220 [03:12<01:03,  1.22s/it]\u001b[A\n",
      " 77%|███████▋  | 169/220 [03:13<01:01,  1.20s/it]\u001b[A\n",
      " 77%|███████▋  | 170/220 [03:14<00:58,  1.17s/it]\u001b[A\n",
      " 78%|███████▊  | 171/220 [03:15<00:56,  1.15s/it]\u001b[A\n",
      " 78%|███████▊  | 172/220 [03:16<00:54,  1.15s/it]\u001b[A\n",
      " 79%|███████▊  | 173/220 [03:18<00:53,  1.15s/it]\u001b[A\n",
      " 79%|███████▉  | 174/220 [03:19<00:52,  1.13s/it]\u001b[A\n",
      " 80%|███████▉  | 175/220 [03:20<00:54,  1.22s/it]\u001b[A\n",
      " 80%|████████  | 176/220 [03:21<00:52,  1.20s/it]\u001b[A\n",
      " 80%|████████  | 177/220 [03:22<00:50,  1.18s/it]\u001b[A\n",
      " 81%|████████  | 178/220 [03:24<00:48,  1.16s/it]\u001b[A\n",
      " 81%|████████▏ | 179/220 [03:25<00:47,  1.15s/it]\u001b[A\n",
      " 82%|████████▏ | 180/220 [03:26<00:45,  1.14s/it]\u001b[A\n",
      " 82%|████████▏ | 181/220 [03:27<00:44,  1.14s/it]\u001b[A\n",
      " 83%|████████▎ | 182/220 [03:28<00:44,  1.16s/it]\u001b[A\n",
      " 83%|████████▎ | 183/220 [03:29<00:42,  1.15s/it]\u001b[A\n",
      " 84%|████████▎ | 184/220 [03:30<00:41,  1.16s/it]\u001b[A\n",
      " 84%|████████▍ | 185/220 [03:32<00:40,  1.15s/it]\u001b[A\n",
      " 85%|████████▍ | 186/220 [03:33<00:38,  1.14s/it]\u001b[A\n",
      " 85%|████████▌ | 187/220 [03:34<00:37,  1.14s/it]\u001b[A\n",
      " 85%|████████▌ | 188/220 [03:35<00:36,  1.13s/it]\u001b[A\n",
      " 86%|████████▌ | 189/220 [03:36<00:35,  1.15s/it]\u001b[A\n",
      " 86%|████████▋ | 190/220 [03:37<00:34,  1.16s/it]\u001b[A\n",
      " 87%|████████▋ | 191/220 [03:38<00:33,  1.15s/it]\u001b[A\n",
      " 87%|████████▋ | 192/220 [03:40<00:31,  1.14s/it]\u001b[A\n",
      " 88%|████████▊ | 193/220 [03:41<00:30,  1.13s/it]\u001b[A\n",
      " 88%|████████▊ | 194/220 [03:42<00:29,  1.13s/it]\u001b[A\n",
      " 89%|████████▊ | 195/220 [03:43<00:28,  1.12s/it]\u001b[A\n",
      " 89%|████████▉ | 196/220 [03:44<00:26,  1.12s/it]\u001b[A\n",
      " 90%|████████▉ | 197/220 [03:45<00:26,  1.14s/it]\u001b[A\n",
      " 90%|█████████ | 198/220 [03:46<00:25,  1.16s/it]\u001b[A\n",
      " 90%|█████████ | 199/220 [03:48<00:24,  1.14s/it]\u001b[A\n",
      " 91%|█████████ | 200/220 [03:49<00:22,  1.14s/it]\u001b[A\n",
      " 91%|█████████▏| 201/220 [03:50<00:21,  1.13s/it]\u001b[A\n",
      " 92%|█████████▏| 202/220 [03:51<00:20,  1.16s/it]\u001b[A\n",
      " 92%|█████████▏| 203/220 [03:52<00:19,  1.14s/it]\u001b[A\n",
      " 93%|█████████▎| 204/220 [03:53<00:18,  1.16s/it]\u001b[A\n",
      " 93%|█████████▎| 205/220 [03:55<00:17,  1.19s/it]\u001b[A\n",
      " 94%|█████████▎| 206/220 [03:56<00:16,  1.17s/it]\u001b[A\n",
      " 94%|█████████▍| 207/220 [03:57<00:15,  1.15s/it]\u001b[A\n",
      " 95%|█████████▍| 208/220 [03:58<00:13,  1.16s/it]\u001b[A\n",
      " 95%|█████████▌| 209/220 [03:59<00:12,  1.17s/it]\u001b[A\n",
      " 95%|█████████▌| 210/220 [04:00<00:11,  1.16s/it]\u001b[A\n",
      " 96%|█████████▌| 211/220 [04:01<00:10,  1.14s/it]\u001b[A\n",
      " 96%|█████████▋| 212/220 [04:03<00:09,  1.17s/it]\u001b[A\n",
      " 97%|█████████▋| 213/220 [04:04<00:08,  1.18s/it]\u001b[A\n",
      " 97%|█████████▋| 214/220 [04:05<00:06,  1.16s/it]\u001b[A\n",
      " 98%|█████████▊| 215/220 [04:06<00:05,  1.16s/it]\u001b[A\n",
      " 98%|█████████▊| 216/220 [04:07<00:04,  1.15s/it]\u001b[A\n",
      " 99%|█████████▊| 217/220 [04:08<00:03,  1.14s/it]\u001b[A\n",
      " 99%|█████████▉| 218/220 [04:10<00:02,  1.15s/it]\u001b[A\n",
      "100%|█████████▉| 219/220 [04:11<00:01,  1.20s/it]\u001b[A\n",
      "100%|██████████| 220/220 [04:12<00:00,  1.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Resume PDFs :\t s3://sagemaker-us-east-1-835319576252/textract_comprehend_NER/resume_pdf/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Uploading PDF resumes to S3\n",
    "pdfResumeFileList = glob.glob(\"./resume_pdf/*.pdf\")\n",
    "prefix_resume_pdf = prefix + \"/resume_pdf/\"\n",
    "\n",
    "for filePath in tqdm(pdfResumeFileList):\n",
    "    file_name = os.path.basename(filePath)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix_resume_pdf, file_name)).upload_file(filePath)\n",
    "\n",
    "resume_pdf_bucket_name = 's3://'+bucket+'/'+prefix+'/'+'resume_pdf/'\n",
    "print('Uploaded Resume PDFs :\\t', resume_pdf_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textract OCR++ <a class=\"anchor\" id=\"Textract-OCR++\"></a>\n",
    "\n",
    "Now these PDFs are ready for Textract to perform OCR++, you can kick off the process with [StartDocumentTextDetection](https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentTextDetection.html) async API cal. Here we are only set to process 2 resume PDF for demonstrating the process. To save time, we have all 220 resumes processed and avaliable for you. See textract_output directory for all the reuslts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['textract_comprehend_NER/resume_pdf/text_output_1.pdf',\n",
       " 'textract_comprehend_NER/resume_pdf/text_output_10.pdf',\n",
       " 'textract_comprehend_NER/resume_pdf/text_output_100.pdf',\n",
       " 'textract_comprehend_NER/resume_pdf/text_output_101.pdf',\n",
       " 'textract_comprehend_NER/resume_pdf/text_output_102.pdf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "pdf_object_list = []\n",
    "\n",
    "# Getting a list of resume PDF files:\n",
    "response = s3_client.list_objects(\n",
    "    Bucket= bucket,\n",
    "    Prefix= prefix+'/'+'resume_pdf/text_output'\n",
    ")\n",
    "\n",
    "for obj in response['Contents']:\n",
    "    pdf_object_list.append(obj['Key'])\n",
    "\n",
    "pdf_object_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textract Processing PDF: \ttextract_comprehend_NER/resume_pdf/text_output_1.pdf\n",
      "Textract Job Submitted: \t110b19aa08b267188afa35b5b7a2b90ef2c3797771f0ef20d6793ac52c80a354\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from s3_textract_functions import *\n",
    "import codecs\n",
    "\n",
    "sample_to_process = 2\n",
    "\n",
    "# We are only processing few files as example; You do not need to process all 220 files\n",
    "for file_obj in tqdm(pdf_object_list[:sample_to_process]):\n",
    "    print('Textract Processing PDF: \\t'+ file_obj)             \n",
    "    job_id = StartDocumentTextDetection(bucket, file_obj)\n",
    "    print('Textract Job Submitted: \\t'+ job_id)\n",
    "    response = getDocumentTextDetection(job_id)\n",
    "    \n",
    "    # renaming .pdf to .text\n",
    "    text_output_name = file_obj.replace('.pdf', '.txt')\n",
    "    text_output_name = text_output_name[(text_output_name.rfind('/')+1):]\n",
    "    print('Output Name:\\t', text_output_name)\n",
    "    \n",
    "    output_dir = './textract_output/'\n",
    "    \n",
    "    # Writing Textract Output to Text Files:\n",
    "    with codecs.open(output_dir + text_output_name, \"w\", \"utf-8\") as output_file:\n",
    "        for item in response[\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                print('\\033[94m' + item[\"Text\"] + '\\033[0m')\n",
    "                output_file.write(item[\"Text\"]+'\\n')\n",
    "    output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "    \n",
    "# Uploading Textract Output to S3\n",
    "textract_output_filelist = glob.glob(\"./textract_output/*.txt\")\n",
    "prefix_textract_output = prefix + \"/textract_output/\"\n",
    "\n",
    "for filePath in tqdm(textract_output_filelist):\n",
    "    file_name = os.path.basename(filePath)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix_textract_output, file_name)).upload_file(filePath)\n",
    "\n",
    "comprehend_input_doucuments = 's3://' + bucket+'/'+prefix_textract_output\n",
    "print('Textract Output:\\t', comprehend_input_doucuments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon GroundTruth Labeling <a class=\"anchor\" id=\"Amazon-GroundTruth-Labeling\"></a>\n",
    "\n",
    "Since we need to train a custom entity recognition model with Comprehend, and with any machine learning models, we need large amount of training data. In this example, we are leveraging Amazon GroundTruth to label our entities. Amazon Comprehend by default already can recognize entities like [Person, Title, Organization, and etc](https://docs.aws.amazon.com/comprehend/latest/dg/how-entities.html). To demonstrate custom entity recognition capability, we are focusing on Skill entities inside these resumes. We have the labeled and cleaned the data with Amazon GroundTruth (see: entity_list.csv). If you are interested, you can follow this blog to [add data labeling workflow for named entity recognition](https://aws.amazon.com/blogs/machine-learning/adding-a-data-labeling-workflow-for-named-entity-recognition-with-amazon-sagemaker-ground-truth/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's upload the entity list onto S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading Entity List to S3\n",
    "entity_list_file = './entity_list.csv'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix+'/entity_list/', 'entity_list.csv')).upload_file(entity_list_file)\n",
    "\n",
    "comprehend_input_entity_list = 's3://' + bucket+'/'+prefix+'/entity_list/'+'entity_list.csv'\n",
    "print('Entity List:\\t', comprehend_input_entity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehend Custom Entity Training <a class=\"anchor\" id=\"Comprehend-Custom-Entity-Training\"></a>\n",
    "\n",
    "Now we have both raw and labeled data, and ready to train our model. You can kick off the process with create_entity_recognizer API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comprehend_client = boto3.client('comprehend')\n",
    "custom_recognizer_name = 'resume-entity-recognizer-'+ str(int(time.time()))\n",
    "comprehend_custom_recognizer_response = comprehend_client.create_entity_recognizer(\n",
    "    RecognizerName = custom_recognizer_name,\n",
    "    DataAccessRoleArn=role,\n",
    "    InputDataConfig={\n",
    "        'EntityTypes': [\n",
    "            {\n",
    "                'Type': 'SKILLS'\n",
    "            },\n",
    "        ],\n",
    "        'Documents': {\n",
    "            'S3Uri': comprehend_input_doucuments\n",
    "        },\n",
    "        'EntityList': {\n",
    "            'S3Uri': comprehend_input_entity_list\n",
    "        }\n",
    "    },\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "print(json.dumps(comprehend_custom_recognizer_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job is submitted, you can see the recognizer is being trained on Comprehend Console. \n",
    "This will take approxiamately 20 minutes to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_model_response = comprehend_client.describe_entity_recognizer(\n",
    "    EntityRecognizerArn= comprehend_custom_recognizer_response['EntityRecognizerArn']\n",
    ")\n",
    "\n",
    "print('ARN:\\t', comprehend_model_response['EntityRecognizerProperties']['EntityRecognizerArn'])\n",
    "print('Training Job Status:\\t', comprehend_model_response['EntityRecognizerProperties']['Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance <a class=\"anchor\" id=\"Model-Performance\"></a>\n",
    "\n",
    "In the training, Comprehend will divide the dataset into training documents and test documents. Once the recognizer is trained, you can see the recognizer’s overall performance, as well as the performance for each entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comprehend_model_response['EntityRecognizerProperties']['Status'] == 'TRAINED':\n",
    "    print('Number of Document Trained:\\t', comprehend_model_response['EntityRecognizerProperties']['RecognizerMetadata']['NumberOfTrainedDocuments'])\n",
    "    print('Number of Document Tested:\\t', comprehend_model_response['EntityRecognizerProperties']['RecognizerMetadata']['NumberOfTestDocuments'])\n",
    "    print('\\n-------------- Evaluation Metrics: ----------------')\n",
    "    print('Precision:\\t', comprehend_model_response['EntityRecognizerProperties']['RecognizerMetadata']['EvaluationMetrics']['Precision'])\n",
    "    print('ReCall:\\t\\t', comprehend_model_response['EntityRecognizerProperties']['RecognizerMetadata']['EvaluationMetrics']['Recall'])\n",
    "    print('F1 Score:\\t', comprehend_model_response['EntityRecognizerProperties']['RecognizerMetadata']['EvaluationMetrics']['F1Score'])\n",
    "else:\n",
    "    print('Please wait for previous step to be completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Next, we have prepared a small sample of text to test out our newly trained custom entity recognizer. First, we will upload the document onto S3 and start a custom recognizer job. Once the job is submitted, you can see the progress in console under Amazon Comprehend → Analysis Jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Test PDF resumes to S3 for OCR++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfResumeFileList = glob.glob(\"./test_document/*.pdf\")\n",
    "prefix_resume_pdf = prefix + \"/test_document/\"\n",
    "\n",
    "for filePath in tqdm(pdfResumeFileList):\n",
    "    file_name = os.path.basename(filePath)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix_resume_pdf, file_name)).upload_file(filePath)\n",
    "\n",
    "resume_pdf_bucket_name = 's3://'+bucket+'/'+prefix+'/'+'test_document/'\n",
    "print('Uploaded Resume PDFs :\\t', resume_pdf_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing OCR++ Using Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_object_list = []\n",
    "pdf_object_list.append(prefix_resume_pdf+\"test_document.pdf\")\n",
    "\n",
    "output_dir = './test_document/'\n",
    "\n",
    "for file_obj in tqdm(pdf_object_list):\n",
    "    print('Textract Processing PDF: \\t'+ file_obj)             \n",
    "    job_id = StartDocumentTextDetection(bucket, file_obj)\n",
    "    print('Textract Job Submitted: \\t'+ job_id)\n",
    "    response = getDocumentTextDetection(job_id)\n",
    "    \n",
    "    # renaming .pdf to .text\n",
    "    text_output_name = file_obj.replace('.pdf', '.txt')\n",
    "    text_output_name = text_output_name[(text_output_name.rfind('/')+1):]\n",
    "    print('Output Name:\\t', text_output_name)\n",
    "    \n",
    "    \n",
    "    # Writing Textract Output to Text Files:\n",
    "    with codecs.open(output_dir + text_output_name, \"w\", \"utf-8\") as output_file:\n",
    "        for item in response[\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                print('\\033[94m' + item[\"Text\"] + '\\033[0m')\n",
    "                output_file.write(item[\"Text\"]+'\\n')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the Textract Result for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading test document onto S3:\n",
    "test_document = './test_document/test_document.txt'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix+'/test_document/', 'test_document.txt')).upload_file(test_document)\n",
    "\n",
    "s3_test_document = 's3://' + bucket+'/'+prefix+'/test_document/'+'test_document.txt'\n",
    "s3_test_document_output = 's3://' + bucket+'/'+prefix+'/test_document/'\n",
    "print('Test Document Input: ', s3_test_document)\n",
    "print('Test Document Output: ', s3_test_document_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a recognizer Job:\n",
    "custom_recognizer_job_name = 'recognizer-job-'+ str(int(time.time()))\n",
    "\n",
    "recognizer_response = comprehend_client.start_entities_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_test_document,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_test_document_output\n",
    "    },\n",
    "    DataAccessRoleArn=role,\n",
    "    JobName=custom_recognizer_job_name,\n",
    "    EntityRecognizerArn=comprehend_model_response['EntityRecognizerProperties']['EntityRecognizerArn'],\n",
    "    LanguageCode='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use follow code to check if the Detection Job for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response = comprehend_client.describe_entities_detection_job(\n",
    "    JobId=recognizer_response['JobId']\n",
    ")\n",
    "print('Detection Job Name:\\t', job_response['EntitiesDetectionJobProperties']['JobName'])\n",
    "print('Detection Job ID:\\t', job_response['EntitiesDetectionJobProperties']['JobId'])\n",
    "print('Detection Job Status:\\t', job_response['EntitiesDetectionJobProperties']['JobStatus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_url=job_response['EntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print('S3 Output URL:\\t', output_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Once the Analysis job is done, you can download the output and see the results. Here we converted the json result into table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "#create dir for output file:\n",
    "!mkdir test_document_output\n",
    "\n",
    "# Downloading Output File\n",
    "if job_response['EntitiesDetectionJobProperties']['JobStatus'] == 'COMPLETED':\n",
    "    filename = './test_document_output/output.tar.gz'\n",
    "    output_url_o = urlparse(output_url, allow_fragments=False)\n",
    "    s3_client.download_file(output_url_o.netloc, output_url_o.path.lstrip('/'), filename)\n",
    "\n",
    "    !cd test_document_output; tar -xvzf output.tar.gz\n",
    "    \n",
    "    print(\"Output downloaded ... \")\n",
    "else:\n",
    "    print(\"Please wait for the analysis job to be completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "output_file_name = './test_document_output/output'\n",
    "data = [['Start Offset', 'End Offset', 'Confidence', 'Text', 'Type']]\n",
    "\n",
    "with open(output_file_name, 'r', encoding='utf-8') as input_file:\n",
    "    for line in input_file.readlines():\n",
    "        json_line = json.loads(line)  # converting line of text into JSON\n",
    "        entities = json_line['Entities']\n",
    "        if(len(entities)>0):\n",
    "            for entry in entities:\n",
    "                entry_data = [entry['BeginOffset'], entry['EndOffset'], entry['Score'], entry['Text'],entry['Type']]\n",
    "                data.append(entry_data)\n",
    "        \n",
    "display(HTML(\n",
    "   '<table><tr>{}</tr></table>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "       )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
