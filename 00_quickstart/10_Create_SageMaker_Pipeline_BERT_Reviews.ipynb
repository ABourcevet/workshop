{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Pipeline\n",
    "\n",
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and model registration:\n",
    "\n",
    "![A typical ML Application pipeline](img/pipeline-full.png)\n",
    "\n",
    "### Create SageMaker Clients and Session\n",
    "\n",
    "First, we create a new SageMaker Session in the current AWS region. We also acquire the role arn for the session.\n",
    "\n",
    "This role arn should be the execution role arn that you set up in the Prerequisites section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the Pipeline as an `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "pipeline_name = 'BERT-pipeline-{}'.format(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup an Experiment to Track our Pipeline\n",
    "\n",
    "**Experiment**: A collection of related Trials.  Add Trials to an Experiment that you wish to compare together.\n",
    "\n",
    "**Trial**: A description of a multi-step machine learning workflow. Each step in the workflow is described by a Trial Component. There is no relationship between Trial Components such as ordering.\n",
    "\n",
    "**Trial Component**: A description of a single step in a machine learning workflow. For example data cleaning, feature extraction, model training, model evaluation, etc.\n",
    "\n",
    "**Tracker**: A logger of information about a single TrialComponent.\n",
    "\n",
    "<img src=\"img/sagemaker-experiments.png\" width=\"90%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline experiment name: BERT-pipeline-1612936551\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "pipeline_experiment = Experiment.create(\n",
    "                experiment_name=pipeline_name,\n",
    "                description='Amazon Customer Reviews BERT Pipeline Experiment', \n",
    "                sagemaker_boto_client=sm)\n",
    "\n",
    "pipeline_experiment_name = pipeline_experiment.experiment_name\n",
    "print('Pipeline experiment name: {}'.format(pipeline_experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_experiment_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1612936551\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "pipeline_trial = Trial.create(trial_name='trial-{}'.format(timestamp),\n",
    "                     experiment_name=pipeline_experiment_name,\n",
    "                     sagemaker_boto_client=sm)\n",
    "\n",
    "pipeline_trial_name = pipeline_trial.trial_name\n",
    "print('Trial name: {}'.format(pipeline_trial_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "We define Workflow Parameters by which we can parametrize our Pipeline and vary the values injected and used in Pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - representing a `str` Python type\n",
    "* `ParameterInteger` - representing an `int` Python type\n",
    "* `ParameterFloat` - representing a `float` Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow below include:\n",
    "\n",
    "* `processing_instance_type` - The `ml.*` instance type of the processing job.\n",
    "* `processing_instance_count` - The instance count of the processing job. For illustrative purposes only: 1 is the only value that makes sense here.\n",
    "* `training_instance_type` - The `ml.*` instance type of the training job.\n",
    "* `model_approval_status` - What approval status to register the trained model with for CI/CD purposes. Defaults to \"PendingManualApproval\". (NOTE: not available in service yet)\n",
    "* `input_data` - The URL location of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = ParameterString(\n",
    "    name=\"ExperimentName\",\n",
    "    default_value=pipeline_experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Step Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-106038154054/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-10 05:29:59   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      "2021-02-10 05:30:00   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\n",
      "2021-02-10 05:30:01   12134676 amazon_reviews_us_Gift_Card_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_input_data_s3_uri,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.c5.2xlarge\"\n",
    ")\n",
    "\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=64,\n",
    ")\n",
    "\n",
    "balance_dataset = ParameterString(\n",
    "    name=\"BalanceDataset\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "    \n",
    "train_split_percentage = ParameterFloat(\n",
    "    name=\"TrainSplitPercentage\",\n",
    "    default_value=0.90,\n",
    ")\n",
    "\n",
    "validation_split_percentage = ParameterFloat(\n",
    "    name=\"ValidationSplitPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "test_split_percentage = ParameterFloat(\n",
    "    name=\"TestSplitPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "feature_store_offline_prefix = ParameterString(\n",
    "    name=\"FeatureStoreOfflinePrefix\",\n",
    "    default_value=\"reviews-feature-store-\" + str(timestamp),\n",
    ")\n",
    "\n",
    "feature_group_name = ParameterString(\n",
    "    name=\"FeatureGroupName\",\n",
    "    default_value=\"reviews-feature-group-\" + str(timestamp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\n",
      "\u001b[37m# This is 2.3.0 (vs. 2.3.1 everywhere else) because we need to \u001b[39;49;00m\n",
      "\u001b[37m# use anaconda and anaconda only supports 2.3.0 at this time\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33manaconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.3.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker==2.24.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_group\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m FeatureGroup\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_definition\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    FeatureDefinition,\n",
      "    FeatureTypeEnum,\n",
      ")\n",
      "\n",
      "region = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRegion: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(region))\n",
      "\n",
      "\u001b[37m#############################\u001b[39;49;00m\n",
      "\u001b[37m## We may need to get the Role and Bucket before setting sm, featurestore_runtime, etc.\u001b[39;49;00m\n",
      "\u001b[37m## Role and Bucket are malformed if we do this later.\u001b[39;49;00m\n",
      "sts = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "caller_identity = sts.get_caller_identity()\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcaller_identity: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(caller_identity))\n",
      "\n",
      "assumed_role_arn = caller_identity[\u001b[33m'\u001b[39;49;00m\u001b[33mArn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m(assumed_role) caller_identity_arn: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(assumed_role_arn))\n",
      "\n",
      "assumed_role_name = assumed_role_arn.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[-\u001b[34m2\u001b[39;49;00m]\n",
      "\n",
      "iam = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33miam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "get_role_response = iam.get_role(RoleName=assumed_role_name) \n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mget_role_response \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(get_role_response))\n",
      "role = get_role_response[\u001b[33m'\u001b[39;49;00m\u001b[33mRole\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mArn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrole \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(role))\n",
      "\n",
      "bucket = sagemaker.Session().default_bucket()\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThe DEFAULT BUCKET is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(bucket))\n",
      "\u001b[37m#############################\u001b[39;49;00m\n",
      "\n",
      "sm = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "featurestore_runtime = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-featurestore-runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "s3 = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name=region), \n",
      "                                      sagemaker_client=sm,\n",
      "                                      sagemaker_featurestore_runtime_client=featurestore_runtime)\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "REVIEW_BODY_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "REVIEW_ID_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\n",
      "\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "    \n",
      "label_map = {}\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\n",
      "    label_map[label] = i\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m'\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\n",
      "            \n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mwait_for_feature_group_creation_complete\u001b[39;49;00m(feature_group):\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group status: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(status))\n",
      "        \u001b[34mwhile\u001b[39;49;00m status == \u001b[33m\"\u001b[39;49;00m\u001b[33mCreating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for Feature Group Creation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            time.sleep(\u001b[34m5\u001b[39;49;00m)\n",
      "            status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group status: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(status))\n",
      "        \u001b[34mif\u001b[39;49;00m status != \u001b[33m\"\u001b[39;49;00m\u001b[33mCreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group status: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(status))\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFailed to create feature group \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroup \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m successfully created.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNo feature group created yet.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "            \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_or_load_feature_group\u001b[39;49;00m(prefix, feature_group_name):\n",
      "\n",
      "    \u001b[37m# Feature Definitions for our records\u001b[39;49;00m\n",
      "    feature_definitions= [\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\n",
      "\u001b[37m#        FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING)            \n",
      "    ]\n",
      "    \n",
      "    feature_group = FeatureGroup(\n",
      "        name=feature_group_name,\n",
      "        feature_definitions=feature_definitions,\n",
      "        sagemaker_session=sagemaker_session)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group))\n",
      "    \n",
      "    \u001b[34mtry\u001b[39;49;00m:                \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for existing Feature Group to become available if it is being created by another instance in our cluster...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        wait_for_feature_group_creation_complete(feature_group)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mBefore CREATE FG wait exeption: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(e))\n",
      "\u001b[37m#        pass\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        record_identifier_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        event_time_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group with role \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(role))\n",
      "        feature_group.create(\n",
      "            s3_uri=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{prefix}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            record_identifier_name=record_identifier_feature_name,\n",
      "            event_time_feature_name=event_time_feature_name,\n",
      "            role_arn=role,\n",
      "            enable_online_store=\u001b[34mTrue\u001b[39;49;00m\n",
      "        )\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group. Completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for new Feature Group to become available...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        wait_for_feature_group_creation_complete(feature_group)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)  \n",
      "        feature_group.describe()\n",
      "        \n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mException: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(e))\n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m feature_group\n",
      "\n",
      "    \n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\n",
      "               input_ids,\n",
      "               input_mask,\n",
      "               segment_ids,\n",
      "               label_id,\n",
      "               review_id,\n",
      "               date,\n",
      "               label):\n",
      "\u001b[37m#               review_body):\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\u001b[37m#        self.review_body = review_body\u001b[39;49;00m\n",
      "    \n",
      "    \n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\n",
      "    \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "    \u001b[36mself\u001b[39;49;00m.date = date\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\n",
      "    \n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    tokens = tokenizer.tokenize(the_input.text)    \n",
      "\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    encode_plus_tokens = tokenizer.encode_plus(the_input.text,\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                               max_length=max_seq_length,\n",
      "\u001b[37m#                                               truncation=True\u001b[39;49;00m\n",
      "                                              )\n",
      "\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\n",
      "\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\n",
      "    label_id = label_map[the_input.label]\n",
      "\n",
      "    features = InputFeatures(\n",
      "        input_ids=input_ids,\n",
      "        input_mask=input_mask,\n",
      "        segment_ids=segment_ids,\n",
      "        label_id=label_id,\n",
      "        review_id=the_input.review_id,\n",
      "        date=the_input.date,\n",
      "        label=the_input.label)\n",
      "\u001b[37m#        review_body=the_input.text)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\n",
      "\u001b[37m#     print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\n",
      "\u001b[37m#     print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\n",
      "\u001b[37m#     print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\n",
      "\u001b[37m#     print('**review_id**\\n{}\\n'.format(features.review_id))\u001b[39;49;00m\n",
      "\u001b[37m#     print('**date**\\n{}\\n'.format(features.date))\u001b[39;49;00m\n",
      "\u001b[37m#     print('**label**\\n{}\\n'.format(features.label))\u001b[39;49;00m\n",
      "\u001b[37m#    print('**review_body**\\n{}\\n'.format(features.review_body))\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs,\n",
      "                                 output_file,\n",
      "                                 max_seq_length):\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    records = []\n",
      "\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\n",
      "\n",
      "        features = convert_input(the_input, max_seq_length)\n",
      "\n",
      "        all_features = collections.OrderedDict()\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
      "\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\n",
      "\n",
      "        records.append({\u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_ids,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_mask,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.segment_ids,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label_id,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.review_id,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.date,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label,\n",
      "\u001b[37m#                        'review_body': features.review_body\u001b[39;49;00m\n",
      "                       })\n",
      "\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\n",
      "        \u001b[37m#####################################            \u001b[39;49;00m\n",
      "        \u001b[37m# break\u001b[39;49;00m\n",
      "        \n",
      "    tf_record_writer.close()\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m records\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "        default=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )  \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    ) \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    ) \n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, \n",
      "                               max_seq_length, \n",
      "                               balance_dataset,\n",
      "                               prefix,\n",
      "                               feature_group_name):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(file))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(balance_dataset))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(prefix)) \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group_name))    \n",
      "\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\n",
      "    feature_group = create_or_load_feature_group(prefix, feature_group_name)\n",
      "    \n",
      "    filename_without_extension = Path(Path(file).stem).stem\n",
      "\n",
      "    df = pd.read_csv(file, \n",
      "                     delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                     quoting=csv.QUOTE_NONE,\n",
      "                     compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    df.isna().values.any()\n",
      "    df = df.dropna()\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:  \n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\n",
      "        five_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        four_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        three_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        two_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        one_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        minority_count = \u001b[36mmin\u001b[39;49;00m(five_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             four_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             three_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             two_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             one_star_df.shape[\u001b[34m0\u001b[39;49;00m]) \n",
      "\n",
      "        five_star_df = resample(five_star_df,\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                n_samples = minority_count,\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        four_star_df = resample(four_star_df,\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                n_samples = minority_count,\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        three_star_df = resample(three_star_df,\n",
      "                                 replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                 n_samples = minority_count,\n",
      "                                 random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        two_star_df = resample(two_star_df,\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                               n_samples = minority_count,\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        one_star_df = resample(one_star_df,\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                               n_samples = minority_count,\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\n",
      "\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_balanced.shape))\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\n",
      "\n",
      "        df = df_balanced\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.train_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.validation_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.test_split_percentage))    \n",
      "    \n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(holdout_percentage))\n",
      "    df_train, df_holdout = train_test_split(df, \n",
      "                                            test_size=holdout_percentage, \n",
      "                                            stratify=df[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_holdout_percentage))\n",
      "    df_validation, df_test = train_test_split(df_holdout, \n",
      "                                              test_size=test_holdout_percentage,\n",
      "                                              stratify=df_holdout[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_validation.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_test.shape))\n",
      "\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\n",
      "\n",
      "    train_inputs = df_train.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "                                    label = x[LABEL_COLUMN],\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\n",
      "                                    date = timestamp\n",
      "                            ),\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    validation_inputs = df_validation.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "                                    label = x[LABEL_COLUMN],\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\n",
      "                                    date = timestamp\n",
      "                            ),\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    test_inputs = df_test.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "                                    label = x[LABEL_COLUMN],\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\n",
      "                                    date = timestamp\n",
      "                            ),\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\n",
      "    train_records = transform_inputs_to_tfrecord(train_inputs, \n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension), \n",
      "                                                         max_seq_length)\n",
      "\n",
      "    validation_records = transform_inputs_to_tfrecord(validation_inputs, \n",
      "                                                              \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension), \n",
      "                                                              max_seq_length)\n",
      "\n",
      "    test_records = transform_inputs_to_tfrecord(test_inputs, \n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension), \n",
      "                                                        max_seq_length)    \n",
      "                \n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\n",
      "    df_train_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    df_train_records.head()   \n",
      "    \n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\n",
      "    df_validation_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \n",
      "    df_validation_records.head()   \n",
      "\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\n",
      "    df_test_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \n",
      "    df_test_records.head()   \n",
      "    \n",
      "    \u001b[37m# Add record to feature store    \u001b[39;49;00m\n",
      "    df_fs_train_records = cast_object_to_string(df_train_records)\n",
      "    df_fs_validation_records = cast_object_to_string(df_validation_records)\n",
      "    df_fs_test_records = cast_object_to_string(df_test_records)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mIngesting Features...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    feature_group.ingest(\n",
      "        data_frame=df_fs_train_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )        \n",
      "    feature_group.ingest(\n",
      "        data_frame=df_fs_validation_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )        \n",
      "    feature_group.ingest(\n",
      "        data_frame=df_fs_test_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )            \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature ingest completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\n",
      "    \n",
      "    feature_group = create_or_load_feature_group(prefix=args.feature_store_offline_prefix, \n",
      "                                                         feature_group_name=args.feature_group_name)\n",
      "\n",
      "    feature_group.describe()\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(feature_group.as_hive_ddl())\n",
      "    \n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    \n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
      "                                                  max_seq_length=args.max_seq_length,\n",
      "                                                  balance_dataset=args.balance_dataset,\n",
      "                                                  prefix=args.feature_store_offline_prefix,\n",
      "                                                  feature_group_name=args.feature_group_name)\n",
      "\n",
      "    input_files = glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\n",
      "\n",
      "    num_cpus = multiprocessing.cpu_count()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_cpus))\n",
      "\n",
      "    p = multiprocessing.Pool(num_cpus)\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\n",
      "    dirs_output = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    dirs_output = os.listdir(train_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\n",
      "    dirs_output = os.listdir(validation_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))\n",
      "    dirs_output = os.listdir(test_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "        \n",
      "    offline_store_contents = \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m (offline_store_contents \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m):\n",
      "        objects_in_bucket = s3.list_objects(Bucket=bucket,\n",
      "                                            Prefix=args.feature_store_offline_prefix)\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m objects_in_bucket \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) > \u001b[34m1\u001b[39;49;00m):\n",
      "            offline_store_contents = objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for data in offline store...\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            sleep(\u001b[34m60\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mData available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of an `SKLearnProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "We also specify the `framework_version` we will use throughout.\n",
    "\n",
    "Note the `processing_instance_type` and `processing_instance_count` parameters that used by the processor instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(framework_version='0.23-1',\n",
    "                             role=role,\n",
    "                             instance_type=processing_instance_type,\n",
    "                             instance_count=processing_instance_count,\n",
    "                             env={'AWS_DEFAULT_REGION': region},                             \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='Processing', step_type=<StepTypeEnum.PROCESSING: 'Processing'>)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name='raw-input-data',\n",
    "            source=input_data,\n",
    "            destination='/opt/ml/processing/input/data/',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "]\n",
    "\n",
    "processing_outputs=[\n",
    "        ProcessingOutput(output_name='bert-train',\n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/bert/train',\n",
    "                        ),\n",
    "        ProcessingOutput(output_name='bert-validation',\n",
    "                         s3_upload_mode='EndOfJob',                         \n",
    "                         source='/opt/ml/processing/output/bert/validation',\n",
    "                        ),\n",
    "        ProcessingOutput(output_name='bert-test',\n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/bert/test',\n",
    "                        ),\n",
    "]        \n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Processing', \n",
    "    code='preprocess-scikit-text-to-bert-feature-store.py',\n",
    "    processor=processor,\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=['--train-split-percentage', str(train_split_percentage.default_value),                   \n",
    "                   '--validation-split-percentage', str(validation_split_percentage.default_value),\n",
    "                   '--test-split-percentage', str(test_split_percentage.default_value),\n",
    "                   '--max-seq-length', str(max_seq_length.default_value),\n",
    "                   '--balance-dataset', str(balance_dataset.default_value),\n",
    "                   '--feature-store-offline-prefix', str(feature_store_offline_prefix.default_value),\n",
    "                   '--feature-group-name', str(feature_group_name.default_value)\n",
    "                  ]\n",
    ")        \n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Processing Step for Feature Engineering](img/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "Note the `input_data` parameters passed into `ProcessingStep` as the input data of the step itself. This input data will be used by the processor instance when it is run.\n",
    "\n",
    "Also, take note the `\"bert-train\"`, `\"bert-validation\"` and `\"bert-test\"` named channels specified in the output configuration for the processing job. Such step `Properties` can be used in subsequent steps and will resolve to their runtime values at execution. In particular, we'll call out this usage when we define our training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/train_model.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.c5.9xlarge\"\n",
    ")\n",
    "\n",
    "train_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Hyper-Parameters\n",
    "Note that `max_seq_length` is re-used from the processing hyper-parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = ParameterInteger(\n",
    "    name=\"Epochs\",\n",
    "    default_value=1\n",
    ")\n",
    "    \n",
    "learning_rate = ParameterFloat(\n",
    "    name=\"LearningRate\",\n",
    "    default_value=0.00001\n",
    ") \n",
    "    \n",
    "epsilon = ParameterFloat(\n",
    "    name=\"Epsilon\",\n",
    "    default_value=0.00000001\n",
    ")\n",
    "        \n",
    "train_batch_size = ParameterInteger(\n",
    "    name=\"TrainBatchSize\",\n",
    "    default_value=128\n",
    ")\n",
    "    \n",
    "validation_batch_size = ParameterInteger(\n",
    "    name=\"ValidationBatchSize\",\n",
    "    default_value=128\n",
    ")\n",
    "    \n",
    "test_batch_size = ParameterInteger(\n",
    "    name=\"TestBatchSize\",\n",
    "    default_value=128\n",
    ")\n",
    "    \n",
    "train_steps_per_epoch = ParameterInteger(\n",
    "    name=\"TrainStepsPerEpoch\",\n",
    "    default_value=50\n",
    ")\n",
    "    \n",
    "validation_steps = ParameterInteger(\n",
    "    name=\"ValidationSteps\",\n",
    "    default_value=50\n",
    ")\n",
    "    \n",
    "test_steps = ParameterInteger(\n",
    "    name=\"TestSteps\",\n",
    "    default_value=50\n",
    ")\n",
    "    \n",
    "train_volume_size = ParameterInteger(\n",
    "    name=\"TrainVolumeSize\",\n",
    "    default_value=1024\n",
    ") \n",
    "    \n",
    "use_xla = ParameterString(\n",
    "    name=\"UseXLA\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "\n",
    "use_amp = ParameterString(\n",
    "    name=\"UseAMP\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "    \n",
    "freeze_bert_layer = ParameterString(\n",
    "    name=\"FreezeBERTLayer\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "enable_sagemaker_debugger = ParameterString(\n",
    "    name=\"EnableSageMakerDebugger\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "    \n",
    "enable_checkpointing = ParameterString(\n",
    "    name=\"EnableCheckpointing\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "enable_tensorboard = ParameterString(\n",
    "    name=\"EnableTensorboard\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "    \n",
    "input_mode = ParameterString(\n",
    "    name=\"InputMode\",\n",
    "    default_value=\"File\",\n",
    ")\n",
    "\n",
    "run_validation = ParameterString(\n",
    "    name=\"RunValidation\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "\n",
    "run_test = ParameterString(\n",
    "    name=\"RunTest\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "    \n",
    "run_sample_predictions = ParameterString(\n",
    "    name=\"RunSamplePredictions\",\n",
    "    default_value=\"False\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\u001b[39;49;00m\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.9.3'])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertModel\n",
      "\u001b[37m#from transformers import TFBertForSequenceClassification\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcallbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\n",
      "\u001b[37m#from tensorflow.keras.mixed_precision import experimental as mixed_precision\u001b[39;49;00m\n",
      "\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\n",
      "    x = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    }\n",
      "\n",
      "    y = record[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(channel,\n",
      "                                     input_filenames,\n",
      "                                     pipe_mode,\n",
      "                                     is_training,\n",
      "                                     drop_remainder,\n",
      "                                     batch_size,\n",
      "                                     epochs,\n",
      "                                     steps_per_epoch,\n",
      "                                     max_seq_length):\n",
      "\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\n",
      "        dataset = PipeModeDataset(channel=channel,\n",
      "                                  record_format=\u001b[33m'\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using input_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_filenames))\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
      "\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * \u001b[34m100\u001b[39;49;00m)\n",
      "\u001b[37m#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49;00m\n",
      "\n",
      "    name_to_features = {\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\n",
      "        \u001b[37m# TODO:  wip/bert/bert_attention_head_view/train.py\u001b[39;49;00m\n",
      "        \u001b[37m# Convert input_ids into input_tokens with DistilBert vocabulary \u001b[39;49;00m\n",
      "        \u001b[37m#  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\u001b[39;49;00m\n",
      "        \u001b[37m#    hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\n",
      "    \n",
      "    dataset = dataset.apply(\n",
      "        tf.data.experimental.map_and_batch(\n",
      "          \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\n",
      "          batch_size=batch_size,\n",
      "          drop_remainder=drop_remainder,\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
      "\n",
      "\u001b[37m#    dataset.cache()\u001b[39;49;00m\n",
      "\n",
      "    dataset = dataset.shuffle(buffer_size=\u001b[34m1000\u001b[39;49;00m,\n",
      "                              reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    row_count = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**************** \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****************\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m dataset.as_numpy_iterator():\n",
      "        \u001b[36mprint\u001b[39;49;00m(row)\n",
      "        \u001b[34mif\u001b[39;49;00m row_count == \u001b[34m5\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        row_count = row_count + \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_checkpoint_model\u001b[39;49;00m(checkpoint_path):\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "    \n",
      "    glob_pattern = os.path.join(checkpoint_path, \u001b[33m'\u001b[39;49;00m\u001b[33m*.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mglob pattern \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(glob_pattern))\n",
      "\n",
      "    list_of_checkpoint_files = glob.glob(glob_pattern)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mList of checkpoint files \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(list_of_checkpoint_files))\n",
      "    \n",
      "    latest_checkpoint_file = \u001b[36mmax\u001b[39;49;00m(list_of_checkpoint_files)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLatest checkpoint file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(latest_checkpoint_file))\n",
      "\n",
      "    initial_epoch_number_str = latest_checkpoint_file.rsplit(\u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m].split(\u001b[33m'\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "    initial_epoch_number = \u001b[36mint\u001b[39;49;00m(initial_epoch_number_str)\n",
      "\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
      "                                               latest_checkpoint_file,\n",
      "                                               config=config)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mloaded_model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(loaded_model))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minitial_epoch_number \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(initial_epoch_number))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m loaded_model, initial_epoch_number\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_base_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.00003\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.00000001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_sagemaker_debugger\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)        \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_checkpointing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# This is unused\u001b[39;49;00m\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# This points to the S3 location - this should not be used by our code\u001b[39;49;00m\n",
      "    \u001b[37m# We should use /opt/ml/model/ instead\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument('--model_dir', \u001b[39;49;00m\n",
      "    \u001b[37m#                     type=str, \u001b[39;49;00m\n",
      "    \u001b[37m#                     default=os.environ['SM_MODEL_DIR'])\u001b[39;49;00m\n",
      "     \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \n",
      "    env_var = os.environ \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(env_var[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    sm_training_env_json = json.loads(env_var[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    is_master = sm_training_env_json[\u001b[33m'\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mis_master \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(is_master))\n",
      "    \n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    validation_data = args.validation_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\n",
      "    test_data = args.test_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))    \n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\n",
      "    job_name = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSAGEMAKER_JOB_NAME\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mjob_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(job_name))    \n",
      "    use_xla = args.use_xla\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_xla \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_xla))    \n",
      "    use_amp = args.use_amp\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))    \n",
      "    max_seq_length = args.max_seq_length\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))    \n",
      "    train_batch_size = args.train_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_batch_size))    \n",
      "    validation_batch_size = args.validation_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_batch_size))    \n",
      "    test_batch_size = args.test_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_batch_size))    \n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))    \n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))    \n",
      "    epsilon = args.epsilon\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepsilon \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epsilon))    \n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_steps_per_epoch))    \n",
      "    validation_steps = args.validation_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_steps))    \n",
      "    test_steps = args.test_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_steps))    \n",
      "    freeze_bert_layer = args.freeze_bert_layer\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfreeze_bert_layer \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(freeze_bert_layer))    \n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))    \n",
      "    run_validation = args.run_validation\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_validation \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_validation))    \n",
      "    run_test = args.run_test\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_test \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_test))    \n",
      "    run_sample_predictions = args.run_sample_predictions\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_sample_predictions \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_sample_predictions))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_tensorboard))       \n",
      "    enable_checkpointing = args.enable_checkpointing\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_checkpointing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_checkpointing))    \n",
      "\n",
      "    checkpoint_base_path = args.checkpoint_base_path\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcheckpoint_base_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_base_path))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_master:\n",
      "        checkpoint_path = checkpoint_base_path\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        checkpoint_path = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcheckpoint_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_path))\n",
      "    \n",
      "    \u001b[37m# Determine if PipeMode is enabled \u001b[39;49;00m\n",
      "    pipe_mode_str = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    pipe_mode = (pipe_mode_str.find(\u001b[33m'\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing pipe_mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(pipe_mode))\n",
      " \n",
      "    \u001b[37m# Model Output \u001b[39;49;00m\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Tensorboard Logs \u001b[39;49;00m\n",
      "    tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Commented out due to incompatibility with transformers library (possibly)\u001b[39;49;00m\n",
      "    \u001b[37m# Set the global precision mixed_precision policy to \"mixed_float16\"    \u001b[39;49;00m\n",
      "\u001b[37m#    mixed_precision_policy = 'mixed_float16'\u001b[39;49;00m\n",
      "\u001b[37m#    print('Mixed precision policy {}'.format(mixed_precision_policy))\u001b[39;49;00m\n",
      "\u001b[37m#    policy = mixed_precision.Policy(mixed_precision_policy)\u001b[39;49;00m\n",
      "\u001b[37m#    mixed_precision.set_policy(policy)    \u001b[39;49;00m\n",
      "    \n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\n",
      "    \u001b[37m# Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\u001b[39;49;00m\n",
      "    \u001b[37m#distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\n",
      "        tf.config.optimizer.set_jit(use_xla)\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\n",
      "\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data_filenames))\n",
      "        train_dataset = file_based_input_dataset_builder(\n",
      "            channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "            input_filenames=train_data_filenames,\n",
      "            pipe_mode=pipe_mode,\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            batch_size=train_batch_size,\n",
      "            epochs=epochs,\n",
      "            steps_per_epoch=train_steps_per_epoch,\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "\n",
      "        tokenizer = \u001b[34mNone\u001b[39;49;00m\n",
      "        config = \u001b[34mNone\u001b[39;49;00m\n",
      "        model = \u001b[34mNone\u001b[39;49;00m\n",
      "        transformer_model = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# This is required when launching many instances at once...  the urllib request seems to get denied periodically\u001b[39;49;00m\n",
      "        successful_download = \u001b[34mFalse\u001b[39;49;00m\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m (retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download):\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "                                                          id2label={\n",
      "                                                            \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\n",
      "                                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\n",
      "                                                          },\n",
      "                                                          label2id={\n",
      "                                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
      "                                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                                            \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\n",
      "                                                          })\n",
      "\n",
      "                transformer_model = TFDistilBertModel.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                                                      config=config)\n",
      "\n",
      "                input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "\n",
      "                embedding_layer = transformer_model.distilbert(input_ids, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "                X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\u001b[34m50\u001b[39;49;00m, return_sequences=\u001b[34mTrue\u001b[39;49;00m, dropout=\u001b[34m0.1\u001b[39;49;00m, recurrent_dropout=\u001b[34m0.1\u001b[39;49;00m))(embedding_layer)\n",
      "                X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "                X = tf.keras.layers.Dense(\u001b[34m50\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "                X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "                X = tf.keras.layers.Dense(\u001b[36mlen\u001b[39;49;00m(CLASSES), activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "\n",
      "                model = tf.keras.Model(inputs=[input_ids, input_mask], outputs = X)\n",
      "\n",
      "                \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "                    layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "\n",
      "                successful_download = \u001b[34mTrue\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSucessfully downloaded after \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m retries.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries))\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRetry #\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.  Sleeping for \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m seconds\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries, random_sleep))\n",
      "                time.sleep(random_sleep)\n",
      "\n",
      "        callbacks = []\n",
      "\n",
      "        initial_epoch_number = \u001b[34m0\u001b[39;49;00m \n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_checkpointing:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Checkpoint enabled *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \n",
      "            os.makedirs(checkpoint_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)        \n",
      "            \u001b[34mif\u001b[39;49;00m os.listdir(checkpoint_path):\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Found checkpoint *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(checkpoint_path)\n",
      "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using checkpoint model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))\n",
      "                \n",
      "            checkpoint_callback = ModelCheckpoint(\n",
      "                    filepath=os.path.join(checkpoint_path, \u001b[33m'\u001b[39;49;00m\u001b[33mtf_model_\u001b[39;49;00m\u001b[33m{epoch:05d}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "                    save_weights_only=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                    verbose=\u001b[34m1\u001b[39;49;00m,\n",
      "                    monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mval_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** CHECKPOINT CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_callback))\n",
      "            callbacks.append(checkpoint_callback)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m** use_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))        \n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m'\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))\n",
      "        \u001b[34mif\u001b[39;49;00m enable_sagemaker_debugger:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGING ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdebug\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msmd\u001b[39;49;00m\n",
      "            \u001b[37m# This assumes that we specified debugger_hook_config\u001b[39;49;00m\n",
      "            debugger_callback = smd.KerasHook.create_from_json_file()\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGER CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(debugger_callback))            \n",
      "            callbacks.append(debugger_callback)\n",
      "            optimizer = debugger_callback.wrap_optimizer(optimizer)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_tensorboard:            \n",
      "            tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
      "                                                        log_dir=tensorboard_logs_path)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** TENSORBOARD CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorboard_callback))\n",
      "            callbacks.append(tensorboard_callback)\n",
      "  \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** OPTIMIZER \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\n",
      "        \n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCompiled model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))          \n",
      "\u001b[37m#        model.layers[0].trainable = not freeze_bert_layer\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data_filenames))\n",
      "            validation_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                input_filenames=validation_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=validation_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=validation_steps,\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "            \n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training and Validation...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\n",
      "            train_and_validation_history = model.fit(train_dataset,\n",
      "                                                     shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                     epochs=epochs,\n",
      "                                                     initial_epoch=initial_epoch_number,\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\n",
      "                                                     validation_data=validation_dataset,\n",
      "                                                     validation_steps=validation_steps,\n",
      "                                                     callbacks=callbacks)                                \n",
      "            \u001b[36mprint\u001b[39;49;00m(train_and_validation_history)\n",
      "        \u001b[34melse\u001b[39;49;00m: \u001b[37m# Not running validation\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training (Without Validation)...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            train_history = model.fit(train_dataset,\n",
      "                                      shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                      epochs=epochs,\n",
      "                                      initial_epoch=initial_epoch_number,\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\n",
      "                                      callbacks=callbacks)                \n",
      "            \u001b[36mprint\u001b[39;49;00m(train_history)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_filenames))\n",
      "            test_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                input_filenames=test_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=test_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=test_steps,\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting test...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            test_history = model.evaluate(test_dataset,\n",
      "                                          steps=test_steps,\n",
      "                                          callbacks=callbacks)\n",
      "                                 \n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest history \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_history))\n",
      "            \n",
      "        \u001b[37m# Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(transformer_fine_tuned_model_path))   \n",
      "        transformer_model.save_pretrained(transformer_fine_tuned_model_path)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel inputs after save_pretrained: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model.inputs))\n",
      "            \n",
      "        \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))   \n",
      "        model.save(tensorflow_saved_model_path,\n",
      "                   include_optimizer=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                   overwrite=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                   save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "        \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "        \u001b[37m#         This appears to be hard-coded and must be called code/\u001b[39;49;00m\n",
      "        inference_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference source files to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "        os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)               \n",
      "        os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "        \u001b[36mprint\u001b[39;49;00m(glob(inference_path))        \n",
      "\u001b[37m#        os.system('cp requirements.txt {}/code'.format(inference_path))\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[37m# Copy test data for the evaluation step\u001b[39;49;00m\n",
      "        os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp -R ./test_data/ \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(local_model_dir))\n",
      "        \n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text):\n",
      "            encode_plus_tokens = tokenizer.encode_plus(text,\n",
      "                                                       pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                       max_length=max_seq_length,\n",
      "                                                       truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                       return_tensors=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "            input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "            \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\n",
      "            input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "            outputs = model.predict(x=(input_ids, input_mask))\n",
      "\n",
      "            scores = np.exp(outputs) / np.exp(outputs).sum(-\u001b[34m1\u001b[39;49;00m, keepdims=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "            prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m scores]\n",
      "\n",
      "            \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "            \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "        df_test_reviews = pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33m./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                        delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                        quoting=csv.QUOTE_NONE,\n",
      "                                        compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "\n",
      "        df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "        df_test_reviews.shape\n",
      "        df_test_reviews.head()\n",
      "        \n",
      "        y_test = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].map(predict)\n",
      "        y_test\n",
      "        \n",
      "        y_actual = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        y_actual\n",
      "\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\n",
      "        \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\n",
      "        \n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "        accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, accuracy)\n",
      "        \n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap = plt.cm.Greens):\n",
      "            \u001b[36mprint\u001b[39;49;00m(cm)\n",
      "            plt.imshow(cm, interpolation=\u001b[33m'\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cmap=cmap)\n",
      "            plt.title(title)\n",
      "            plt.colorbar()\n",
      "            tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\n",
      "            plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\n",
      "            plt.yticks(tick_marks, classes)\n",
      "\n",
      "            fmt = \u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "            thresh = cm.max() / \u001b[34m2.\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\n",
      "                plt.text(j, i, \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\n",
      "                horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "                plt.tight_layout()\n",
      "                plt.ylabel(\u001b[33m'\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                plt.xlabel(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "\n",
      "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\n",
      "\n",
      "        plt.figure()\n",
      "        fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m,\u001b[34m5\u001b[39;49;00m))\n",
      "        plot_conf_mat(cm, \n",
      "                      classes=[\u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                      title=\u001b[33m'\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Save the confusion matrix        \u001b[39;49;00m\n",
      "        plt.show()\n",
      "        \n",
      "        \u001b[37m# Model Output         \u001b[39;49;00m\n",
      "        metrics_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        plt.savefig(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\n",
      "        \n",
      "        report_dict = {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\n",
      "                },\n",
      "            },\n",
      "        }\n",
      "\n",
      "        evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            f.write(json.dumps(report_dict))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Training Step to Train a Model\n",
    "\n",
    "We configure an Estimator and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later.\n",
    "\n",
    "We also specify the model path where the models from training will be saved.\n",
    "\n",
    "Note the `train_instance_type` parameter passed may be also used and passed into other places in the pipeline. In this case, the `train_instance_type` is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       instance_type=train_instance_type,\n",
    "                       volume_size=train_volume_size,                       \n",
    "                       py_version='py37',\n",
    "                       framework_version='2.3.1',\n",
    "                       hyperparameters={'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'train_batch_size': train_batch_size,\n",
    "                                        'validation_batch_size': validation_batch_size,\n",
    "                                        'test_batch_size': test_batch_size,                                             \n",
    "                                        'train_steps_per_epoch': train_steps_per_epoch,\n",
    "                                        'validation_steps': validation_steps,\n",
    "                                        'test_steps': test_steps,\n",
    "                                        'use_xla': use_xla,\n",
    "                                        'use_amp': use_amp,                                             \n",
    "                                        'max_seq_length': max_seq_length,\n",
    "                                        'freeze_bert_layer': freeze_bert_layer,\n",
    "                                        'enable_sagemaker_debugger': enable_sagemaker_debugger,\n",
    "                                        'enable_checkpointing': enable_checkpointing,\n",
    "                                        'enable_tensorboard': enable_tensorboard,                                        \n",
    "                                        'run_validation': run_validation,\n",
    "                                        'run_test': run_test,\n",
    "                                        'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the estimator instance to construct a `TrainingStep` as well as the `Properties` of the prior `ProcessingStep` used as input in the `TrainingStep` inputs and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to an estimator's `fit` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3Uri` of the `\"train\"`, `\"validation\"` and `\"test\"` output channel to the `TrainingStep`. The `properties` attribute of a Workflow step match the object model of the corresponding response of a describe call. These properties can be referenced as placeholder values and are resolved, or filled in, at runtime. For example, the `ProcessingStep` `properties` attribute matches the object model of the [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Train',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'bert-train'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'bert-validation'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'test': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'bert-test'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )        \n",
    "    },\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Training Step to Train a Model](img/pipeline-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Step\n",
    "\n",
    "First, we develop an evaluation script that will be specified in a Processing step that will perform the model evaluation.\n",
    "\n",
    "The evaluation script `evaluation.py` takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics such as accuracy.\n",
    "\n",
    "After pipeline execution, we will examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script:\n",
    "\n",
    "* loads in the model\n",
    "* reads in the test data\n",
    "* issues a bunch of predictions against the test data\n",
    "* builds a classification report, including accuracy\n",
    "* saves the evaluation report to the evaluation directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance of a `ScriptProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "Note the `processing_instance_type` parameter passed into the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(framework_version='0.23-1',\n",
    "                                        role=role,\n",
    "                                        instance_type=processing_instance_type,\n",
    "                                        instance_count=processing_instance_count,\n",
    "                                        env={'AWS_DEFAULT_REGION': region},\n",
    "                                        max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33manaconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.3.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "                                          id2label={\n",
      "                                            \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\n",
      "                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\n",
      "                                          },\n",
      "                                          label2id={\n",
      "                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
      "                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                            \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\n",
      "                                          })\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )  \n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minput_data: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minput_model: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model))          \n",
      "          \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of input model dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model))\n",
      "    input_files = os.listdir(args.input_model)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m input_files:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "    model_tar_path = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model)                \n",
      "    model_tar = tarfile.open(model_tar_path)\n",
      "    model_tar.extractall(args.input_model)\n",
      "    model_tar.close()                                \n",
      "\n",
      "    model = keras.models.load_model(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/tensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model))\n",
      "    \u001b[36mprint\u001b[39;49;00m(model)\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text):\n",
      "        encode_plus_tokens = tokenizer.encode_plus(text,\n",
      "                                                   pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                   max_length=args.max_seq_length,\n",
      "                                                   truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                   return_tensors=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "        input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "        \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\n",
      "        input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "        outputs = model.predict(x=(input_ids, input_mask))\n",
      "\n",
      "        scores = np.exp(outputs) / np.exp(outputs).sum(-\u001b[34m1\u001b[39;49;00m, keepdims=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m scores]\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "\n",
      "    \u001b[37m###########################################################################################\u001b[39;49;00m\n",
      "    \u001b[37m# TODO:  Replace this with glob for all files and remove test_data/ from the model.tar.gz #\u001b[39;49;00m\n",
      "    \u001b[37m###########################################################################################    \u001b[39;49;00m\n",
      "\u001b[37m#    evaluation_data_path = '/opt/ml/processing/input/data/'\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of input data dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\n",
      "    input_files = os.listdir(args.input_data)\n",
      "\n",
      "    test_data_path = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing only \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m to evaluate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_path))\n",
      "    df_test_reviews = pd.read_csv(test_data_path, \n",
      "                                  delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                  quoting=csv.QUOTE_NONE,\n",
      "                                  compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "\n",
      "    df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "    df_test_reviews.shape\n",
      "    df_test_reviews.head()\n",
      "\n",
      "    y_test = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].map(predict)\n",
      "    y_test\n",
      "\n",
      "    y_actual = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    y_actual\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\n",
      "\n",
      "    accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, accuracy)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap):\n",
      "        \u001b[36mprint\u001b[39;49;00m(cm)\n",
      "        plt.imshow(cm, interpolation=\u001b[33m'\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cmap=cmap)\n",
      "        plt.title(title)\n",
      "        plt.colorbar()\n",
      "        tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\n",
      "        plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\n",
      "        plt.yticks(tick_marks, classes)\n",
      "\n",
      "        fmt = \u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "        thresh = cm.max() / \u001b[34m2.\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\n",
      "            plt.text(j, i, \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\n",
      "            horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "            plt.tight_layout()\n",
      "            plt.ylabel(\u001b[33m'\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            plt.xlabel(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\n",
      "\n",
      "    plt.figure()\n",
      "    fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m,\u001b[34m5\u001b[39;49;00m))\n",
      "    plot_conf_mat(cm, \n",
      "                  classes=CLASSES, \n",
      "                  title=\u001b[33m'\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                  cmap=plt.cm.Greens)\n",
      "\n",
      "    \u001b[37m# Save the confusion matrix        \u001b[39;49;00m\n",
      "    plt.show()\n",
      "\n",
      "    \u001b[37m# Model Output         \u001b[39;49;00m\n",
      "    metrics_path = os.path.join(args.output_data, \u001b[33m'\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    plt.savefig(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\n",
      "\n",
      "    report_dict = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\n",
      "            },\n",
      "        },\n",
      "    }\n",
      "\n",
      "    evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        f.write(json.dumps(report_dict))\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of output dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\n",
      "    output_files = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m output_files:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of output/metrics dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\n",
      "    output_files = os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m output_files:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)    \n"
     ]
    }
   ],
   "source": [
    "!pygmentize evaluate_model_metrics.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "The `TrainingStep` and `ProcessingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) and  [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='metrics',\n",
    "    path='evaluation.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=evaluation_processor,\n",
    "    code='evaluate_model_metrics.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/input/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingInputs['raw-input-data'].S3Input.S3Uri,\n",
    "            destination='/opt/ml/processing/input/data'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='metrics', \n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/metrics/'),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "                   '--max-seq-length', str(max_seq_length.default_value),\n",
    "                  ],\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Model Evaluation Step to Evaluate the Trained Model](img/pipeline-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model_metrics.ModelMetrics object at 0x7f0e7ea335d0>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Model Step\n",
    "\n",
    "![](img/pipeline-5.png)\n",
    "\n",
    "We use the estimator instance that was used for the training step to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a Model Package. A Model Package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A Model Package Group is a collection of Model Packages. You can create a Model Package Group for a specific ML business problem, and you can keep adding versions/model packages into it. Typically, we expect customers to create a ModelPackageGroup for a SageMaker Workflow Pipeline so that they can keep adding versions/model packages to the group for every Workflow Pipeline run.\n",
    "\n",
    "The construction of `RegisterModel` is very similar to an estimator instance's `register` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3ModelArtifacts` from the `TrainingStep`, `step_train` properties. The `TrainingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) response object.\n",
    "\n",
    "Of note, we provided a specific model package group name which we will use in the Model Registry and CI/CD work later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "deploy_instance_type = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=\"ml.m5.4xlarge\"\n",
    ")\n",
    "\n",
    "deploy_instance_count = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Reviews-1612936553\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"BERT-Reviews-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=deploy_instance_type,\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "#    entry_point='inference.py', # Adds a Repack Step:  https://github.com/aws/sagemaker-python-sdk/blob/01c6ee3a9ec1831e935e86df58cf70bc92ed1bbe/src/sagemaker/workflow/_utils.py#L44\n",
    "#    source_dir='src',\n",
    "    estimator=estimator,\n",
    "    image_uri=inference_image_uri, # we have to specify, by default it's using training image\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[deploy_instance_type],\n",
    "    transform_instances=[\"ml.c5.18xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model for Deployment Step\n",
    "\n",
    "![](img/pipeline-5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'bert-model-{}'.format(timestamp)\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "create_inputs = CreateModelInput(\n",
    "    instance_type=deploy_instance_type, # \"ml.m5.4xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_step = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model,\n",
    "    inputs=create_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Condition Step to Check Accuracy and Conditionally Register Model\n",
    "\n",
    "![](img/pipeline-6.png)\n",
    "\n",
    "Finally, we'd like to only register this model if the accuracy of the model, as determined by our evaluation step `step_eval`, exceeded some value. A `ConditionStep` allows for pipelines to support conditional execution in the pipeline DAG based on conditions of step properties. \n",
    "\n",
    "Below, we:\n",
    "\n",
    "* define a `ConditionGreaterThan` on the accuracy value found in the output of the evaluation step, `step_eval`.\n",
    "* use the condition in the list of conditions in a `ConditionStep`\n",
    "* pass the `RegisterModel` step collection into the `if_steps` of the `ConditionStep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy_value = ParameterFloat(\n",
    "    name=\"MinAccuracyValue\",\n",
    "    default_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=evaluation_step,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=min_accuracy_value # accuracy\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[minimum_accuracy_condition],\n",
    "    if_steps=[register_step, create_step], # success, continue with model registration\n",
    "    else_steps=[], # fail, end the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters, Steps, and Conditions\n",
    "\n",
    "Let's tie it all up into a workflow pipeline so we can execute it, and even schedule it.\n",
    "\n",
    "A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair so we tack on the timestamp to the name.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline need not be in the order of execution. The SageMaker Workflow service will resolve the _data dependency_ DAG as steps the execution complete.\n",
    "* Steps must be unique to either pipeline step list or a single condition step if/else list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        max_seq_length,\n",
    "        balance_dataset,\n",
    "        train_split_percentage,\n",
    "        validation_split_percentage,\n",
    "        test_split_percentage,\n",
    "        feature_store_offline_prefix,\n",
    "        feature_group_name,\n",
    "        train_instance_type,\n",
    "        train_instance_count,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        epsilon,\n",
    "        train_batch_size,\n",
    "        validation_batch_size,\n",
    "        test_batch_size,\n",
    "        train_steps_per_epoch,\n",
    "        validation_steps,\n",
    "        test_steps,\n",
    "        train_volume_size,\n",
    "        use_xla,\n",
    "        use_amp,\n",
    "        freeze_bert_layer,\n",
    "        enable_sagemaker_debugger,\n",
    "        enable_checkpointing,\n",
    "        enable_tensorboard,\n",
    "        input_mode,\n",
    "        run_validation,\n",
    "        run_test,\n",
    "        run_sample_predictions,\n",
    "        min_accuracy_value,\n",
    "        model_approval_status,\n",
    "        deploy_instance_type,\n",
    "        deploy_instance_count\n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the Json of the pipeline definition that meets the SageMaker Workflow Pipeline DSL specification.\n",
    "\n",
    "By examining the definition, we're also confirming that the pipeline was well-defined, and that the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [{'DefaultValue': 's3://sagemaker-us-east-1-106038154054/amazon-reviews-pds/tsv/',\n",
      "                 'Name': 'InputData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'ProcessingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'ml.c5.2xlarge',\n",
      "                 'Name': 'ProcessingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 64, 'Name': 'MaxSeqLength', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'True',\n",
      "                 'Name': 'BalanceDataset',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.9,\n",
      "                 'Name': 'TrainSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'ValidationSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'TestSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'reviews-feature-store-1612936553',\n",
      "                 'Name': 'FeatureStoreOfflinePrefix',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'reviews-feature-group-1612936553',\n",
      "                 'Name': 'FeatureGroupName',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'ml.c5.9xlarge',\n",
      "                 'Name': 'TrainingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'TrainingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 1, 'Name': 'Epochs', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 1e-05,\n",
      "                 'Name': 'LearningRate',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 1e-08, 'Name': 'Epsilon', 'Type': 'Float'},\n",
      "                {'DefaultValue': 128,\n",
      "                 'Name': 'TrainBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 128,\n",
      "                 'Name': 'ValidationBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 128,\n",
      "                 'Name': 'TestBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 50,\n",
      "                 'Name': 'TrainStepsPerEpoch',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 50,\n",
      "                 'Name': 'ValidationSteps',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 50, 'Name': 'TestSteps', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 1024,\n",
      "                 'Name': 'TrainVolumeSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'True', 'Name': 'UseXLA', 'Type': 'String'},\n",
      "                {'DefaultValue': 'True', 'Name': 'UseAMP', 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'FreezeBERTLayer',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'EnableSageMakerDebugger',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'EnableCheckpointing',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'EnableTensorboard',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'File', 'Name': 'InputMode', 'Type': 'String'},\n",
      "                {'DefaultValue': 'True',\n",
      "                 'Name': 'RunValidation',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False', 'Name': 'RunTest', 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'RunSamplePredictions',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.01,\n",
      "                 'Name': 'MinAccuracyValue',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'PendingManualApproval',\n",
      "                 'Name': 'ModelApprovalStatus',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'ml.m5.4xlarge',\n",
      "                 'Name': 'DeployInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'DeployInstanceCount',\n",
      "                 'Type': 'Integer'}],\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--train-split-percentage',\n",
      "                                                                      '0.9',\n",
      "                                                                      '--validation-split-percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--test-split-percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--max-seq-length',\n",
      "                                                                      '64',\n",
      "                                                                      '--balance-dataset',\n",
      "                                                                      'True',\n",
      "                                                                      '--feature-store-offline-prefix',\n",
      "                                                                      'reviews-feature-store-1612936553',\n",
      "                                                                      '--feature-group-name',\n",
      "                                                                      'reviews-feature-group-1612936553'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'raw-input-data',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data/',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Parameters.InputData'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-57-798/input/code/preprocess-scikit-text-to-bert-feature-store.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-train',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/train',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-57-798/output/bert-train'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-validation',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/validation',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-57-798/output/bert-validation'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-test',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/test',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-57-798/output/bert-test'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::106038154054:role/service-role/workshop-SageMakerExecutionRole-Y10S3AO1C964'},\n",
      "            'Name': 'Processing',\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'AlgorithmSpecification': {'EnableSageMakerMetricsTimeSeries': True,\n",
      "                                                     'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                            'Regex': 'loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'train:accuracy',\n",
      "                                                                            'Regex': 'accuracy: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:loss',\n",
      "                                                                            'Regex': 'val_loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:accuracy',\n",
      "                                                                            'Regex': 'val_accuracy: '\n",
      "                                                                                     '([0-9\\\\.]+)'}],\n",
      "                                                     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37',\n",
      "                                                     'TrainingInputMode': {'Get': 'Parameters.InputMode'}},\n",
      "                          'DebugHookConfig': {'CollectionConfigurations': [],\n",
      "                                              'S3OutputPath': 's3://sagemaker-us-east-1-106038154054/'},\n",
      "                          'HyperParameters': {'enable_checkpointing': '\"False\"',\n",
      "                                              'enable_sagemaker_debugger': '\"False\"',\n",
      "                                              'enable_tensorboard': '\"False\"',\n",
      "                                              'epochs': '1',\n",
      "                                              'epsilon': '1e-08',\n",
      "                                              'freeze_bert_layer': '\"False\"',\n",
      "                                              'learning_rate': '1e-05',\n",
      "                                              'max_seq_length': '64',\n",
      "                                              'model_dir': '\"s3://sagemaker-us-east-1-106038154054/tensorflow-training-2021-02-10-05-55-58-091/model\"',\n",
      "                                              'run_sample_predictions': '\"False\"',\n",
      "                                              'run_test': '\"False\"',\n",
      "                                              'run_validation': '\"True\"',\n",
      "                                              'sagemaker_container_log_level': '20',\n",
      "                                              'sagemaker_job_name': '\"tensorflow-training-2021-02-10-05-55-58-091\"',\n",
      "                                              'sagemaker_program': '\"tf_bert_reviews.py\"',\n",
      "                                              'sagemaker_region': '\"us-east-1\"',\n",
      "                                              'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-106038154054/tensorflow-training-2021-02-10-05-55-58-091/source/sourcedir.tar.gz\"',\n",
      "                                              'test_batch_size': '128',\n",
      "                                              'test_steps': '50',\n",
      "                                              'train_batch_size': '128',\n",
      "                                              'train_steps_per_epoch': '50',\n",
      "                                              'use_amp': '\"True\"',\n",
      "                                              'use_xla': '\"True\"',\n",
      "                                              'validation_batch_size': '128',\n",
      "                                              'validation_steps': '50'},\n",
      "                          'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-train'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'validation',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-validation'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'test',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-test'].S3Output.S3Uri\"}}}}],\n",
      "                          'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-106038154054/'},\n",
      "                          'ResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'},\n",
      "                                             'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
      "                                             'VolumeSizeInGB': {'Get': 'Parameters.TrainVolumeSize'}},\n",
      "                          'RoleArn': 'arn:aws:iam::106038154054:role/service-role/workshop-SageMakerExecutionRole-Y10S3AO1C964',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 86400}},\n",
      "            'Name': 'Train',\n",
      "            'Type': 'Training'},\n",
      "           {'Arguments': {'AppSpecification': {'ContainerArguments': ['--max-seq-length',\n",
      "                                                                      '64'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/evaluate_model_metrics.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'input-1',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/model',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-2',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': \"Steps.Processing.ProcessingInputs['raw-input-data'].S3Input.S3Uri\"}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-59-407/input/code/evaluate_model_metrics.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'metrics',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/metrics/',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-56-619/output/metrics'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::106038154054:role/service-role/workshop-SageMakerExecutionRole-Y10S3AO1C964',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      "            'Name': 'EvaluateModel',\n",
      "            'PropertyFiles': [{'FilePath': 'evaluation.json',\n",
      "                               'OutputName': 'metrics',\n",
      "                               'PropertyFileName': 'EvaluationReport'}],\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'Conditions': [{'LeftValue': {'Std:JsonGet': {'Path': 'metrics.accuracy.value',\n",
      "                                                                        'PropertyFile': {'Get': 'Steps.EvaluateModel.PropertyFiles.EvaluationReport'}}},\n",
      "                                          'RightValue': {'Get': 'Parameters.MinAccuracyValue'},\n",
      "                                          'Type': 'GreaterThanOrEqualTo'}],\n",
      "                          'ElseSteps': [],\n",
      "                          'IfSteps': [{'Arguments': {'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu',\n",
      "                                                                                                'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}],\n",
      "                                                                                'SupportedContentTypes': ['application/json'],\n",
      "                                                                                'SupportedRealtimeInferenceInstanceTypes': [{'Get': 'Parameters.DeployInstanceType'}],\n",
      "                                                                                'SupportedResponseMIMETypes': ['application/json'],\n",
      "                                                                                'SupportedTransformInstanceTypes': ['ml.c5.18xlarge']},\n",
      "                                                     'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
      "                                                     'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
      "                                                                                                      'S3Uri': 's3://sagemaker-us-east-1-106038154054/sagemaker-scikit-learn-2021-02-10-05-55-56-619/output/metrics/evaluation.json'}}},\n",
      "                                                     'ModelPackageGroupName': 'BERT-Reviews-1612936553'},\n",
      "                                       'Name': 'RegisterModel',\n",
      "                                       'Type': 'RegisterModel'},\n",
      "                                      {'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::106038154054:role/service-role/workshop-SageMakerExecutionRole-Y10S3AO1C964',\n",
      "                                                     'PrimaryContainer': {'Environment': {},\n",
      "                                                                          'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu',\n",
      "                                                                          'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                       'Name': 'CreateModel',\n",
      "                                       'Type': 'Model'}]},\n",
      "            'Name': 'AccuracyCondition',\n",
      "            'Type': 'Condition'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Let's submit our pipeline definition to the workflow service. The role passed in will be used by the workflow service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-pipeline-1612936551\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the `WARNING` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:106038154054:pipeline/bert-pipeline-1612936551\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the pipeline, accepting all the default parameters.\n",
    "\n",
    "Values can also be passed into these pipeline parameters on starting of the pipeline, and will be covered later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:106038154054:pipeline/bert-pipeline-1612936551/execution/ps98hx9y5ym1\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        InputData=raw_input_data_s3_uri,\n",
    "        ProcessingInstanceCount=1,\n",
    "        ProcessingInstanceType='ml.c5.2xlarge',\n",
    "        MaxSeqLength=64,\n",
    "        BalanceDataset='True',\n",
    "        TrainSplitPercentage=0.9,\n",
    "        ValidationSplitPercentage=0.05,\n",
    "        TestSplitPercentage=0.05,\n",
    "        FeatureStoreOfflinePrefix='reviews-feature-store-'+str(timestamp),\n",
    "        FeatureGroupName='reviews-feature-group-'+str(timestamp),\n",
    "        LearningRate=0.000012,\n",
    "        TrainingInstanceType='ml.c5.9xlarge',\n",
    "        TrainingInstanceCount=1,\n",
    "        Epochs=1,\n",
    "        Epsilon=0.00000001,\n",
    "        TrainBatchSize=128,\n",
    "        ValidationBatchSize=128,\n",
    "        TestBatchSize=128,\n",
    "        TrainStepsPerEpoch=50,\n",
    "        ValidationSteps=50,\n",
    "        TestSteps=50,\n",
    "        TrainVolumeSize=1024,\n",
    "        UseXLA='True',\n",
    "        UseAMP='True',\n",
    "        FreezeBERTLayer='False',\n",
    "        EnableSageMakerDebugger='False',\n",
    "        EnableCheckpointing='False',\n",
    "        EnableTensorboard='False',\n",
    "        InputMode='File',\n",
    "        RunValidation='True',\n",
    "        RunTest='Fasle',\n",
    "        RunSamplePredictions='False', \n",
    "        MinAccuracyValue=0.01,\n",
    "        ModelApprovalStatus='PendingManualApproval', \n",
    "        DeployInstanceType='ml.m5.4xlarge',\n",
    "        DeployInstanceCount=1 \n",
    "    )\n",
    ")\n",
    "\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Now we describe execution instance and list the steps in the execution to find out more about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreatedBy': {'DomainId': 'd-pvzhchlflwyr',\n",
      "               'UserProfileArn': 'arn:aws:sagemaker:us-east-1:106038154054:user-profile/d-pvzhchlflwyr/defaultuser',\n",
      "               'UserProfileName': 'defaultuser'},\n",
      " 'CreationTime': datetime.datetime(2021, 2, 10, 5, 56, 1, 405000, tzinfo=tzlocal()),\n",
      " 'LastModifiedBy': {'DomainId': 'd-pvzhchlflwyr',\n",
      "                    'UserProfileArn': 'arn:aws:sagemaker:us-east-1:106038154054:user-profile/d-pvzhchlflwyr/defaultuser',\n",
      "                    'UserProfileName': 'defaultuser'},\n",
      " 'LastModifiedTime': datetime.datetime(2021, 2, 10, 5, 56, 1, 405000, tzinfo=tzlocal()),\n",
      " 'PipelineArn': 'arn:aws:sagemaker:us-east-1:106038154054:pipeline/bert-pipeline-1612936551',\n",
      " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:106038154054:pipeline/bert-pipeline-1612936551/execution/ps98hx9y5ym1',\n",
      " 'PipelineExecutionDisplayName': 'execution-1612936561476',\n",
      " 'PipelineExecutionStatus': 'Executing',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '731',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Wed, 10 Feb 2021 05:56:00 GMT',\n",
      "                                      'x-amzn-requestid': '65f4b0fc-73d9-4aa5-8bab-4b1982af6cde'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '65f4b0fc-73d9-4aa5-8bab-4b1982af6cde',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "execution_run = execution.describe()\n",
    "pprint(execution_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Execution Run as Trial to Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution-1612936561476\n"
     ]
    }
   ],
   "source": [
    "execution_run_name = execution_run['PipelineExecutionDisplayName']\n",
    "print(execution_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:106038154054:pipeline/bert-pipeline-1612936551/execution/ps98hx9y5ym1\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_arn = execution_run['PipelineExecutionArn']\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'Processing',\n",
       "  'StartTime': datetime.datetime(2021, 2, 10, 5, 56, 2, 78000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Executing',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:106038154054:processing-job/pipelines-ps98hx9y5ym1-processing-jb91rno2ex'}}}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Giving the first step time to start up\n",
    "time.sleep(30)\n",
    "\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)['PipelineExecutionSummaries']\n",
    "pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status=='Executing':\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)['PipelineExecutionSummaries']\n",
    "        pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "#        print('Executions for our pipeline...')\n",
    "#        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print('Please wait...')\n",
    "        time.sleep(30)    \n",
    "    \n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline ^^ Above ^^ to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_arn = executions_response[0]['PipelineExecutionArn']\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Pipeline Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List All Artifacts Generated By The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name=None\n",
    "training_job_name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps['PipelineExecutionSteps']):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step['StepName'] == 'Processing':\n",
    "        processing_job_name=execution_step['Metadata']['ProcessingJob']['Arn'].split('/')[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step['StepName'] == 'Train':\n",
    "        training_job_name=execution_step['Metadata']['TrainingJob']['Arn'].split('/')[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Additional Parameters in our Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -aws-processing-job is the default name assigned by ProcessingJob\n",
    "processing_job_tc = '{}-aws-processing-job'.format(processing_job_name)\n",
    "print(processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(\n",
    "    TrialComponentName=processing_job_tc,\n",
    "    TrialName=pipeline_trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -aws-training-job is the default name assigned by TrainingJob\n",
    "training_job_tc = '{}-aws-training-job'.format(training_job_name)\n",
    "print(training_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(\n",
    "    TrialComponentName=training_job_tc,\n",
    "    TrialName=pipeline_trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments import tracker\n",
    "\n",
    "processing_job_tracker = tracker.Tracker.load(trial_component_name=processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters({\n",
    "    \"balance_dataset\": str(balance_dataset), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters({\n",
    "    \"train_split_percentage\": str(train_split_percentage), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters({\n",
    "    \"validation_split_percentage\": str(validation_split_percentage), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters({\n",
    "    \"test_split_percentage\": str(test_split_percentage), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters({\n",
    "    \"max_seq_length\": str(max_seq_length), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # avoid throttling exception \n",
    "\n",
    "processing_job_tracker.log_parameters({\n",
    "    \"feature_store_offline_prefix\": str(feature_store_offline_prefix), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # avoid throttling exception \n",
    "\n",
    "processing_job_tracker.log_parameters({\n",
    "    \"feature_group_name\": str(feature_group_name), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "time.sleep(30) # avoid throttling exception\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    experiment_name=pipeline_experiment_name,\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "\n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
