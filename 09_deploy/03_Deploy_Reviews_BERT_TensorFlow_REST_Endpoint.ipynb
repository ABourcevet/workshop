{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving a TensorFlow Model as a REST Endpoint with TensorFlow Serving and SageMaker\n",
    "\n",
    "We need to understand the application and business context to choose between real-time and batch predictions. Are we trying to optimize for latency or throughput? Does the application require our models to scale automatically throughout the day to handle cyclic traffic requirements? Do we plan to compare models in production through A/B tests?\n",
    "\n",
    "If our application requires low latency, then we should deploy the model as a real-time API to provide super-fast predictions on single prediction requests over HTTPS. We can deploy, scale, and compare our model prediction servers with SageMaker Endpoints.\n",
    "\n",
    "## Interesting Reads\n",
    "* [**How Roblox Scaled BERT to Serve 1+ Billion Daily Requests on CPUs**](https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sagemaker-architecture.png\" width=\"80%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    training_job_name\n",
    "    print('[OK]')\n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the previous TRAIN section before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the Model to the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/tensorflow-training-2021-01-27-02-29-07-903/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/inference.py\n",
      "tensorboard/\n",
      "tensorboard/train/\n",
      "tensorboard/train/events.out.tfevents.1611715129.ip-10-2-87-218.ec2.internal.97.28316.v2\n",
      "tensorboard/train/plugins/\n",
      "tensorboard/train/plugins/profile/\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.memory_profile.json.gz\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.input_pipeline.pb\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.tensorflow_stats.pb\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.trace.json.gz\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.kernel_stats.pb\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.overview_page.pb\n",
      "tensorboard/train/plugins/profile/2021_01_27_02_34_24/ip-10-2-87-218.ec2.internal.xplane.pb\n",
      "tensorboard/train/events.out.tfevents.1611714826.ip-10-2-87-218.ec2.internal.97.5854.v2\n",
      "tensorboard/train/events.out.tfevents.1611714864.ip-10-2-87-218.ec2.internal.profile-empty\n",
      "tensorboard/validation/\n",
      "tensorboard/validation/events.out.tfevents.1611715130.ip-10-2-87-218.ec2.internal.97.28336.v2\n",
      "tensorboard/validation/events.out.tfevents.1611715056.ip-10-2-87-218.ec2.internal.97.25686.v2\n",
      "transformers/\n",
      "transformers/fine-tuned/\n",
      "transformers/fine-tuned/config.json\n",
      "transformers/fine-tuned/tf_model.h5\n",
      "test_data/\n",
      "test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      "tensorflow/\n",
      "tensorflow/saved_model/\n",
      "tensorflow/saved_model/0/\n",
      "tensorflow/saved_model/0/saved_model.pb\n",
      "tensorflow/saved_model/0/variables/\n",
      "tensorflow/saved_model/0/variables/variables.data-00000-of-00001\n",
      "tensorflow/saved_model/0/variables/variables.index\n",
      "tensorflow/saved_model/0/assets/\n",
      "metrics/\n",
      "metrics/evaluation.json\n",
      "metrics/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: saved_model_cli: not found\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir './model/tensorflow/saved_model/0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: saved_model_cli: not found\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli run --dir './model/tensorflow/saved_model/0/' --tag_set serve --signature_def serving_default \\\n",
    "    --input_exprs 'input_ids=np.zeros((1,64));input_mask=np.zeros((1,64))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show `inference.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.3.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==4.1.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[37m# Workaround for https://github.com/huggingface/tokenizers/issues/120 and\u001b[39;49;00m\n",
      "\u001b[37m#                https://github.com/kaushaltrivedi/fast-bert/issues/174\u001b[39;49;00m\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'tokenizers'])\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\n",
      "classes=[\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "max_seq_length=\u001b[34m64\u001b[39;49;00m\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_handler\u001b[39;49;00m(data, context):\n",
      "    data_str = data.read().decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdata_str: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(data_str))\n",
      "\n",
      "    data_json = json.loads(data_str)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdata_json: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(data_json))\n",
      "\n",
      "    transformed_instances = []\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m data_json_line \u001b[35min\u001b[39;49;00m data_json:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdata_json_line: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(data_json_line))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtype(data_json_line): \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mtype\u001b[39;49;00m(data_json_line)))\n",
      "\n",
      "        \u001b[37m# features[0]:  review_body\u001b[39;49;00m\n",
      "        \u001b[37m# features[1..n]:  is anything else (we can define the order ourselves)\u001b[39;49;00m\n",
      "        \u001b[37m# Example:  \u001b[39;49;00m\n",
      "        \u001b[37m#    {\"star_rating\": 5,\"features\": [\"The best gift ever\", \"Gift Cards\"]}        \u001b[39;49;00m\n",
      "        \u001b[37m#\u001b[39;49;00m\n",
      "        review_body = data_json_line[\u001b[33m'\u001b[39;49;00m\u001b[33mfeatures\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mreview_body: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m.format(review_body))\n",
      "        \n",
      "        encode_plus_tokens = tokenizer.encode_plus(review_body,\n",
      "                                                   pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                   max_length=max_seq_length,\n",
      "                                                   truncation=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Convert the text-based tokens to ids from the pre-trained BERT vocabulary\u001b[39;49;00m\n",
      "        input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "        \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1)\u001b[39;49;00m\n",
      "        input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "        transformed_instance = { \n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_ids, \n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_mask\n",
      "        }\n",
      "    \n",
      "        transformed_instances.append(transformed_instance)\n",
      "    \n",
      "    transformed_data = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msignature_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[33m\"\u001b[39;49;00m\u001b[33mserving_default\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: transformed_instances\n",
      "    }\n",
      "\n",
      "    transformed_data_json = json.dumps(transformed_data)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtransformed_data_json: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(transformed_data_json))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m transformed_data_json\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_handler\u001b[39;49;00m(response, context):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mresponse: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(response))\n",
      "    response_json = response.json()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mresponse_json: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(response_json))\n",
      "    \n",
      "    log_probabilities = response_json[\u001b[33m\"\u001b[39;49;00m\u001b[33mpredictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlog_probabilities: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(log_probabilities))\n",
      "    \n",
      "    predicted_classes = []\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m log_probability \u001b[35min\u001b[39;49;00m log_probabilities:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlog_probability in loop: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(log_probability))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtype(log_probability) in loop: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mtype\u001b[39;49;00m(log_probability)))\n",
      "        \n",
      "        softmax = tf.nn.softmax(log_probability) \n",
      "        \n",
      "        predicted_class_idx = tf.argmax(softmax, axis=-\u001b[34m1\u001b[39;49;00m, output_type=tf.int32)   \n",
      "        predicted_class = classes[predicted_class_idx]\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpredicted_class: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(predicted_class))\n",
      "\n",
      "        prediction_dict = {}\n",
      "        prediction_dict[\u001b[33m'\u001b[39;49;00m\u001b[33mpredicted_label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = predicted_class\n",
      "        \n",
      "        jsonline = json.dumps(prediction_dict)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mjsonline: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(jsonline))\n",
      "        \n",
      "        predicted_classes.append(jsonline)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpredicted_classes in the loop: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(predicted_classes))\n",
      "    \n",
      "    predicted_classes_jsonlines = \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(predicted_classes)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpredicted_classes_jsonlines: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(predicted_classes_jsonlines))\n",
      "\n",
      "    response_content_type = context.accept_header\n",
      "    \n",
      "    predicted_classes_jsonlines_dump = json.dumps(predicted_classes_jsonlines)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_classes_jsonlines_dump, response_content_type\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model\n",
    "This will create a default `EndpointConfig` with a single model.  \n",
    "\n",
    "The next notebook will demonstrate how to perform more advanced `EndpointConfig` strategies to support canary rollouts and A/B testing.\n",
    "\n",
    "_Note:  If not using a US-based region, you may need to adapt the container image to your current region using the following table:_\n",
    "\n",
    "https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-2021-01-27-02-29-07-903-tf-1611724324\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "tensorflow_model_name = '{}-{}-{}'.format(training_job_name, 'tf', timestamp)\n",
    "\n",
    "print(tensorflow_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-27 02:53:02 Starting - Preparing the instances for training\n",
      "2021-01-27 02:53:02 Downloading - Downloading input data\n",
      "2021-01-27 02:53:02 Training - Training image download completed. Training in progress.\n",
      "2021-01-27 02:53:02 Uploading - Uploading generated training model\n",
      "2021-01-27 02:53:02 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.estimator import TensorFlow\n",
    "\n",
    "estimator = TensorFlow.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires enough disk space for tensorflow, transformers, and bert downloads\n",
    "instance_type = 'ml.m5.4xlarge' # evt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "tensorflow_model = TensorFlowModel(name=tensorflow_model_name,\n",
    "                                   source_dir='code',\n",
    "                                   entry_point='inference.py',\n",
    "                                   model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "                                   role=role,\n",
    "                                   framework_version='2.3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-2021-01-27-02-29-07-903-tf-1611724324\n"
     ]
    }
   ],
   "source": [
    "tensorflow_endpoint_name = '{}-{}-{}'.format(training_job_name, 'tf', timestamp)\n",
    "\n",
    "print(tensorflow_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.tensorflow.model.TensorFlowPredictor at 0x7fb0e04cda50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow_model.deploy(endpoint_name=tensorflow_endpoint_name,\n",
    "                        initial_instance_count=1, # Should use >=2 for high(er) availability \n",
    "                        instance_type=instance_type,\n",
    "                        wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/tensorflow-training-2021-01-27-02-29-07-903-tf-1611724324\">SageMaker REST Endpoint</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(region, tensorflow_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the Endpoint is Deployed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "waiter = sm.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=tensorflow_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the ^^ Endpoint ^^ is Deployed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow_endpoint_arn = sm.describe_endpoint(EndpointName=tensorflow_endpoint_name)['EndpointArn']\n",
    "print(tensorflow_endpoint_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "lineage_table_viz = LineageTableVisualizer(sess)\n",
    "lineage_table_viz_df = lineage_table_viz.show(endpoint_arn=tensorflow_endpoint_arn)\n",
    "lineage_table_viz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(endpoint_name=tensorflow_endpoint_name,\n",
    "                                sagemaker_session=sess,\n",
    "                                model_name='saved_model',\n",
    "                                model_version=0,\n",
    "                                content_type='application/jsonlines',\n",
    "                                accept_type='application/jsonlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for the Endpoint to be ready to Serve Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the `star_rating` with Ad Hoc `review_body` Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    {\"features\": [\"This is great!\"]}\n",
    "    {\"features\": [\"This is bad.\"]}\n",
    "]\n",
    "\n",
    "predicted_classes_str = predictor.predict(inputs)\n",
    "predicted_classes = predicted_classes_str.splitlines()\n",
    "\n",
    "for predicted_class_json, input_data in zip(predicted_classes, inputs):\n",
    "    predicted_class = json.loads(predicted_class_json)['predicted_label']\n",
    "    print('Predicted star_rating: {} for review_body \"{}\"'.format(predicted_class, input_data[\"features\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the `star_rating` with `review_body` Samples from our TSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df_reviews = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \n",
    "                         delimiter='\\t', \n",
    "                         quoting=csv.QUOTE_NONE,\n",
    "                         compression='gzip')\n",
    "df_sample_reviews = df_reviews[['review_body', 'star_rating']].sample(n=5)\n",
    "df_sample_reviews = df_sample_reviews.reset_index()\n",
    "df_sample_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict(review_body):\n",
    "    inputs = [\n",
    "        {\"features\": [review_body]}\n",
    "    ]\n",
    "    predicted_classes_str = predictor.predict(inputs)\n",
    "    predicted_classes_json = predicted_classes_str.splitlines()\n",
    "    for predicted_class_json, input_data in zip(predicted_classes_json, inputs):\n",
    "        predicted_class = json.loads(predicted_class_json)['predicted_label']\n",
    "    return predicted_class\n",
    "\n",
    "df_sample_reviews['predicted_class'] = df_sample_reviews['review_body'].map(predict)\n",
    "df_sample_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tensorflow_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tensorflow_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tensorflow_endpoint_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources\n",
    "To save cost, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.delete_endpoint(\n",
    "#      EndpointName=tensorflow_endpoint_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
