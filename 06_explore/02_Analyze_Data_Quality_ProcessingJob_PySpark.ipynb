{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  THIS NOTEBOOK WILL TAKE 5-10 MINUTES TO COMPLETE.\n",
    "\n",
    "# PLEASE BE PATIENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [**Deequ**](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here are some great resources on Deequ: \n",
    "* Blog Post:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "* Research Paper:  https://assets.amazon.science/4a/75/57047bd343fabc46ec14b34cdb3b/towards-automated-data-quality-management-for-machine-learning.pdf\n",
    "\n",
    "![Deequ](img/deequ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ingest_create_athena_table_tsv_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ingest_create_athena_table_tsv_passed\n",
    "except NameError:\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] YOU HAVE TO RUN THE NOTEBOOKS IN THE INGEST FOLDER FIRST. You did not register the TSV Data.')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ingest_create_athena_table_tsv_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "if not ingest_create_athena_table_tsv_passed:\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] YOU HAVE TO RUN THE NOTEBOOKS IN THE INGEST FOLDER FIRST. You did not register the TSV Data.')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "else:\n",
    "    print('[OK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Analysis Job using a SageMaker Processing Job with Spark\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpydeequ==0.1.2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpandas==1.1.4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mboto3==1.16.17\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StringIO\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StructField, StructType, StringType, IntegerType, DoubleType\n",
      "\n",
      "\u001b[37m#def main():\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36manalyzers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\n",
      "args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\n",
      "args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\n",
      "\n",
      "\u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\n",
      "s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[36mprint\u001b[39;49;00m(s3_input_data)\n",
      "s3_output_analyze_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_analyze_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[36mprint\u001b[39;49;00m(s3_output_analyze_data)\n",
      "\n",
      "spark = SparkSession.builder \\\n",
      "     .appName(\u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon_Reviews_Spark_Analyzer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \\\n",
      "     .getOrCreate()\n",
      "\n",
      "spark.sparkContext._jsc.hadoopConfiguration().set(\u001b[33m\"\u001b[39;49;00m\u001b[33mmapred.output.committer.class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                                                  \u001b[33m\"\u001b[39;49;00m\u001b[33morg.apache.hadoop.mapred.FileOutputCommitter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "schema = StructType([\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\n",
      "    StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m)\n",
      "])\n",
      "\n",
      "dataset = spark.read.csv(s3_input_data, \n",
      "                         header=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                         schema=schema,\n",
      "                         sep=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                         quote=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m#     dataset = spark.read.option(\"sep\", \"\\t\")\u001b[39;49;00m\n",
      "\u001b[37m#                             .option(\"header\", \"true\")\u001b[39;49;00m\n",
      "\u001b[37m#                             .option(\"quote\", \"\")\u001b[39;49;00m\n",
      "\u001b[37m#                             .schema(schema)\u001b[39;49;00m\n",
      "\u001b[37m#                             .csv(s3_input_data)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Calculate statistics on the dataset\u001b[39;49;00m\n",
      "analysisResult = AnalysisRunner(spark) \\\n",
      "                    .onData(dataset) \\\n",
      "                    .addAnalyzer(Size()) \\\n",
      "                    .addAnalyzer(Completeness(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \\\n",
      "                    .addAnalyzer(ApproxCountDistinct(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \\\n",
      "                    .addAnalyzer(Mean(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \\\n",
      "                    .addAnalyzer(Compliance(\u001b[33m\"\u001b[39;49;00m\u001b[33mtop star_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating >= 4.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \\\n",
      "                    .addAnalyzer(Correlation(\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \\\n",
      "                    .addAnalyzer(Correlation(\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \\\n",
      "                    .run()\n",
      "\n",
      "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
      "analysisResult_df.show()\n",
      "\n",
      "\u001b[37m# Passing pandas=True in any call for getting metrics as DataFrames will return the dataframe in Pandas form! We'll see more of it down the line!\u001b[39;49;00m\n",
      "analysisResult_pd_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult, pandas=\u001b[34mTrue\u001b[39;49;00m)\n",
      "analysisResult_pd_df\n",
      "\n",
      "\u001b[37m# Check data quality\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mchecks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mverification\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\n",
      "check = Check(spark, CheckLevel.Warning, \u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon Customer Reviews\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "checkResult = VerificationSuite(spark) \\\n",
      "    .onData(dataset) \\\n",
      "    .addCheck(\n",
      "        check.hasSize(\u001b[34mlambda\u001b[39;49;00m x: x >= \u001b[34m200000\u001b[39;49;00m) \\\n",
      "        .hasMin(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mlambda\u001b[39;49;00m x: x == \u001b[34m1.0\u001b[39;49;00m) \\\n",
      "        .hasMax(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mlambda\u001b[39;49;00m x: x == \u001b[34m5.0\u001b[39;49;00m)  \\\n",
      "        .isComplete(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \\\n",
      "        .isUnique(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \\\n",
      "        .isComplete(\u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \\\n",
      "        .isContainedIn(\u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mUK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mDE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mJP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mFR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])) \\\n",
      "    .run()\n",
      "            \n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mVerification Run Status: \u001b[39;49;00m\u001b[33m{checkResult.status}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
      "checkResult_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\n",
      "checkResult_df.repartition(\u001b[34m1\u001b[39;49;00m).write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[34mTrue\u001b[39;49;00m).mode(\u001b[33m'\u001b[39;49;00m\u001b[33moverwrite\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33msep\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/constraint-checks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_analyze_data))\n",
      "\n",
      "\u001b[37m# checkResult_df_pandas = VerificationResult.checkResultsAsDataFrame(spark, checkResult, pandas=True)\u001b[39;49;00m\n",
      "\u001b[37m# csv_buffer = StringIO()\u001b[39;49;00m\n",
      "\u001b[37m# checkResult_df_pandas.to_csv(csv_buffer)\u001b[39;49;00m\n",
      "\u001b[37m# s3_resource = boto3.resource('s3')\u001b[39;49;00m\n",
      "\u001b[37m# s3_resource.Object('sagemaker-us-east-1-835319576252', 'blahblah/output/constraint-checks').put(Body=csv_buffer.getvalue())\u001b[39;49;00m\n",
      "\n",
      "checkResult_success_df = VerificationResult.successMetricsAsDataFrame(spark, checkResult)\n",
      "checkResult_success_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\n",
      "checkResult_success_df.repartition(\u001b[34m1\u001b[39;49;00m).write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[34mTrue\u001b[39;49;00m).mode(\u001b[33m'\u001b[39;49;00m\u001b[33moverwrite\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33msep\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/success-metrics\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_analyze_data))\n",
      "\n",
      "\u001b[37m# checkResult_success_df_pandas = VerificationResult.successMetricsAsDataFrame(spark, checkResult, pandas=True)\u001b[39;49;00m\n",
      "\u001b[37m# csv_buffer = StringIO()\u001b[39;49;00m\n",
      "\u001b[37m# checkResult_success_df_pandas.to_csv(csv_buffer)\u001b[39;49;00m\n",
      "\u001b[37m# s3_resource = boto3.resource('s3')\u001b[39;49;00m\n",
      "\u001b[37m# s3_resource.Object('sagemaker-us-east-1-835319576252', 'blahblah/output/success-metrics').put(Body=csv_buffer.getvalue())\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# Suggest new checks and constraints\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msuggestions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\n",
      "\n",
      "suggestionResult = ConstraintSuggestionRunner(spark) \\\n",
      "             .onData(dataset) \\\n",
      "             .addConstraintRule(DEFAULT()) \\\n",
      "             .run()\n",
      "\n",
      "\u001b[37m# Constraint Suggestions\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(suggestionResult))\n",
      "\u001b[36mprint\u001b[39;49;00m(suggestionResult)\n",
      "\n",
      "\u001b[37m# Constraint Suggestions in JSON format\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(json.dumps(suggestionResult, indent=\u001b[34m2\u001b[39;49;00m))    \n",
      "\n",
      "\u001b[37m# We can now investigate the constraints that Deequ suggested. \u001b[39;49;00m\n",
      "\u001b[37m#     suggestionsDataFrame = suggestionsResult['constraint_suggestions'].flatMap(lambda row: row { \u001b[39;49;00m\n",
      "\u001b[37m#           case (column, suggestions) => \u001b[39;49;00m\n",
      "\u001b[37m#             suggestions.map { constraint =>\u001b[39;49;00m\n",
      "\u001b[37m#               (column, constraint.description, constraint.codeForConstraint)\u001b[39;49;00m\n",
      "\u001b[37m#             } \u001b[39;49;00m\n",
      "\u001b[37m#     }.toSeq.toDS()\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m#if __name__ == \"__main__\":\u001b[39;49;00m\n",
      "    \u001b[37m#main()\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-deequ-pyspark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "processor = PySparkProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            role=role,\n",
    "#                            py_version='py37',\n",
    "#                            container_version='v1.0',\n",
    "                            instance_count=1,\n",
    "                            instance_type='ml.r5.2xlarge',\n",
    "                            max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-12 03:50:07   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-11-12 03:50:09   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-11-13-02-57-16\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569\n",
      "Inputs:  [{'InputName': 'jars', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569/input/jars', 'LocalPath': '/opt/ml/processing/input/jars', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569/input/code/preprocess-deequ-pyspark.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(submit_app='preprocess-deequ-pyspark.py',\n",
    "              submit_jars=['deequ-1.0.3-rc2.jar'],\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_analyze_data', s3_output_analyze_data,\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor the Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ProcessingInputs': [{'InputName': 'jars', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569/input/jars', 'LocalPath': '/opt/ml/processing/input/jars', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569/input/code/preprocess-deequ-pyspark.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.r5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}, 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess-deequ-pyspark.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output']}, 'Environment': {}, 'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:835319576252:processing-job/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 11, 13, 2, 57, 17, 137000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 11, 13, 2, 57, 17, 137000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '7d366e74-4e0e-4f46-ad80-05389b3afeea', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7d366e74-4e0e-4f46-ad80-05389b3afeea', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1768', 'date': 'Fri, 13 Nov 2020 02:57:16 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................\u001b[34m11-13 03:01 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess-deequ-pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output']\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/preprocess-deequ-pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output']\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:835319576252:processing-job/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569', 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess-deequ-pyspark.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output']}, 'ProcessingInputs': [{'InputName': 'jars', 'S3Input': {'LocalPath': '/opt/ml/processing/input/jars', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569/input/jars', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}}, {'InputName': 'code', 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-11-13-02-57-16-569/input/code/preprocess-deequ-pyspark.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}}], 'ProcessingOutputConfig': {'Outputs': [], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.r5.2xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar /opt/ml/processing/input/code/preprocess-deequ-pyspark.py s3_input_data s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/ s3_output_analyze_data s3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     Detected instance type: r5.2xlarge with total memory: 65536M and total cores: 8\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.2.198.187</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      "\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>63569</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\n",
      "\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.2.198.187\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabledm\u001b[0m\n",
      "\u001b[34mspark.executor.memory 55742m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 5574m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 8\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=2 -XX:ParallelGCThreads=6 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 1\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\n",
      "\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m11-13 03:01 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:12 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   user = root\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.198.187\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.8.5-amzn-6\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.10.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.4.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/okio-1.4.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//okhttp-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-kms-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-s3-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-core-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//okio-1.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-2.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-dataformat-cbor-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoo\u001b[0m\n",
      "\u001b[34mp-mapreduce/.//hadoop-mapreduce-examples-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//ion-java-1.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//joda-time-2.9.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jmespath-java-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r f473ee8336d640640582287162ace6dd1c0848e8; compiled by 'ec2-user' on 2020-04-17T15:08Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:12 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:12 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-7573a2fe-b0b4-442a-a4d8-2547b95b1e20\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Nov 13 03:01:13\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: 2.0% max memory 945 MB = 18.9 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: 1.0% max memory 945 MB = 9.4 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: 0.25% max memory 945 MB = 2.4 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: 0.029999999329447746% max memory 945 MB = 290.3 KB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1279668995-10.2.198.187-1605236473473\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 321 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO util.ExitUtil: Exiting with status 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:13 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.198.187\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   user = root\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.198.187\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.8.5-amzn-6\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.10.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.4.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/okio-1.4.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//okhttp-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-kms-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-s3-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-core-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//okio-1.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-2.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-dataformat-cbor-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//ion-java-1.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//joda-time-2.9.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jmespath-java-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/etc/hadoop/rm-config/log4j.properties\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r f473ee8336d640640582287162ace6dd1c0848e8; compiled by 'ec2-user' on 2020-04-17T15:08Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   user = root\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.198.187\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.8.5-amzn-6\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.10.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.4.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/okio-1.4.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//okhttp-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-kms-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-s3-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-core-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//okio-1.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-2.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-dataformat-cbor-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoo\u001b[0m\n",
      "\u001b[34mp-mapreduce/.//hadoop-mapreduce-examples-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//ion-java-1.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//joda-time-2.9.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jmespath-java-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r f473ee8336d640640582287162ace6dd1c0848e8; compiled by 'ec2-user' on 2020-04-17T15:08Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   user = root\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.198.187\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.8.5-amzn-6\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.10.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.4.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/okio-1.4.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//okhttp-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-kms-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-s3-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-core-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//okio-1.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-2.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-dataformat-cbor-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop\u001b[0m\n",
      "\u001b[34m-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//ion-java-1.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//joda-time-2.9.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jmespath-java-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r f473ee8336d640640582287162ace6dd1c0848e8; compiled by 'ec2-user' on 2020-04-17T15:08Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   user = root\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.198.187\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.8.5-amzn-6\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.10.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-common-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.4.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/okio-1.4.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.8.5-amzn-6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14-tests.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-test-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javassist-3.18.1-GA.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-math-2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.8.5-amzn-6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//okhttp-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.8.5-amzn-6-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-kms-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.0.1-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-s3-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-core-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//okio-1.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-2.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//jackson-dataformat-cbor-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoo\u001b[0m\n",
      "\u001b[34mp-mapreduce/.//hadoop-mapreduce-examples-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//ion-java-1.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//joda-time-2.9.4.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//jmespath-java-1.11.759.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.8.5-amzn-6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r f473ee8336d640640582287162ace6dd1c0848e8; compiled by 'ec2-user' on 2020-04-17T15:08Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO security.Groups: clearing userToGroupsMap cache\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO namenode.NameNode: fs.defaultFS is hdfs://10.2.198.187/\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:14 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Jetty bound to port 50070\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: Balancing bandwith is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.ParentQueue: root, capacity=1.0, asboluteCapacity=1.0, maxCapacity=1.0, asboluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,, offswitchPerHeartbeatLimit = 1, reservationsContinueLooking=true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34masboluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99998426 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:63569, vCores:8> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacityScheduler: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacityScheduler: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacityScheduler: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacityScheduler: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:63569, vCores:8>>, asynchronousScheduling=false, asyncScheduleInterval=5ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO http.HttpServer2: Jetty bound to port 36299\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@359df09a\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@66982506\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 WARN monitor.ContainersMonitorImpl: NodeManager configured with 62.1 G physical memory allocated to containers, which is more than 80% of the total physical memory available (62.1 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources: memory set to 63569MB.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources: vcores set to 8.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=63569 virtual-memory=317845 virtual-cores=8\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: Starting Socket Reader #1 for port 37205\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Nov 13 03:01:15\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: 2.0% max memory 945 MB = 18.9 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36299\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: 1.0% max memory 945 MB = 9.4 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO containermanager.ContainerManagerImpl: Blocking new container-requests as container manager rpc server is still starting.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server listener on 37205: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 101@algo-1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:37205\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.2.198.187:37205\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.2.198.187:0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:15 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.FSNamesystem: Finished loading FSImage in 230 msecs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.198.187:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.2.198.187:8020\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO namenode.FSDirectory: Quota initialization completed in 4 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-2.8.5-amzn-6.jar!/webapps/node to work/Jetty_algo.1_8042_node____.afclh/webapp\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-2.8.5-amzn-6.jar!/webapps/cluster to work/Jetty_10_2_198_187_8088_cluster____wgnzc2/webapp\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:16 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-1:8042\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:37205\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO client.RMProxy: Connecting to ResourceManager at /10.2.198.187:8031\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO ipc.Client: Retrying connect to server: algo-1/10.2.198.187:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.198.187:8020\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 102@algo-1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 1692954178. Formatting...\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Generated new storageID DS-4476b5d8-5821-4e6f-b609-c2e40395d831 for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34m11-13 03:01 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1']\u001b[0m\n",
      "\u001b[34m11-13 03:01 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2020-11-13T03:01:17.407213'))])\u001b[0m\n",
      "\u001b[34mNov 13, 2020 3:01:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Analyzing storage directories for bpid BP-1279668995-10.2.198.187-1605236473473\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1279668995-10.2.198.187-1605236473473\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1279668995-10.2.198.187-1605236473473 is not formatted for BP-1279668995-10.2.198.187-1605236473473. Formatting ...\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO common.Storage: Formatting block pool BP-1279668995-10.2.198.187-1605236473473 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1279668995-10.2.198.187-1605236473473/current\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Setting up storage: nsid=1692954178;bpid=BP-1279668995-10.2.198.187-1605236473473;lv=-57;nsInfo=lv=-63;cid=CID-7573a2fe-b0b4-442a-a4d8-2547b95b1e20;nsid=1692954178;c=1605236473473;bpid=BP-1279668995-10.2.198.187-1605236473473;dnuuid=null\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Generated and persisted new Datanode UUID 1560f5ba-6c38-49af-9d6c-69482055cbec\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@10.2.198.187:8088\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Added new volume: DS-4476b5d8-5821-4e6f-b609-c2e40395d831\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Volume reference is released.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Adding block pool BP-1279668995-10.2.198.187-1605236473473\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Scanning block pool BP-1279668995-10.2.198.187-1605236473473 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 37205 httpPort: 8042) registered with capability: <memory:63569, vCores:8>, assigned nodeId algo-1:37205\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO rmnode.RMNodeImpl: algo-1:37205 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1279668995-10.2.198.187-1605236473473 on /opt/amazon/hadoop/hdfs/datanode/current: 15ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO capacity.CapacityScheduler: Added node algo-1:37205 clusterResource: <memory:63569, vCores:8>\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1279668995-10.2.198.187-1605236473473: 17ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1279668995-10.2.198.187-1605236473473 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1279668995-10.2.198.187-1605236473473/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1279668995-10.2.198.187-1605236473473 on volume /opt/amazon/hadoop/hdfs/datanode/current: 0ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO impl.FsDatasetImpl: Total time to add all replicas to map: 2ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.VolumeScanner: Now scanning bpid BP-1279668995-10.2.198.187-1605236473473 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-4476b5d8-5821-4e6f-b609-c2e40395d831): finished scanning block pool BP-1279668995-10.2.198.187-1605236473473\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 111015975\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 217356953\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:37205 with total resource of <memory:63569, vCores:8>\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO nodemanager.NodeStatusUpdaterImpl: Notifying ContainerManager to unblock new container-requests\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/13/20 5:12 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Block pool BP-1279668995-10.2.198.187-1605236473473 (Datanode Uuid 1560f5ba-6c38-49af-9d6c-69482055cbec) service to algo-1/10.2.198.187:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-4476b5d8-5821-4e6f-b609-c2e40395d831): no suitable block pools found to scan.  Waiting 1814399973 ms.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.2.198.187:50010, datanodeUuid=1560f5ba-6c38-49af-9d6c-69482055cbec, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-7573a2fe-b0b4-442a-a4d8-2547b95b1e20;nsid=1692954178;c=1605236473473) storage 1560f5ba-6c38-49af-9d6c-69482055cbec\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO net.NetworkTopology: Adding a new node: /default-rack/10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO blockmanagement.BlockReportLeaseManager: Registered DN 1560f5ba-6c38-49af-9d6c-69482055cbec (10.2.198.187:50010).\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Block pool Block pool BP-1279668995-10.2.198.187-1605236473473 (Datanode Uuid 1560f5ba-6c38-49af-9d6c-69482055cbec) service to algo-1/10.2.198.187:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: For namenode algo-1/10.2.198.187:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4476b5d8-5821-4e6f-b609-c2e40395d831 for DN 10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO BlockStateChange: BLOCK* processReport 0x88693214988e09ac: Processing first storage report for DS-4476b5d8-5821-4e6f-b609-c2e40395d831 from datanode 1560f5ba-6c38-49af-9d6c-69482055cbec\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO BlockStateChange: BLOCK* processReport 0x88693214988e09ac: from storage DS-4476b5d8-5821-4e6f-b609-c2e40395d831 node DatanodeRegistration(10.2.198.187:50010, datanodeUuid=1560f5ba-6c38-49af-9d6c-69482055cbec, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-7573a2fe-b0b4-442a-a4d8-2547b95b1e20;nsid=1692954178;c=1605236473473), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Successfully sent block report 0x88693214988e09ac,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 37 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:17 INFO datanode.DataNode: Got finalize command for block pool BP-1279668995-10.2.198.187-1605236473473\u001b[0m\n",
      "\u001b[34mWARNING: Running pip install with root privileges is generally not a good idea. Try `__main__.py install --user` instead.\u001b[0m\n",
      "\u001b[34mCollecting pydeequ==0.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/51/05/b7273173a179564b42c29db4ddcb490e69a3d20b0969514397fec348474c/pydeequ-0.1.2-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting pyspark>=2.4.5 (from pydeequ==0.1.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCollecting py4j==0.10.9 (from pyspark>=2.4.5->pydeequ==0.1.2)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py4j, pyspark, pydeequ\n",
      "  Running setup.py install for pyspark: started\u001b[0m\n",
      "\u001b[34m    Running setup.py install for pyspark: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed py4j-0.10.9 pydeequ-0.1.2 pyspark-3.0.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip install with root privileges is generally not a good idea. Try `__main__.py install --user` instead.\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/4c/cb7da76f3a5e077e545f9cf8575b8f488a4e8ad60490838f89c5cdd5bb57/pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas==1.1.4)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2017.2 (from pandas==1.1.4)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/f8/ff09af6ff61a3efaad5f61ba5facdf17e7722c4393f7d8a66674d2dbd29f/pytz-2020.4-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.4)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-1.1.4 pytz-2020.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip install with root privileges is generally not a good idea. Try `__main__.py install --user` instead.\u001b[0m\n",
      "\u001b[34mCollecting boto3==1.16.17\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/68/2d/a4f2925ca09fac709bbd7e73eef8990000aa0f473e186133bb34b4e67f4a/boto3-1.16.17.tar.gz (97kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.20.0,>=1.19.17 (from boto3==1.16.17)\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/bf/c0/81559bf9186741b581af93435800e4fb4867ec83b9c5438a241a0a1ac055/botocore-1.19.17-py2.py3-none-any.whl (6.8MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3==1.16.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/site-packages (from boto3==1.16.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.17->boto3==1.16.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /usr/local/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.17->boto3==1.16.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.17->boto3==1.16.17)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: botocore, boto3\n",
      "  Found existing installation: botocore 1.17.60\n",
      "    Uninstalling botocore-1.17.60:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled botocore-1.17.60\n",
      "  Found existing installation: boto3 1.14.58\n",
      "    Uninstalling boto3-1.14.58:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled boto3-1.14.58\n",
      "  Running setup.py install for boto3: started\n",
      "    Running setup.py install for boto3: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed boto3-1.16.17 botocore-1.19.17\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SparkContext: Running Spark version 2.4.5-amzn-0\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SparkContext: Submitted application: Amazon_Reviews_Spark_Analyzer\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m20/11/13 03:01:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO util.Utils: Successfully started service 'sparkDriver' on port 32775.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c04b5391-0626-493a-b12b-498dfaed7e51\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO memory.MemoryStore: MemoryStore started with capacity 912.3 MB\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO util.log: Logging initialized @37696ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO server.Server: Started @37771ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO server.AbstractConnector: Started ServerConnector@1df1d30a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e15a265{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3644cc6{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@575e0e33{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3cbfd8e6{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d1cb1cb{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a940bcd{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b7ffe27{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bffbcc7{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f75ff59{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d287ab0{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74d4db24{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@526b58fd{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2214eade{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c67f9d6{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e380a85{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@769c190f{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73db4f18{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6061a1fc{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5eb3e357{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4871cdef{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@617be876{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@687ecc13{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5aa8c67a{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75917d41{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b04274a{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:55 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.2.198.187:4040\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO client.RMProxy: Connecting to ResourceManager at /10.2.198.187:8032\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63569 MB per container)\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:56 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:59 INFO yarn.Client: Uploading resource file:/tmp/spark-a3b92df5-6d5b-4757-99f9-28e0c685507f/__spark_libs__8611582365811399839.zip -> hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:01:59 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:01 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741825_1001 src: /10.2.198.187:44948 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:01 INFO DataNode.clienttrace: src: /10.2.198.187:44948, dest: /10.2.198.187:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741825_1001, duration: 238248886\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:01 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:01 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:01 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741826_1002 src: /10.2.198.187:44950 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO DataNode.clienttrace: src: /10.2.198.187:44950, dest: /10.2.198.187:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741826_1002, duration: 195012812\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741827_1003 src: /10.2.198.187:44952 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO DataNode.clienttrace: src: /10.2.198.187:44952, dest: /10.2.198.187:50010, bytes: 122906829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741827_1003, duration: 167671888\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip is closed by DFSClient_NONMAPREDUCE_655638925_19\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO namenode.FSDirectory: Increasing replication from 3 to 3 for /user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO yarn.Client: Uploading resource file:/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar -> hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741828_1004 src: /10.2.198.187:44954 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO DataNode.clienttrace: src: /10.2.198.187:44954, dest: /10.2.198.187:50010, bytes: 1714634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741828_1004, duration: 3021013\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar is closed by DFSClient_NONMAPREDUCE_655638925_19\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO namenode.FSDirectory: Increasing replication from 3 to 3 for /user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741829_1005 src: /10.2.198.187:44956 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO DataNode.clienttrace: src: /10.2.198.187:44956, dest: /10.2.198.187:50010, bytes: 595027, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741829_1005, duration: 1346293\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO namenode.FSNamesystem: BLOCK* blk_1073741829_1005 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_655638925_19\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO namenode.FSDirectory: Increasing replication from 3 to 3 for /user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741830_1006 src: /10.2.198.187:44958 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO DataNode.clienttrace: src: /10.2.198.187:44958, dest: /10.2.198.187:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741830_1006, duration: 930162\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip is closed by DFSClient_NONMAPREDUCE_655638925_19\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO namenode.FSDirectory: Increasing replication from 3 to 3 for /user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO yarn.Client: Uploading resource file:/tmp/spark-a3b92df5-6d5b-4757-99f9-28e0c685507f/__spark_conf__8534049260535946813.zip -> hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.2.198.187:50010 for /user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: Receiving BP-1279668995-10.2.198.187-1605236473473:blk_1073741831_1007 src: /10.2.198.187:44960 dest: /10.2.198.187:50010\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO DataNode.clienttrace: src: /10.2.198.187:44960, dest: /10.2.198.187:50010, bytes: 210942, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_655638925_19, offset: 0, srvID: 1560f5ba-6c38-49af-9d6c-69482055cbec, blockid: BP-1279668995-10.2.198.187-1605236473473:blk_1073741831_1007, duration: 1500436\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO datanode.DataNode: PacketResponder: BP-1279668995-10.2.198.187-1605236473473:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:02 INFO namenode.FSNamesystem: BLOCK* blk_1073741831_1007 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_655638925_19\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO namenode.FSDirectory: Increasing replication from 3 to 3 for /user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:03 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO yarn.Client: Submitting application application_1605236475429_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.CapacityScheduler: Application 'application_1605236475429_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1605236475429_0001 for the user: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO rmapp.RMAppImpl: Storing application with id application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.2.198.187#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO rmapp.RMAppImpl: application_1605236475429_0001 State change from NEW to NEW_SAVING on event=START\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO recovery.RMStateStore: Storing info for app: application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO rmapp.RMAppImpl: application_1605236475429_0001 State change from NEW_SAVING to SUBMITTED on event=APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.ParentQueue: Application added - appId: application_1605236475429_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.CapacityScheduler: Accepted application application_1605236475429_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO rmapp.RMAppImpl: application_1605236475429_0001 State change from SUBMITTED to ACCEPTED on event=APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO attempt.RMAppAttemptImpl: appattempt_1605236475429_0001_000001 State change from NEW to SUBMITTED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.LeafQueue: Application application_1605236475429_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.LeafQueue: Application added - appId: application_1605236475429_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1605236475429_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO attempt.RMAppAttemptImpl: appattempt_1605236475429_0001_000001 State change from SUBMITTED to SCHEDULED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO impl.YarnClientImpl: Submitted application application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1605236475429_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO rmcontainer.RMContainerImpl: container_1605236475429_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1605236475429_0001#011CONTAINERID=container_1605236475429_0001_01_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO scheduler.SchedulerNode: Assigned container container_1605236475429_0001_01_000001 of capacity <memory:896, vCores:1> on host algo-1:37205, which has 1 containers, <memory:896, vCores:1> used and <memory:62673, vCores:7> available after allocation\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1605236475429_0001_000001 container=container_1605236475429_0001_01_000001 queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@463019e clusterResource=<memory:63569, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:37205 for container : container_1605236475429_0001_01_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO rmcontainer.RMContainerImpl: container_1605236475429_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1605236475429_0001 AttemptId: appattempt_1605236475429_0001_000001 MasterContainer: Container: [ContainerId: container_1605236475429_0001_01_000001, Version: 0, NodeId: algo-1:37205, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.198.187:37205 }, ]\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:896, vCores:1>, usedCapacity=0.014094921, absoluteUsedCapacity=0.014094921, numApps=1, numContainers=1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.014094921 absoluteUsedCapacity=0.014094921 used=<memory:896, vCores:1> cluster=<memory:63569, vCores:8>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO attempt.RMAppAttemptImpl: appattempt_1605236475429_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO attempt.RMAppAttemptImpl: appattempt_1605236475429_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO amlauncher.AMLauncher: Launching masterappattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1605236475429_0001_01_000001, Version: 0, NodeId: algo-1:37205, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.198.187:37205 }, ] for AM appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:04 INFO ipc.Server: Auth successful for appattempt_1605236475429_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO containermanager.ContainerManagerImpl: Start request for container_1605236475429_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO application.ApplicationImpl: Application application_1605236475429_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.198.187#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1605236475429_0001#011CONTAINERID=container_1605236475429_0001_01_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO application.ApplicationImpl: Adding container_1605236475429_0001_01_000001 to application application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO application.ApplicationImpl: Application application_1605236475429_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO container.ContainerImpl: Container container_1605236475429_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1605236475429_0001_01_000001, Version: 0, NodeId: algo-1:37205, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.198.187:37205 }, ] for AM appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO attempt.RMAppAttemptImpl: appattempt_1605236475429_0001_000001 State change from ALLOCATED to LAUNCHED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip transitioned from INIT to DOWNLOADING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip transitioned from INIT to DOWNLOADING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip transitioned from INIT to DOWNLOADING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip transitioned from INIT to DOWNLOADING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar transitioned from INIT to DOWNLOADING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.ResourceLocalizationService: Created localizer for container_1605236475429_0001_01_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1605236475429_0001_01_000001.tokens. Credentials list: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1605236475429_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1605236475429_0001/container_1605236475429_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1605236475429_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO yarn.Client: Application report for application_1605236475429_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1605236524353\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1605236475429_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip(->/tmp/hadoop-root/nm-local-dir/usercache/root/filecache/10/pyspark.zip) transitioned from DOWNLOADING to LOCALIZED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:05 INFO rmcontainer.RMContainerImpl: container_1605236475429_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:06 INFO yarn.Client: Application report for application_1605236475429_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip(->/tmp/hadoop-root/nm-local-dir/usercache/root/filecache/11/__spark_libs__8611582365811399839.zip) transitioned from DOWNLOADING to LOCALIZED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip(->/tmp/hadoop-root/nm-local-dir/usercache/root/filecache/12/__spark_conf__.zip) transitioned from DOWNLOADING to LOCALIZED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip(->/tmp/hadoop-root/nm-local-dir/usercache/root/filecache/13/py4j-0.10.7-src.zip) transitioned from DOWNLOADING to LOCALIZED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO localizer.LocalizedResource: Resource hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar(->/tmp/hadoop-root/nm-local-dir/usercache/root/filecache/14/deequ-1.0.3-rc2.jar) transitioned from DOWNLOADING to LOCALIZED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO container.ContainerImpl: Container container_1605236475429_0001_01_000001 transitioned from LOCALIZING to LOCALIZED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO container.ContainerImpl: Container container_1605236475429_0001_01_000001 transitioned from LOCALIZED to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1605236475429_0001_01_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1605236475429_0001/container_1605236475429_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:07 INFO yarn.Client: Application report for application_1605236475429_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:08 INFO yarn.Client: Application report for application_1605236475429_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:09 INFO yarn.Client: Application report for application_1605236475429_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:09 INFO ipc.Server: Auth successful for appattempt_1605236475429_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:09 INFO resourcemanager.ApplicationMasterService: AM registration appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:09 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.2.198.187#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1605236475429_0001#011APPATTEMPTID=appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:09 INFO attempt.RMAppAttemptImpl: appattempt_1605236475429_0001_000001 State change from LAUNCHED to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:09 INFO rmapp.RMAppImpl: application_1605236475429_0001 State change from ACCEPTED to RUNNING on event=ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1605236475429_0001), /proxy/application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:07 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:07 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:07 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:08 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:08 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:08 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:08 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:08 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:08 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:09 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1605236475429_0001_000001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:09 INFO client.RMProxy: Connecting to ResourceManager at /10.2.198.187:8030\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:09 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:09 INFO client.TransportClientFactory: Successfully created connection to /10.2.198.187:32775 after 93 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:10 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] YARN executor launch context:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.2.198.187/user/root/.sparkStaging/application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     SPARK_NO_DAEMONIZE -> TRUE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     SPARK_MASTER_HOST -> 10.2.198.187\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     SPARK_HOME -> /usr/lib/spark\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       -Xmx55742m \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       '-Dspark.driver.port=32775' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       -XX:OnOutOfMemoryError='kill %p' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.2.198.187:32775 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       8 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       application_1605236475429_0001 \\ \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO yarn.Client: Application report for application_1605236475429_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.198.187\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1605236524353\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1605236475429_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO cluster.YarnClientSchedulerBackend: Application application_1605236475429_0001 has started running.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41113.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO netty.NettyBlockTransferService: Server created on 10.2.198.187:41113\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.198.187, 41113, None)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.198.187:41113 with 912.3 MB RAM, BlockManagerId(driver, 10.2.198.187, 41113, None)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.198.187, 41113, None)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.198.187, 41113, None)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f30cbb7{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO rmcontainer.RMContainerImpl: container_1605236475429_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1605236475429_0001#011CONTAINERID=container_1605236475429_0001_01_000002\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO scheduler.SchedulerNode: Assigned container container_1605236475429_0001_01_000002 of capacity <memory:61316, vCores:1> on host algo-1:37205, which has 2 containers, <memory:62212, vCores:2> used and <memory:1357, vCores:6> available after allocation\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1605236475429_0001_000001 container=container_1605236475429_0001_01_000002 queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@463019e clusterResource=<memory:63569, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:62212, vCores:2>, usedCapacity=0.97865313, absoluteUsedCapacity=0.97865313, numApps=1, numContainers=2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.97865313 absoluteUsedCapacity=0.97865313 used=<memory:62212, vCores:2> cluster=<memory:63569, vCores:8>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2445bb1b{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e2e4db8{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70a0a8db{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78226eef{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@138dc9d1{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:37205 for container : container_1605236475429_0001_01_000002\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO rmcontainer.RMContainerImpl: container_1605236475429_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO ipc.Server: Auth successful for appattempt_1605236475429_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO containermanager.ContainerManagerImpl: Start request for container_1605236475429_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.198.187#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1605236475429_0001#011CONTAINERID=container_1605236475429_0001_01_000002\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO application.ApplicationImpl: Adding container_1605236475429_0001_01_000002 to application application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO container.ContainerImpl: Container container_1605236475429_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO container.ContainerImpl: Container container_1605236475429_0001_01_000002 transitioned from LOCALIZING to LOCALIZED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       file:$PWD/deequ-1.0.3-rc2.jar \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.2.198.187\" port: -1 file: \"/user/root/.sparkStaging/application_1605236475429_0001/__spark_conf__.zip\" } size: 210942 timestamp: 1605236523363 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.2.198.187\" port: -1 file: \"/user/root/.sparkStaging/application_1605236475429_0001/pyspark.zip\" } size: 595027 timestamp: 1605236522838 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     deequ-1.0.3-rc2.jar -> resource { scheme: \"hdfs\" host: \"10.2.198.187\" port: -1 file: \"/user/root/.sparkStaging/application_1605236475429_0001/deequ-1.0.3-rc2.jar\" } size: 1714634 timestamp: 1605236522420 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.2.198.187\" port: -1 file: \"/user/root/.sparkStaging/application_1605236475429_0001/__spark_libs__8611582365811399839.zip\" } size: 391342285 timestamp: 1605236522321 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr]     py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"10.2.198.187\" port: -1 file: \"/user/root/.sparkStaging/application_1605236475429_0001/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1605236522856 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:10 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 8 core(s) and 61316 MB memory (including 5574 MB of overhead)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:10 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:10 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:11 INFO impl.AMRMClientImpl: Received new token for : algo-1:37205\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/stderr] 20/11/13 03:02:11 INFO yarn.YarnAllocator: Launching container container_1605236475429_0001_01_000002 on host algo-1 for executor with ID 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO container.ContainerImpl: Container container_1605236475429_0001_01_000002 transitioned from LOCALIZED to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1605236475429_0001_01_000002\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1605236475429_0001/container_1605236475429_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000001/sHandling create event for file: /var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:11 INFO rmcontainer.RMContainerImpl: container_1605236475429_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:12 INFO datasources.InMemoryFileIndex: It took 166 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.198.187:47504) with ID 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1605236475429_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1136@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO client.TransportClientFactory: Successfully created connection to /10.2.198.187:32775 after 134 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO client.TransportClientFactory: Successfully created connection to /10.2.198.187:32775 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1605236475429_0001/blockmgr-01ace4f2-20e4-4b43-a2b1-12792334081d\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:13 INFO memory.MemoryStore: MemoryStore started with capacity 28.9 GB\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:38619 with 28.9 GB RAM, BlockManagerId(1, algo-1, 38619, None)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m20/11/13 03:02:14 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string, star_rating: int, helpful_votes: int, total_votes: int ... 2 more fields>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:14 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO codegen.CodeGenerator: Code generated in 301.986602 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO codegen.CodeGenerator: Code generated in 12.033156 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 258.1 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.0 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO spark.SparkContext: Created broadcast 0 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:15 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at AnalysisRunner.scala:323) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Got map stage job 0 (collect at AnalysisRunner.scala:323) with 2 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.9 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.2 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.198.187:41113 (size: 14.2 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:16 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:38619 (size: 14.2 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.2.198.187:32775\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38619.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO netty.NettyBlockTransferService: Server created on algo-1:38619\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 38619, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 38619, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:14 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 38619, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 2027 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO client.TransportClientFactory: Successfully created connection to /10.2.198.187:41113 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.2 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 124 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:16 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.9 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO codegen.CodeGenerator: Code generated in 223.41923 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:17 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3618 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3923 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at AnalysisRunner.scala:323) finished in 3.969 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 48.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Got job 1 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.8 KB, free 911.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.9 KB, free 911.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.198.187:41113 (size: 15.9 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:38619 (size: 15.9 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO codegen.CodeGenerator: Code generated in 16.367826 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 385.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO codegen.CodeGenerator: Code generated in 12.038911 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:17 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:18 INFO codegen.CodeGenerator: Code generated in 71.99565 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:18 INFO codegen.CodeGenerator: Code generated in 6.623381 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:18 INFO codegen.CodeGenerator: Code generated in 10.732273 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:18 INFO codegen.CodeGenerator: Code generated in 44.818699 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:19 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2428 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2385 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 223 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: ResultStage 2 (collect at AnalysisRunner.scala:323) finished in 0.233 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO scheduler.DAGScheduler: Job 1 finished: collect at AnalysisRunner.scala:323, took 0.245611 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.2.198.187:41113 in memory (size: 22.0 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:38619 in memory (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:38619 in memory (size: 15.9 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.2.198.187:41113 in memory (size: 15.9 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.198.187:41113 in memory (size: 14.2 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:38619 in memory (size: 14.2 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO codegen.CodeGenerator: Code generated in 31.815644 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO codegen.CodeGenerator: Code generated in 8.872206 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:20 INFO codegen.CodeGenerator: Code generated in 7.54326 ms\u001b[0m\n",
      "\u001b[34m[/v+-----------+--------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|     entity|            instance|               name|              value|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|     Column|           review_id|       Completeness|                1.0|\u001b[0m\n",
      "\u001b[34m|     Column|           review_id|ApproxCountDistinct|           238027.0|\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,star_...|        Correlation|-0.0808806564857777|\u001b[0m\n",
      "\u001b[34m|    Dataset|                   *|               Size|           247515.0|\u001b[0m\n",
      "\u001b[34m|     Column|         star_rating|               Mean| 3.7237056340019796|\u001b[0m\n",
      "\u001b[34m|     Column|     top star_rating|         Compliance| 0.6633375755004747|\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,helpf...|        Correlation| 0.9805294402834748|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+-------------------+-------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO codegen.CodeGenerator: Code generated in 7.580457 ms\u001b[0m\n",
      "\u001b[34mPython Callback server started!\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, review_id: string, star_rating: int ... 1 more fields>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO codegen.CodeGenerator: Code generated in 7.960396 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO codegen.CodeGenerator: Code generated in 16.968785 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 258.1 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.0 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO spark.SparkContext: Created broadcast 3 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Registering RDD 9 (collect at AnalysisRunner.scala:323) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:323) with 2 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.7 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.5 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.198.187:41113 (size: 9.5 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO cluster.YarnScheduler: Adding task set 3.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:38619 (size: 9.5 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:21 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34mar/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.9 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.8 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO codegen.CodeGenerator: Code generated in 28.506331 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO codegen.CodeGenerator: Code generated in 8.933088 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO codegen.CodeGenerator: Code generated in 12.960637 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:20 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 3668 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.5 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 12 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 1192 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1577 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (collect at AnalysisRunner.scala:323) finished in 1.585 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO codegen.CodeGenerator: Code generated in 14.398202 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[12] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.6 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.0 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.198.187:41113 (size: 6.0 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:38619 (size: 6.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: ResultStage 5 (collect at AnalysisRunner.scala:323) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:22 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:323, took 0.047166 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(none)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO codegen.CodeGenerator: Code generated in 11.059705 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 258.1 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.0 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO spark.SparkContext: Created broadcast 6 from count at GroupingAnalyzers.scala:80\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Registering RDD 15 (count at GroupingAnalyzers.scala:80) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Got map stage job 4 (count at GroupingAnalyzers.scala:80) with 2 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (count at GroupingAnalyzers.scala:80)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[15] at count at GroupingAnalyzers.scala:80), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.1 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.198.187:41113 (size: 7.4 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[15] at count at GroupingAnalyzers.scala:80) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 7, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:38619 (size: 7.4 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:23 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.7 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO codegen.CodeGenerator: Code generated in 25.088826 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO datasources.FileScanRDD: TID: 3 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO codegen.CodeGenerator: Code generated in 8.639825 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:21 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 385.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 4). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.7 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO codegen.CodeGenerator: Code generated in 16.019486 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:22 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 5). 1955 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.1 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO codegen.CodeGenerator: Code generated in 11.447849 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO datasources.FileScanRDD: TID: 7 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO codegen.CodeGenerator: Code generated in 7.766501 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 7) in 994 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1323 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (count at GroupingAnalyzers.scala:80) finished in 1.329 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 9.230447 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO spark.SparkContext: Starting job: count at GroupingAnalyzers.scala:80\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Got job 5 (count at GroupingAnalyzers.scala:80) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (count at GroupingAnalyzers.scala:80)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[18] at count at GroupingAnalyzers.scala:80), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.6 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.2 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.198.187:41113 (size: 4.2 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at count at GroupingAnalyzers.scala:80) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:38619 (size: 4.2 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: ResultStage 8 (count at GroupingAnalyzers.scala:80) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Job 5 finished: count at GroupingAnalyzers.scala:80, took 0.037833 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(none)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 5.512185 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 40.609733 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 258.1 KB, free 911.4 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.0 KB, free 911.4 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO spark.SparkContext: Created broadcast 9 from collect at AnalysisRunner.scala:523\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Registering RDD 21 (collect at AnalysisRunner.scala:523) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Got map stage job 6 (collect at AnalysisRunner.scala:523) with 2 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (collect at AnalysisRunner.scala:523)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[21] at collect at AnalysisRunner.scala:523), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 27.7 KB, free 911.4 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.0 KB, free 911.4 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.198.187:41113 (size: 13.0 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[21] at collect at AnalysisRunner.scala:523) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO cluster.YarnScheduler: Adding task set 9.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 10, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:38619 (size: 13.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:24 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 14 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:23 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 385.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 7). 1808 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 6). 1808 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 8)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.2 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.6 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 9.041951 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 8). 1937 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 10) in 1319 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1741 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at AnalysisRunner.scala:523) finished in 1.749 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 457622.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO codegen.CodeGenerator: Code generated in 21.778113 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Registering RDD 24 (collect at AnalysisRunner.scala:523) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Got map stage job 7 (collect at AnalysisRunner.scala:523) with 26 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:523)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[24] at collect at AnalysisRunner.scala:523), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 30.4 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.1 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.198.187:41113 (size: 14.1 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: Submitting 26 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[24] at collect at AnalysisRunner.scala:523) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO cluster.YarnScheduler: Adding task set 11.0 with 26 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 12, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 13, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 11.0 (TID 14, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 11.0 (TID 15, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 11.0 (TID 16, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 11.0 (TID 17, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 11.0 (TID 18, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:38619 (size: 14.1 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 11.0 (TID 19, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 155 ms on algo-1 (executor 1) (1/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 11.0 (TID 20, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 11.0 (TID 16) in 160 ms on algo-1 (executor 1) (2/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 11.0 (TID 21, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 11.0 (TID 22, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 11.0 (TID 18) in 198 ms on algo-1 (executor 1) (3/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 11.0 (TID 15) in 199 ms on algo-1 (executor 1) (4/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 11.0 (TID 23, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 11.0 (TID 24, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 11.0 (TID 25, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 11.0 (TID 26, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 13) in 210 ms on algo-1 (executor 1) (5/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 11.0 (TID 14) in 210 ms on algo-1 (executor 1) (6/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 11.0 (TID 17) in 210 ms on algo-1 (executor 1) (7/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 12) in 211 ms on algo-1 (executor 1) (8/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 11.0 (TID 27, algo-1, executor 1, partition 16, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 11.0 (TID 20) in 68 ms on algo-1 (executor 1) (9/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 11.0 (TID 28, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 11.0 (TID 19) in 121 ms on algo-1 (executor 1) (10/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 11.0 (TID 29, algo-1, executor 1, partition 18, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 11.0 (TID 22) in 104 ms on algo-1 (executor 1) (11/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 11.0 (TID 30, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 11.0 (TID 25) in 99 ms on algo-1 (executor 1) (12/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 11.0 (TID 31, algo-1, executor 1, partition 20, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 11.0 (TID 21) in 128 ms on algo-1 (executor 1) (13/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 11.0 (TID 32, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 11.0 (TID 23) in 120 ms on algo-1 (executor 1) (14/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 11.0 (TID 33, algo-1, executor 1, partition 22, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 11.0 (TID 34, algo-1, executor 1, partition 23, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 11.0 (TID 24) in 146 ms on algo-1 (executor 1) (15/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 11.0 (TID 27) in 128 ms on algo-1 (executor 1) (16/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 11.0 (TID 35, algo-1, executor 1, partition 24, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 11.0 (TID 26) in 149 ms on algo-1 (executor 1) (17/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 11.0 (TID 36, algo-1, executor 1, partition 25, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 11.0 (TID 29) in 67 ms on algo-1 (executor 1) (18/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 11.0 (TID 28) in 104 ms on algo-1 (executor 1) (19/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 11.0 (TID 30) in 95 ms on algo-1 (executor 1) (20/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 11.0 (TID 31) in 83 ms on algo-1 (executor 1) (21/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 11.0 (TID 36) in 47 ms on algo-1 (executor 1) (22/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 11.0 (TID 32) in 90 ms on algo-1 (executor 1) (23/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 11.0 (TID 33) in 88 ms on algo-1 (executor 1) (24/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 11.0 (TID 34) in 88 ms on algo-1 (executor 1) (25/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 11.0 (TID 35) in 88 ms on algo-1 (executor 1) (26/26)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:523) finished in 0.453 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 22.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:26 INFO codegen.CodeGenerator: Code generated in 11.691041 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:523\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Got job 8 (collect at AnalysisRunner.scala:523) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at AnalysisRunner.scala:523)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:523), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.7 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.7 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.198.187:41113 (size: 4.7 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:523) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 37, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:38619 (size: 4.7 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 37) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: ResultStage 14 (collect at AnalysisRunner.scala:523) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Job 8 finished: collect at AnalysisRunner.scala:523, took 0.047865 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/coVerification Run Status: Success\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO codegen.CodeGenerator: Code generated in 19.678142 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO codegen.CodeGenerator: Code generated in 7.646974 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO codegen.CodeGenerator: Code generated in 6.25379 ms\u001b[0m\n",
      "\u001b[34m+-----------------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|check                  |check_level|check_status|constraint                                                                                                                                         |constraint_status|constraint_message|\u001b[0m\n",
      "\u001b[34m+-----------------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |SizeConstraint(Size(None))                                                                                                                         |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |MinimumConstraint(Minimum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |MaximumConstraint(Maximum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |CompletenessConstraint(Completeness(review_id,None))                                                                                               |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |UniquenessConstraint(Uniqueness(List(review_id),None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |CompletenessConstraint(Completeness(marketplace,None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Amazon Customer Reviews|Warning    |Success     |ComplianceConstraint(Compliance(marketplace contained in US,UK,DE,JP,FR,`marketplace` IS NULL OR `marketplace` IN ('US','UK','DE','JP','FR'),None))|Success          |                  |\u001b[0m\n",
      "\u001b[34m+-----------------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\u001b[0m\n",
      "\u001b[34mntainer_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 9)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 27.7 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 43.754583 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 11.503118 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 4.085577 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO codegen.CodeGenerator: Code generated in 5.12184 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO datasources.FileScanRDD: TID: 10 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO datasources.FileScanRDD: TID: 9 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.7 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:24 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 385.3 KB, free 28.7 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 10). 4470 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 9). 4470 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 2.0 in stage 11.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 3.0 in stage 11.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.1 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 4.0 in stage 11.0 (TID 15)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 5.0 in stage 11.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 6.0 in stage 11.0 (TID 17)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 7.0 in stage 11.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 17 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 30.4 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO codegen.CodeGenerator: Code generated in 23.194795 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 11). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 5.0 in stage 11.0 (TID 16). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 8.0 in stage 11.0 (TID 19)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 9.0 in stage 11.0 (TID 20)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 7.0 in stage 11.0 (TID 18). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 4.0 in stage 11.0 (TID 15). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 2.0 in stage 11.0 (TID 13). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 12). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 6.0 in stage 11.0 (TID 17). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 10.0 in stage 11.0 (TID 21)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 3.0 in stage 11.0 (TID 14). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 11.0 in stage 11.0 (TID 22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 12.0 in stage 11.0 (TID 23)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 13.0 in stage 11.0 (TID 24)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 14.0 in stage 11.0 (TID 25)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 15.0 in stage 11.0 (TID 26)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 9.0 in stage 11.0 (TID 20). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 16.0 in stage 11.0 (TID 27)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 8.0 in stage 11.0 (TID 19). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 17.0 in stage 11.0 (TID 28)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 11.0 in stage 11.0 (TID 22). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 14.0 in stage 11.0 (TID 25). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 18.0 in stage 11.0 (TID 29)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 19.0 in stage 11.0 (TID 30)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 10.0 in stage 11.0 (TID 21). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 12.0 in stage 11.0 (TID 23). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 20.0 in stage 11.0 (TID 31)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 21.0 in stage 11.0 (TID 32)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 16.0 in stage 11.0 (TID 27). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 13.0 in stage 11.0 (TID 24). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 15.0 in stage 11.0 (TID 26). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 18.0 in stage 11.0 (TID 29). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 22.0 in stage 11.0 (TID 33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 24.0 in stage 11.0 (TID 35)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 25.0 in stage 11.0 (TID 36)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Running task 23.0 in stage 11.0 (TID 34)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 17.0 in stage 11.0 (TID 28). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 19.0 in stage 11.0 (TID 30). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 20.0 in stage 11.0 (TID 31). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 25.0 in stage 11.0 (TID 36). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Registering RDD 30 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Got map stage job 9 (save at NativeMethodAccessorImpl.java:0) with 7 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[30] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.5 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.3 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.198.187:41113 (size: 3.3 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[30] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO cluster.YarnScheduler: Adding task set 15.0 with 7 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 38, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8164 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 39, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 40, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 15.0 (TID 41, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8188 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 15.0 (TID 42, algo-1, executor 1, partition 4, PROCESS_LOCAL, 8188 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 15.0 (TID 43, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8188 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 15.0 (TID 44, algo-1, executor 1, partition 6, PROCESS_LOCAL, 8287 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:38619 (size: 3.3 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 39) in 25 ms on algo-1 (executor 1) (1/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 15.0 (TID 42) in 27 ms on algo-1 (executor 1) (2/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 15.0 (TID 41) in 28 ms on algo-1 (executor 1) (3/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 15.0 (TID 43) in 27 ms on algo-1 (executor 1) (4/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 38) in 30 ms on algo-1 (executor 1) (5/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 15.0 (TID 44) in 28 ms on algo-1 (executor 1) (6/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 40) in 34 ms on algo-1 (executor 1) (7/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (save at NativeMethodAccessorImpl.java:0) finished in 0.044 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Got job 10 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (ShuffledRowRDD[31] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 138.0 KB, free 911.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 49.7 KB, free 911.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.198.187:41113 (size: 49.7 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (ShuffledRowRDD[31] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 45, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:38619 (size: 49.7 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 21.0 in stage 11.0 (TID 32). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 22.0 in stage 11.0 (TID 33). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 23.0 in stage 11.0 (TID 34). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:26 INFO executor.Executor: Finished task 24.0 in stage 11.0 (TID 35). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 37)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.8 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO storage.ShuffleBlockFetcherIterator: Getting 26 non-empty blocks including 26 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO codegen.CodeGenerator: Code generated in 10.96701 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 37). 1974 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 38)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 2.0 in stage 15.0 (TID 40)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 3.0 in stage 15.0 (TID 41)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 5.0 in stage 15.0 (TID 43)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 4.0 in stage 15.0 (TID 42)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 6.0 in stage 15.0 (TID 44)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 39)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.5 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 39). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 3.0 in stage 15.0 (TID 41). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 4.0 in stage 15.0 (TID 42). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 5.0 in stage 15.0 (TID 43). 1427 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 38). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:29 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 45) in 1717 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:29 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:29 INFO scheduler.DAGScheduler: ResultStage 17 (save at NativeMethodAccessorImpl.java:0) finished in 1.735 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:29 INFO scheduler.DAGScheduler: Job 10 finished: save at NativeMethodAccessorImpl.java:0, took 1.737902 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO datasources.FileFormatWriter: Write Job 1d9c43ab-42f5-4236-88b3-78b16a2f2e42 committed.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO datasources.FileFormatWriter: Finished processing stats for write job 1d9c43ab-42f5-4236-88b3-78b16a2f2e42.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|entity |instance                               |name        |value   |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Uniqueness  |1.0     |\u001b[0m\n",
      "\u001b[34m|Dataset|*                                      |Size        |247515.0|\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Maximum     |5.0     |\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Minimum     |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace contained in US,UK,DE,JP,FR|Compliance  |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace                            |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\n",
      "\u001b[0m\n",
      "\u001b[34m27 INFO executor.Executor: Finished task 6.0 in stage 15.0 (TID 44). 1383 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Finished task 2.0 in stage 15.0 (TID 40). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 45)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 49.7 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 138.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO storage.ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:27 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:29 INFO output.FileOutputCommitter: Saved output of task 'attempt_20201113030227_0017_m_000000_45' to s3a://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output/constraint-checks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Registering RDD 36 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Got map stage job 11 (save at NativeMethodAccessorImpl.java:0) with 7 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[36] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.4 KB, free 911.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 911.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.198.187:41113 (size: 3.3 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[36] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO cluster.YarnScheduler: Adding task set 18.0 with 7 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 46, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 47, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 18.0 (TID 48, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8092 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 18.0 (TID 49, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 18.0 (TID 50, algo-1, executor 1, partition 4, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 18.0 (TID 51, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8132 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 18.0 (TID 52, algo-1, executor 1, partition 6, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:38619 (size: 3.3 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 18.0 (TID 51) in 19 ms on algo-1 (executor 1) (1/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 18.0 (TID 50) in 20 ms on algo-1 (executor 1) (2/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 46) in 21 ms on algo-1 (executor 1) (3/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 47) in 22 ms on algo-1 (executor 1) (4/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 18.0 (TID 52) in 22 ms on algo-1 (executor 1) (5/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 18.0 (TID 49) in 22 ms on algo-1 (executor 1) (6/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 18.0 (TID 48) in 23 ms on algo-1 (executor 1) (7/7)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (save at NativeMethodAccessorImpl.java:0) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Got job 12 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRowRDD[37] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 137.9 KB, free 911.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 49.6 KB, free 910.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.198.187:41113 (size: 49.6 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (ShuffledRowRDD[37] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 53, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:38619 (size: 49.6 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:29 INFO mapred.SparkHadoopMapRedUtil: attempt_20201113030227_0017_m_000000_45: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:29 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 45). 2477 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 46)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 2.0 in stage 18.0 (TID 48)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 3.0 in stage 18.0 (TID 49)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 1.0 in stage 18.0 (TID 47)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 5.0 in stage 18.0 (TID 51)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 4.0 in stage 18.0 (TID 50)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 6.0 in stage 18.0 (TID 52)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.4 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 5.0 in stage 18.0 (TID 51). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 46). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 4.0 in stage 18.0 (TID 50). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 2.0 in stage 18.0 (TID 48). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 1.0 in stage 18.0 (TID 47). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 3.0 in stage 18.0 (TID 49). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Finished task 6.0 in stage 18.0 (TID 52). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 53)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 49.6 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 137.9 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO storage.ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:32 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 53) in 1907 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:32 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:32 INFO scheduler.DAGScheduler: ResultStage 20 (save at NativeMethodAccessorImpl.java:0) finished in 1.932 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:32 INFO scheduler.DAGScheduler: Job 12 finished: save at NativeMethodAccessorImpl.java:0, took 1.934112 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO datasources.FileFormatWriter: Write Job dcdbd957-d916-4b59-bf7d-0a06f7220eb6 committed.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO datasources.FileFormatWriter: Finished processing stats for write job dcdbd957-d916-4b59-bf7d-0a06f7220eb6.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 409\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 359\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 303\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 287\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 354\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 424\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 449\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 389\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 461\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 358\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 463\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 321\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 337\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 469\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 489\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 299\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned shuffle 5\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 313\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 314\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 319\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 291\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 426\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 349\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.198.187:41113 in memory (size: 4.2 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:38619 in memory (size: 4.2 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 295\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 483\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 393\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 452\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 487\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned shuffle 3\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 460\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 310\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 338\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 331\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 342\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 455\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 403\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 395\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.198.187:41113 in memory (size: 3.3 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:38619 in memory (size: 3.3 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 484\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 361\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 506\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 472\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 333\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 376\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 415\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 432\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.198.187:41113 in memory (size: 6.0 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:38619 in memory (size: 6.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 339\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 473\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 320\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 498\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 475\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 505\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 425\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 453\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 408\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 311\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 362\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 414\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 492\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 366\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 346\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 394\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 330\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 329\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 351\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 343\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 336\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 464\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 285\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 353\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 297\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 399\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 365\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 466\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 482\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 509\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 373\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 386\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 421\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 387\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 404\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 507\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 308\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 304\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 436\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 367\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 355\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 335\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 419\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 374\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 443\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 448\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 456\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 450\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 301\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 480\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 370\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 429\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 332\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 307\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 495\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 457\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 444\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 322\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 471\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 379\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 392\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 446\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 496\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 324\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 420\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 499\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 372\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 418\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 382\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 417\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 412\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 368\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 341\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 427\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 326\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 442\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 325\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 327\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 494\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 348\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 345\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 402\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 423\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 459\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.198.187:41113 in memory (size: 22.0 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:38619 in memory (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 447\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 431\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 467\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 352\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 293\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 391\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 384\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 439\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned shuffle 4\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 458\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 437\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 501\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 508\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.198.187:41113 in memory (size: 49.6 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:38619 in memory (size: 49.6 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 441\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 474\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.198.187:41113 in memory (size: 13.0 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:38619 in memory (size: 13.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 465\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 306\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.198.187:41113 in memory (size: 49.7 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:38619 in memory (size: 49.7 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 357\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 318\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 315\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 479\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 350\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 292\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 309\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 312\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 454\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 328\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 364\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 385\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 288\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 410\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 468\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 381\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 294\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 493\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 363\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.198.187:41113 in memory (size: 3.3 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:38619 in memory (size: 3.3 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 500\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 296\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 375\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 462\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 488\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 430\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.198.187:41113 in memory (size: 4.7 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:38619 in memory (size: 4.7 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 491\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 503\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 411\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 317\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 451\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 286\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 334\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.198.187:41113 in memory (size: 22.0 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:38619 in memory (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 369\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 289\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 405\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 407\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 383\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 502\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 398\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 316\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 416\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 476\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 481\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.198.187:41113 in memory (size: 22.0 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:38619 in memory (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 388\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 390\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 445\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 413\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 478\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 396\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 290\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 300\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 380\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.198.187:41113 in memory (size: 7.4 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:38619 in memory (size: 7.4 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 497\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 397\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 510\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 485\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 305\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 302\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 344\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 477\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 435\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 340\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 356\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned shuffle 6\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 438\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 371\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 377\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 433\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 490\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 401\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 378\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 360\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 486\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 400\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 347\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.198.187:41113 in memory (size: 9.5 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:38619 in memory (size: 9.5 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 434\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 470\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 406\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 298\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 422\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 440\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 428\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.198.187:41113 in memory (size: 14.1 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:38619 in memory (size: 14.1 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO codegen.CodeGenerator: Code generated in 17.022434 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 258.1 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 22.0 KB, free 912.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.SparkContext: Created broadcast 17 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Registering RDD 43 (collect at AnalysisRunner.scala:323) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Got map stage job 13 (collect at AnalysisRunner.scala:323) with 2 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[43] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 144.1 KB, free 911.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 46.4 KB, free 911.8 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.198.187:41113 (size: 46.4 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[43] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO cluster.YarnScheduler: Adding task set 21.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 54, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 55, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:38619 (size: 46.4 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:33 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:30 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_20201113030230_0020_m_000000_53' to s3a://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-11-13-02-57-16/output/success-metrics\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:32 INFO mapred.SparkHadoopMapRedUtil: attempt_20201113030230_0020_m_000000_53: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:32 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 53). 2434 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 54)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO executor.Executor: Running task 1.0 in stage 21.0 (TID 55)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 46.4 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 144.1 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO datasources.FileScanRDD: TID: 54 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO datasources.FileScanRDD: TID: 55 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO codegen.CodeGenerator: Code generated in 7.541323 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 55) in 3159 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 54) in 4076 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (collect at AnalysisRunner.scala:323) finished in 4.085 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:37 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 566.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Got job 14 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[46] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 175.4 KB, free 911.7 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 57.2 KB, free 911.6 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.198.187:41113 (size: 57.2 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[46] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 56, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:38619 (size: 57.2 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:33 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 385.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 10.046755 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 20.715841 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 6.920665 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 141.881766 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 6.114182 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.450952 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.755938 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.276478 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.393224 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.267725 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 6.346104 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.523683 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.743606 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.942371 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.461945 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:34 INFO codegen.CodeGenerator: Code generated in 4.394731 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:37 INFO executor.Executor: Finished task 1.0 in stage 21.0 (TID 55). 2386 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:37 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 54). 2386 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 56) in 281 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: ResultStage 23 (collect at AnalysisRunner.scala:323) finished in 0.289 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Job 14 finished: collect at AnalysisRunner.scala:323, took 0.290238 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: string, product_parent: string, star_rating: int, helpful_votes: int, total_votes: int ... 3 more fields>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 33.945886 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 38.723645 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 258.1 KB, free 911.4 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 22.0 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO spark.SparkContext: Created broadcast 20 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Registering RDD 49 (collect at AnalysisRunner.scala:323) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Got map stage job 15 (collect at AnalysisRunner.scala:323) with 2 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 53.6 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.4 KB, free 911.3 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.198.187:41113 (size: 19.4 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO cluster.YarnScheduler: Adding task set 24.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 57, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 58, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:38619 (size: 19.4 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:38 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 56)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 57.2 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 175.4 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 7, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 16.858665 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 7.547618 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 11.359318 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 56). 7720 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 57)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO executor.Executor: Running task 1.0 in stage 24.0 (TID 58)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:39 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 58) in 1204 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 57) in 1470 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:323) finished in 1.477 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 25.\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO codegen.CodeGenerator: Code generated in 32.331508 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Got job 16 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 50.7 KB, free 911.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 16.2 KB, free 911.2 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.198.187:41113 (size: 16.2 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 59, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:38619 (size: 16.2 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 59) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:323) finished in 0.061 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Job 16 finished: collect at AnalysisRunner.scala:323, took 0.062367 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.4 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 53.6 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 35.611274 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO datasources.FileScanRDD: TID: 58 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO datasources.FileScanRDD: TID: 57 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO codegen.CodeGenerator: Code generated in 6.864082 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:38 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 385.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:39 INFO executor.Executor: Finished task 1.0 in stage 24.0 (TID 58). 1740 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 57). 1740 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 59)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO spark.MapOutputTrackerWorker: Updating epoch to 9 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 16.2 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 5 ms\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 258.1 KB, free 911.0 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 22.0 KB, free 910.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.198.187:41113 (size: 22.0 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO spark.SparkContext: Created broadcast 23 from rdd at ColumnProfiler.scala:591\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 2, prefetch: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,2))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:605\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Registering RDD 59 (countByKey at ColumnProfiler.scala:605) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Got job 17 (countByKey at ColumnProfiler.scala:605) with 16 output partitions\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:605)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[59] at countByKey at ColumnProfiler.scala:605), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 21.2 KB, free 910.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 10.5 KB, free 910.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.198.187:41113 (size: 10.5 KB, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[59] at countByKey at ColumnProfiler.scala:605) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO cluster.YarnScheduler: Adding task set 27.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 60, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 61, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:40 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:38619 (size: 10.5 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:41 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:38619 (size: 22.0 KB, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 50.7 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO codegen.CodeGenerator: Code generated in 36.548179 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 59). 2197 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 60)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO executor.Executor: Running task 1.0 in stage 27.0 (TID 61)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 10.5 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:40 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 21.2 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO codegen.CodeGenerator: Code generated in 9.856716 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO datasources.FileScanRDD: TID: 60 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 61) in 2691 ms on algo-1 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 60) in 3344 ms on algo-1 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:605) finished in 3.355 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[60] at countByKey at ColumnProfiler.scala:605), which has no missing parents\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 3.1 KB, free 910.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 1921.0 B, free 910.9 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.198.187:41113 (size: 1921.0 B, free: 912.1 MB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ResultStage 28 (ShuffledRDD[60] at countByKey at ColumnProfiler.scala:605) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO cluster.YarnScheduler: Adding task set 28.0 with 16 tasks\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 28.0 (TID 62, algo-1, executor 1, partition 2, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 28.0 (TID 63, algo-1, executor 1, partition 5, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 28.0 (TID 64, algo-1, executor 1, partition 7, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 28.0 (TID 65, algo-1, executor 1, partition 8, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 28.0 (TID 66, algo-1, executor 1, partition 9, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 28.0 (TID 67, algo-1, executor 1, partition 10, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 28.0 (TID 68, algo-1, executor 1, partition 11, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 28.0 (TID 69, algo-1, executor 1, partition 14, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:38619 (size: 1921.0 B, free: 28.9 GB)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.198.187:47504\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 28.0 (TID 70, algo-1, executor 1, partition 15, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 28.0 (TID 62) in 31 ms on algo-1 (executor 1) (1/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 71, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 28.0 (TID 72, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 28.0 (TID 63) in 31 ms on algo-1 (executor 1) (2/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 28.0 (TID 65) in 31 ms on algo-1 (executor 1) (3/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 28.0 (TID 73, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 28.0 (TID 74, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 28.0 (TID 75, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 28.0 (TID 76, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 28.0 (TID 67) in 32 ms on algo-1 (executor 1) (4/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 28.0 (TID 69) in 32 ms on algo-1 (executor 1) (5/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 28.0 (TID 66) in 32 ms on algo-1 (executor 1) (6/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 28.0 (TID 64) in 34 ms on algo-1 (executor 1) (7/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 28.0 (TID 77, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 28.0 (TID 68) in 33 ms on algo-1 (executor 1) (8/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 28.0 (TID 72) in 5 ms on algo-1 (executor 1) (9/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 28.0 (TID 70) in 8 ms on algo-1 (executor 1) (10/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 71) in 8 ms on algo-1 (executor 1) (11/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 28.0 (TID 73) in 9 ms on algo-1 (executor 1) (12/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 28.0 (TID 75) in 9 ms on algo-1 (executor 1) (13/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 28.0 (TID 76) in 9 ms on algo-1 (executor 1) (14/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 28.0 (TID 74) in 9 ms on algo-1 (executor 1) (15/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 28.0 (TID 77) in 8 ms on algo-1 (executor 1) (16/16)\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:605) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m20/11/13 03:02:43 INFO scheduler.DAGScheduler: Job 17 finished: countByKey at ColumnProfiler.scala:605, took 3.405353 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO datasources.FileScanRDD: TID: 61 - Reading current file: path: s3a://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, r<class 'dict'>\u001b[0m\n",
      "\u001b[34m{'constraint_suggestions': [{'constraint_name': 'CompletenessConstraint(Completeness(review_id,None))', 'column_name': 'review_id', 'current_value': 'Completeness: 1.0', 'description': \"'review_id' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"review_id\")'}, {'constraint_name': 'UniquenessConstraint(Uniqueness(List(review_id),None))', 'column_name': 'review_id', 'current_value': 'ApproxDistinctness: 0.9616669696786053', 'description': \"'review_id' is unique\", 'suggesting_rule': 'UniqueIfApproximatelyUniqueRule()', 'rule_description': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint', 'code_for_constraint': '.isUnique(\"review_id\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(customer_id,None))', 'column_name': 'customer_id', 'current_value': 'Completeness: 1.0', 'description': \"'customer_id' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"customer_id\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('customer_id' has no negative values,customer_id >= 0,None))\", 'column_name': 'customer_id', 'current_value': 'Minimum: 10229.0', 'description': \"'customer_id' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"customer_id\")'}, {'constraint_name': 'AnalysisBasedConstraint(DataType(customer_id,None),<function1>,Some(<function1>),None)', 'column_name': 'customer_id', 'current_value': 'DataType: Integral', 'description': \"'customer_id' has type Integral\", 'suggesting_rule': 'RetainTypeRule()', 'rule_description': 'If we detect a non-string type, we suggest a type constraint', 'code_for_constraint': '.hasDataType(\"customer_id\", ConstrainableDataTypes.Integral)'}, {'constraint_name': 'CompletenessConstraint(Completeness(review_date,None))', 'column_name': 'review_date', 'current_value': 'Completeness: 1.0', 'description': \"'review_date' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"review_date\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(helpful_votes,None))', 'column_name': 'helpful_votes', 'current_value': 'Completeness: 1.0', 'description': \"'helpful_votes' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"helpful_votes\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('helpful_votes' has no negative values,helpful_votes >= 0,None))\", 'column_name': 'helpful_votes', 'current_value': 'Minimum: 0.0', 'description': \"'helpful_votes' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"helpful_votes\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(star_rating,None))', 'column_name': 'star_rating', 'current_value': 'Completeness: 1.0', 'description': \"'star_rating' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"star_rating\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('star_rating' has no negative values,star_rating >= 0,None))\", 'column_name': 'star_rating', 'current_value': 'Minimum: 1.0', 'description': \"'star_rating' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"star_rating\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(product_title,None))', 'column_name': 'product_title', 'current_value': 'Completeness: 1.0', 'description': \"'product_title' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"product_title\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(review_headline,None))', 'column_name': 'review_headline', 'current_value': 'Completeness: 1.0', 'description': \"'review_headline' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"review_headline\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(product_id,None))', 'column_name': 'product_id', 'current_value': 'Completeness: 1.0', 'description': \"'product_id' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"product_id\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(total_votes,None))', 'column_name': 'total_votes', 'current_value': 'Completeness: 1.0', 'description': \"'total_votes' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"total_votes\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('total_votes' has no negative values,total_votes >= 0,None))\", 'column_name': 'total_votes', 'current_value': 'Minimum: 0.0', 'description': \"'total_votes' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"total_votes\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('product_category' has value range 'Digital_Video_Games', 'Digital_Software',`product_category` IN ('Digital_Video_Games', 'Digital_Software'),None))\", 'column_name': 'product_category', 'current_value': 'Compliance: 1', 'description': \"'product_category' has value range 'Digital_Video_Games', 'Digital_Software'\", 'suggesting_rule': 'CategoricalRangeRule()', 'rule_description': 'If we see a categorical range for a column, we suggest an IS IN (...) constraint', 'code_for_constraint': '.isContainedIn(\"product_category\", [\"Digital_Video_Games\", \"Digital_Software\"])'}, {'constraint_name': 'CompletenessConstraint(Completeness(product_category,None))', 'column_name': 'product_category', 'current_value': 'Completeness: 1.0', 'description': \"'product_category' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"product_category\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(product_parent,None))', 'column_name': 'product_parent', 'current_value': 'Completeness: 1.0', 'description': \"'product_parent' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"product_parent\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('product_parent' has no negative values,product_parent >= 0,None))\", 'column_name': 'product_parent', 'current_value': 'Minimum: 209709.0', 'description': \"'product_parent' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"product_parent\")'}, {'constraint_name': 'AnalysisBasedConstraint(DataType(product_parent,None),<function1>,Some(<function1>),None)', 'column_name': 'product_parent', 'current_value': 'DataType: Integral', 'description': \"'product_parent' has type Integral\", 'suggesting_rule': 'RetainTypeRule()', 'rule_description': 'If we detect a non-string type, we suggest a type constraint', 'code_for_constraint': '.hasDataType(\"product_parent\", ConstrainableDataTypes.Integral)'}, {'constraint_name': 'CompletenessConstraint(Completeness(review_body,None))', 'column_name': 'review_body', 'current_value': 'Completeness: 0.9999919196816355', 'description': \"'review_body' has less than 1% missing values\", 'suggesting_rule': 'RetainCompletenessRule()', 'rule_description': 'If a column is incomplete in the sample, we model its completeness as a binomial variable, estimate a confidence interval and use this to define a lower bound for the completeness', 'code_for_constraint': '.hasCompleteness(\"review_body\", lambda x: x >= 0.99, \"It should be above 0.99!\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('vine' has value range 'N',`vine` IN ('N'),None))\", 'column_name': 'vine', 'current_value': 'Compliance: 1', 'description': \"'vine' has value range 'N'\", 'suggesting_rule': 'CategoricalRangeRule()', 'rule_description': 'If we see a categorical range for a column, we suggest an IS IN (...) constraint', 'code_for_constraint': '.isContainedIn(\"vine\", [\"N\"])'}, {'constraint_name': 'CompletenessConstraint(Completeness(vine,None))', 'column_name': 'vine', 'current_value': 'Completeness: 1.0', 'description': \"'vine' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"vine\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('marketplace' has value range 'US',`marketplace` IN ('US'),None))\", 'column_name': 'marketplace', 'current_value': 'Compliance: 1', 'description': \"'marketplace' has value range 'US'\", 'suggesting_rule': 'CategoricalRangeRule()', 'rule_description': 'If we see a categorical range for a column, we suggest an IS IN (...) constraint', 'code_for_constraint': '.isContainedIn(\"marketplace\", [\"US\"])'}, {'constraint_name': 'CompletenessConstraint(Completeness(marketplace,None))', 'column_name': 'marketplace', 'current_value': 'Completeness: 1.0', 'description': \"'marketplace' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"marketplace\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('verified_purchase' has value range 'Y', 'N',`verified_purchase` IN ('Y', 'N'),None))\", 'column_name': 'verified_purchase', 'current_value': 'Compliance: 1', 'description': \"'verified_purchase' has value range 'Y', 'N'\", 'suggesting_rule': 'CategoricalRangeRule()', 'rule_description': 'If we see a categorical range for a column, we suggest an IS IN (...) constraint', 'code_for_constraint': '.isContainedIn(\"verified_purchase\", [\"Y\", \"N\"])'}, {'constraint_name': 'CompletenessConstraint(Completeness(verified_purchase,None))', 'column_name': 'verified_purchase', 'current_value': 'Completeness: 1.0', 'description': \"'verified_purchase' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"verified_purchase\")'}]}\u001b[0m\n",
      "\u001b[34m{\n",
      "  \"constraint_suggestions\": [\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(review_id,None))\",\n",
      "      \"column_name\": \"review_id\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'review_id' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"review_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"UniquenessConstraint(Uniqueness(List(review_id),None))\",\n",
      "      \"column_name\": \"review_id\",\n",
      "      \"current_value\": \"ApproxDistinctness: 0.9616669696786053\",\n",
      "      \"description\": \"'review_id' is unique\",\n",
      "      \"suggesting_rule\": \"UniqueIfApproximatelyUniqueRule()\",\n",
      "      \"rule_description\": \"If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint\",\n",
      "      \"code_for_constraint\": \".isUnique(\\\"review_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(customer_id,None))\",\n",
      "      \"column_name\": \"customer_id\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'customer_id' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"customer_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('customer_id' has no negative values,customer_id >= 0,None))\",\n",
      "      \"column_name\": \"customer_id\",\n",
      "      \"current_value\": \"Minimum: 10229.0\",\n",
      "      \"description\": \"'customer_id' has no negative values\",\n",
      "      \"suggesting_rule\": \"NonNegativeNumbersRule()\",\n",
      "      \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",\n",
      "      \"code_for_constraint\": \".isNonNegative(\\\"customer_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"AnalysisBasedConstraint(DataType(customer_id,None),<function1>,Some(<function1>),None)\",\n",
      "      \"column_name\": \"customer_id\",\n",
      "      \"current_value\": \"DataType: Integral\",\n",
      "      \"description\": \"'customer_id' has type Integral\",\n",
      "      \"suggesting_rule\": \"RetainTypeRule()\",\n",
      "      \"rule_description\": \"If we detect a non-string type, we suggest a type constraint\",\n",
      "      \"code_for_constraint\": \".hasDataType(\\\"customer_id\\\", ConstrainableDataTypes.Integral)\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(review_date,None))\",\n",
      "      \"column_name\": \"review_date\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'review_date' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"review_date\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(helpful_votes,None))\",\n",
      "      \"column_name\": \"helpful_votes\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'helpful_votes' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"helpful_votes\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('helpful_votes' has no negative values,helpful_votes >= 0,None))\",\n",
      "      \"column_name\": \"helpful_votes\",\n",
      "      \"current_value\": \"Minimum: 0.0\",\n",
      "      \"description\": \"'helpful_votes' has no negative values\",\n",
      "      \"suggesting_rule\": \"NonNegativeNumbersRule()\",\n",
      "      \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",\n",
      "      \"code_for_constraint\": \".isNonNegative(\\\"helpful_votes\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(star_rating,None))\",\n",
      "      \"column_name\": \"star_rating\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'star_rating' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"star_rating\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('star_rating' has no negative values,star_rating >= 0,None))\",\n",
      "      \"column_name\": \"star_rating\",\n",
      "      \"current_value\": \"Minimum: 1.0\",\n",
      "      \"description\": \"'star_rating' has no negative values\",\n",
      "      \"suggesting_rule\": \"NonNegativeNumbersRule()\",\n",
      "      \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",\n",
      "      \"code_for_constraint\": \".isNonNegative(\\\"star_rating\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(product_title,None))\",\n",
      "      \"column_name\": \"product_title\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'product_title' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"product_title\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(review_headline,None))\",\n",
      "      \"column_name\": \"review_headline\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'review_headline' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"review_headline\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(product_id,None))\",\n",
      "      \"column_name\": \"product_id\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'product_id' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"product_id\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(total_votes,None))\",\n",
      "      \"column_name\": \"total_votes\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'total_votes' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"total_votes\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('total_votes' has no negative values,total_votes >= 0,None))\",\n",
      "      \"column_name\": \"total_votes\",\n",
      "      \"current_value\": \"Minimum: 0.0\",\n",
      "      \"description\": \"'total_votes' has no negative values\",\n",
      "      \"suggesting_rule\": \"NonNegativeNumbersRule()\",\n",
      "      \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",\n",
      "      \"code_for_constraint\": \".isNonNegative(\\\"total_votes\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('product_category' has value range 'Digital_Video_Games', 'Digital_Software',`product_category` IN ('Digital_Video_Games', 'Digital_Software'),None))\",\n",
      "      \"column_name\": \"product_category\",\n",
      "      \"current_value\": \"Compliance: 1\",\n",
      "      \"description\": \"'product_category' has value range 'Digital_Video_Games', 'Digital_Software'\",\n",
      "      \"suggesting_rule\": \"CategoricalRangeRule()\",\n",
      "      \"rule_description\": \"If we see a categorical range for a column, we suggest an IS IN (...) constraint\",\n",
      "      \"code_for_constraint\": \".isContainedIn(\\\"product_category\\\", [\\\"Digital_Video_Games\\\", \\\"Digital_Software\\\"])\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(product_category,None))\",\n",
      "      \"column_name\": \"product_category\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'product_category' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"product_category\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(product_parent,None))\",\n",
      "      \"column_name\": \"product_parent\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'product_parent' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"product_parent\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('product_parent' has no negative values,product_parent >= 0,None))\",\n",
      "      \"column_name\": \"product_parent\",\n",
      "      \"current_value\": \"Minimum: 209709.0\",\n",
      "      \"description\": \"'product_parent' has no negative values\",\n",
      "      \"suggesting_rule\": \"NonNegativeNumbersRule()\",\n",
      "      \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",\n",
      "      \"code_for_constraint\": \".isNonNegative(\\\"product_parent\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"AnalysisBasedConstraint(DataType(product_parent,None),<function1>,Some(<function1>),None)\",\n",
      "      \"column_name\": \"product_parent\",\n",
      "      \"current_value\": \"DataType: Integral\",\n",
      "      \"description\": \"'product_parent' has type Integral\",\n",
      "      \"suggesting_rule\": \"RetainTypeRule()\",\n",
      "      \"rule_description\": \"If we detect a non-string type, we suggest a type constraint\",\n",
      "      \"code_for_constraint\": \".hasDataType(\\\"product_parent\\\", ConstrainableDataTypes.Integral)\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(review_body,None))\",\n",
      "      \"column_name\": \"review_body\",\n",
      "      \"current_value\": \"Completeness: 0.9999919196816355\",\n",
      "      \"description\": \"'review_body' has less than 1% missing values\",\n",
      "      \"suggesting_rule\": \"RetainCompletenessRule()\",\n",
      "      \"rule_description\": \"If a column is incomplete in the sample, we model its completeness as a binomial variable, estimate a confidence interval and use this to define a lower bound for the completeness\",\n",
      "      \"code_for_constraint\": \".hasCompleteness(\\\"review_body\\\", lambda x: x >= 0.99, \\\"It should be above 0.99!\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('vine' has value range 'N',`vine` IN ('N'),None))\",\n",
      "      \"column_name\": \"vine\",\n",
      "      \"current_value\": \"Compliance: 1\",\n",
      "      \"description\": \"'vine' has value range 'N'\",\n",
      "      \"suggesting_rule\": \"CategoricalRangeRule()\",\n",
      "      \"rule_description\": \"If we see a categorical range for a column, we suggest an IS IN (...) constraint\",\n",
      "      \"code_for_constraint\": \".isContainedIn(\\\"vine\\\", [\\\"N\\\"])\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(vine,None))\",\n",
      "      \"column_name\": \"vine\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'vine' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"vine\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('marketplace' has value range 'US',`marketplace` IN ('US'),None))\",\n",
      "      \"column_name\": \"marketplace\",\n",
      "      \"current_value\": \"Compliance: 1\",\n",
      "      \"description\": \"'marketplace' has value range 'US'\",\n",
      "      \"suggesting_rule\": \"CategoricalRangeRule()\",\n",
      "      \"rule_description\": \"If we see a categorical range for a column, we suggest an IS IN (...) constraint\",\n",
      "      \"code_for_constraint\": \".isContainedIn(\\\"marketplace\\\", [\\\"US\\\"])\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(marketplace,None))\",\n",
      "      \"column_name\": \"marketplace\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'marketplace' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"marketplace\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"ComplianceConstraint(Compliance('verified_purchase' has value range 'Y', 'N',`verified_purchase` IN ('Y', 'N'),None))\",\n",
      "      \"column_name\": \"verified_purchase\",\n",
      "      \"current_value\": \"Compliance: 1\",\n",
      "      \"description\": \"'verified_purchase' has value range 'Y', 'N'\",\n",
      "      \"suggesting_rule\": \"CategoricalRangeRule()\",\n",
      "      \"rule_description\": \"If we see a categorical range for a column, we suggest an IS IN (...) constraint\",\n",
      "      \"code_for_constraint\": \".isContainedIn(\\\"verified_purchase\\\", [\\\"Y\\\", \\\"N\\\"])\"\n",
      "    },\n",
      "    {\n",
      "      \"constraint_name\": \"CompletenessConstraint(Completeness(verified_purchase,None))\",\n",
      "      \"column_name\": \"verified_purchase\",\n",
      "      \"current_value\": \"Completeness: 1.0\",\n",
      "      \"description\": \"'verified_purchase' is not null\",\n",
      "      \"suggesting_rule\": \"CompleteIfCompleteRule()\",\n",
      "      \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",\n",
      "      \"code_for_constraint\": \".isComplete(\\\"verified_purchase\\\")\"\n",
      "    }\n",
      "  ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mange: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 22.0 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:41 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 385.3 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 1.0 in stage 27.0 (TID 61). 1962 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 60). 1919 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 2.0 in stage 28.0 (TID 62)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 5.0 in stage 28.0 (TID 63)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 7.0 in stage 28.0 (TID 64)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Updating epoch to 10 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 1921.0 B, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 8.0 in stage 28.0 (TID 65)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 9.0 in stage 28.0 (TID 66)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 10.0 in stage 28.0 (TID 67)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 14.0 in stage 28.0 (TID 69)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 11.0 in stage 28.0 (TID 68)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 3.2 KB, free 28.9 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.198.187:32775)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 2.0 in stage 28.0 (TID 62). 1344 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 5.0 in stage 28.0 (TID 63). 1364 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 8.0 in stage 28.0 (TID 65). 1302 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 14.0 in stage 28.0 (TID 69). 1350 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 10.0 in stage 28.0 (TID 67). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 9.0 in stage 28.0 (TID 66). 1347 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 7.0 in stage 28.0 (TID 64). 1371 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 11.0 in stage 28.0 (TID 68). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 15.0 in stage 28.0 (TID 70)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 71)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 1.0 in stage 28.0 (TID 72)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 1.0 in stage 28.0 (TID 72). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 15.0 in stage 28.0 (TID 70). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 3.0 in stage 28.0 (TID 73)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 4.0 in stage 28.0 (TID 74)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 71). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 12.0 in stage 28.0 (TID 76)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 6.0 in stage 28.0 (TID 75)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 3.0 in stage 28.0 (TID 73). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 12.0 in stage 28.0 (TID 76). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Finished task 6.0 in stage 28.0 (TID 75). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO executor.Executor: Running task 13.0 in stage 28.0 (TID 77)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1605236475429_0001/container_1605236475429_0001_01_000002/stderr] 20/11/13 03:02:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m20/11/13 03:11:15 INFO scheduler.AbstractYarnScheduler: Release request cache is cleaned up\u001b[0m\n",
      "\u001b[34m20/11/13 03:11:15 INFO localizer.ResourceLocalizationService: Cache Size Before Clean: 393786438, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_constraint_checks = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-checks/', sep='\\t', header=0)\n",
    "df_constraint_checks[['check', 'constraint', 'constraint_status', 'constraint_message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/dataset-metrics/', sep='\\t', header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_success_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/success-metrics/', sep='\\t', header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions.columns=['column_name', 'description', 'code']\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
