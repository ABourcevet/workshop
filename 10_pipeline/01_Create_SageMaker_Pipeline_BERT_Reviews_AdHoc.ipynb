{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating Jobs, Model Registration, Continuous Deployment, and Lineage Tracking with Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker offers Machine Learning application developers and Machine Learning operations engineers the ability to orchestrate SageMaker jobs and author reproducible Machine Learning pipelines, deploy custom-build models for inference in real-time with low latency or offline inferences with Batch Transform, and track lineage of artifacts. You can institute sound operational practices in deploying and monitoring production workflows, deployment of model artifacts, and track artifact lineage through a simple interface, adhering to safety and best-practice paradigmsfor Machine Learning application development.\n",
    "\n",
    "The SageMaker Workflow service supports a SageMaker Machine Learning Pipeline Domain Specific Language (DSL), which is a declarative Json specification. This DSL defines a Directed Acyclic Graph (DAG) of pipeline parameters and SageMaker job steps. The SageMaker Python Software Developer Kit (SDK) streamlines the generation of the pipeline DSL using constructs that are already familiar to engineers and scientists alike.\n",
    "\n",
    "The SageMaker Model Registry is where trained models are stored, versioned, and managed. Data Scientists and Machine Learning Engineers can compare model versions, approve models for deployment, and deploy models from different AWS accounts, all from a single Model Registry. SageMaker enables customers to follow the best practices with ML Ops and getting started right. Customers are able to standup a full ML Ops end-to-end system with a single API call.\n",
    "\n",
    "And the SageMaker Lineage service makes it easy to track all the artifacts created in a SageMaker Machine Learning Pipeline from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Pipelines\n",
    "\n",
    "Amazon SageMaker Pipelines support the following:\n",
    "\n",
    "* Pipelines - A Directed Acyclic Graph of steps and conditions to orchestrate SageMaker jobs and resource creation.\n",
    "* Processing Job steps - A simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation.\n",
    "* Training Job steps - An iterative process that teaches a model to make predictions by presenting examples from a training dataset.\n",
    "* Conditional step execution - Provides conditional execution of branches in a pipeline.\n",
    "* Registering Models - Creates a model package resource in the Model Registry that can be used to create deployable models in Amazon SageMaker.\n",
    "* Parametrized Pipeline executions - Allows pipeline executions to vary by supplied parameters.\n",
    "* Transform Job steps - A batch transform to preprocess datasets to remove noise or bias that interferes with training or inference from your dataset, get inferences from large datasets, and run inference when you don't need a persistent endpoint.\n",
    "\n",
    "## SageMaker Model Registry\n",
    "\n",
    "Amazon SageMaker Model Registry supports the following:\n",
    "\n",
    "* Catalog models after the training step - data scientists run tens to thousands of experiments and may select a small set of models as candidates for production.\n",
    "* Manage model versions - data scientists can register new models which will be automatically versioned in the model registry.\n",
    "* Compare models - data scientists can run model evaluation steps in Tioga pipeline and generate model metrics (e.g. accuracy metrics and bias metrics) which are recorded in the Model Registry and can be used to compare model versions.\n",
    "* Approve models - data scientists can mark model versions as “approved” or “rejected”. Alternately, the Tioga pipeline can also automate the model approvals. If there is a deployment pipeline associated with a Model and a live endpoint, then the model version is propagated in production. Currently SageMaker supports a Blue/ Green update, but as part of Yosemite, we are adding support for Canary and Rolling deployments also.\n",
    "* Deploy models in different AWS accounts - models in the Model Registry support resource sharing across accounts which enables models built in data scientist accounts to be deployed in different pre-production and production accounts.\n",
    "\n",
    "## SageMaker Lineage\n",
    "\n",
    "Amazon SageMaker Lineage supports the following:\n",
    "\n",
    "* Automatically tracks all the artifacts created in a machine learning workflow from start to finish.  Modeled as a directed graph like structure.\n",
    "* Explore the lineage artifacts with easy to use SDK methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "### SageMaker Workflows\n",
    "\n",
    "* Define a set of Workflow Parameters that can be used to parametrize a Workflow Pipeline\n",
    "* Define a Processing step that performs cleaning and feature engineering, splitting the input data into train and test data sets\n",
    "* Define a Training step that trains a model on the pre-processed train data set\n",
    "* Define a Processing step that evaluates the trained model's performance on the test data set\n",
    "* Define a Register Model step that creates a model package from the estimator and model artifacts used in training\n",
    "* Define a Conditional step that measures a condition based on output from prior steps and conditionally executes the Register Model step\n",
    "* Define and create a Pipeline in a Workflow DAG, with the defined parameters and steps defined\n",
    "* Start a Pipeline execution and wait for execution to complete\n",
    "* Download from S3 the model evaluation report for examination\n",
    "* Start a second Pipeline execution\n",
    "\n",
    "### SageMaker Model Registry\n",
    "\n",
    "* Create a SageMaker Project based on the Model Package Group name from the pipeline execution defined before\n",
    "* Observe CI/CD code pipeline on subsequent successful executions of the pipeline and the registration of a new Model Package version.\n",
    "\n",
    "### SageMaker Lineage\n",
    "\n",
    "Amazon SageMaker Lineage supports the following:\n",
    "\n",
    "* Provide the inputs and outputs of SageMaker job artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SageMaker Workflow\n",
    "\n",
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and model registration:\n",
    "\n",
    "![A typical ML Application pipeline](img/pipeline-full.png)\n",
    "\n",
    "### Create SageMaker Clients and Session\n",
    "\n",
    "First, we create a new SageMaker Session in the `us-east-2` region. We also acquire the role arn for the session.\n",
    "\n",
    "This role arn should be the execution role arn that you set up in the Prerequisites section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q sagemaker==2.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q sagemaker-experiments==0.1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16096406156145956\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = str(int(time.time() * 10**7))\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Pipeline In `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'BERT-pipeline-{}'.format(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: BERT-pipeline-16096406156145956\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "experiment = Experiment.create(\n",
    "                experiment_name=pipeline_name,\n",
    "                description='Amazon Customer Reviews BERT Pipeline Experiment', \n",
    "                sagemaker_boto_client=sm)\n",
    "\n",
    "experiment_name = experiment.experiment_name\n",
    "print('Experiment name: {}'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1609640615\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial = Trial.create(trial_name='trial-{}'.format(timestamp),\n",
    "                     experiment_name=experiment_name,\n",
    "                     sagemaker_boto_client=sm)\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print('Trial name: {}'.format(trial_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Experiment Config`'s for Each Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config_prepare = {\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': trial_name,\n",
    "    'TrialComponentDisplayName': 'prepare'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config_train = {\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': trial_name,\n",
    "    'TrialComponentDisplayName': 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config_evaluate = {\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': trial_name,\n",
    "    'TrialComponentDisplayName': 'evaluate'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config_model_register = {\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': trial_name,\n",
    "    'TrialComponentDisplayName': 'model_register'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Raw Inputs S3 Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-02 02:04:30   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2021-01-02 02:04:31   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Processing Job Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length=64\n",
    "# train_split_percentage=0.90\n",
    "# validation_split_percentage=0.05\n",
    "# test_split_percentage=0.05\n",
    "balance_dataset='True'\n",
    "# processing_instance_count=1\n",
    "# processing_instance_type='ml.c5.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "We define Workflow Parameters by which we can parametrize our Pipeline and vary the values injected and used in Pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - representing a `str` Python type\n",
    "* `ParameterInteger` - representing an `int` Python type\n",
    "* `ParameterFloat` - representing a `float` Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow below include:\n",
    "\n",
    "* `processing_instance_type` - The `ml.*` instance type of the processing job.\n",
    "* `processing_instance_count` - The instance count of the processing job. For illustrative purposes only: 1 is the only value that makes sense here.\n",
    "* `training_instance_type` - The `ml.*` instance type of the training job.\n",
    "* `model_approval_status` - What approval status to register the trained model with for CI/CD purposes. Defaults to \"PendingManualApproval\". (NOTE: not available in service yet)\n",
    "* `input_data` - The URL location of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "#    ParameterBoolean\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = ParameterString(\n",
    "    name=\"ExperimentName\",\n",
    "    default_value=experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = ParameterFloat(\n",
    "    name=\"TrainSplitPercentage\",\n",
    "    default_value=0.90,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_input_data_s3_uri,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.c5.2xlarge\"\n",
    ")\n",
    "\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=64,\n",
    ")\n",
    "\n",
    "balance_dataset = ParameterString(\n",
    "    name=\"BalanceDataset\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "    \n",
    "train_split_percentage = ParameterFloat(\n",
    "    name=\"TrainSplitPercentage\",\n",
    "    default_value=0.90,\n",
    ")\n",
    "\n",
    "validation_split_percentage = ParameterFloat(\n",
    "    name=\"ValidationSplitPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "test_split_percentage = ParameterFloat(\n",
    "    name=\"TestSplitPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "feature_store_offline_prefix = ParameterString(\n",
    "    name=\"FeatureStoreOfflinePrefix\",\n",
    "    default_value=\"reviews-feature-store-\" + str(timestamp),\n",
    ")\n",
    "\n",
    "feature_group_name = ParameterString(\n",
    "    name=\"FeatureGroupName\",\n",
    "    default_value=\"reviews-feature-group-\" + str(timestamp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33manaconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.3.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker==2.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_group\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m FeatureGroup\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_definition\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\r\n",
      "    FeatureDefinition,\r\n",
      "    FeatureTypeEnum,\r\n",
      ")\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m############################\u001b[39;49;00m\r\n",
      "region = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRegion: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(region))\r\n",
      "\u001b[37m############################\u001b[39;49;00m\r\n",
      "\r\n",
      "sm = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\r\n",
      "\r\n",
      "featurestore_runtime = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-featurestore-runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\r\n",
      "\r\n",
      "s3 = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\r\n",
      "\r\n",
      "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name=region), sagemaker_client=sm, sagemaker_featurestore_runtime_client=featurestore_runtime)\r\n",
      "\r\n",
      "role = sagemaker.get_execution_role()\r\n",
      "bucket = sagemaker_session.default_bucket()\r\n",
      "\r\n",
      "\u001b[37m############################\u001b[39;49;00m\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "REVIEW_BODY_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "REVIEW_ID_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\r\n",
      "\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "    \r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\r\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\r\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m'\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\r\n",
      "            \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mwait_for_feature_group_creation_complete\u001b[39;49;00m(feature_group):\r\n",
      "    status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwhile\u001b[39;49;00m status == \u001b[33m\"\u001b[39;49;00m\u001b[33mCreating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for Feature Group Creation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        time.sleep(\u001b[34m5\u001b[39;49;00m)\r\n",
      "        status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m status != \u001b[33m\"\u001b[39;49;00m\u001b[33mCreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFailed to create feature group \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroup \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m successfully created.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "            \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_or_load_feature_group\u001b[39;49;00m(prefix, feature_group_name):\r\n",
      "\r\n",
      "    \u001b[37m# Feature Definitions for our records\u001b[39;49;00m\r\n",
      "    feature_definitions= [\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\r\n",
      "\u001b[37m#        FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING)            \r\n",
      "    ]\r\n",
      "    \r\n",
      "    feature_group = FeatureGroup(\r\n",
      "        name=feature_group_name,\r\n",
      "        feature_definitions=feature_definitions,\r\n",
      "        sagemaker_session=sagemaker_session)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group))\r\n",
      "    \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:                \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for existing Feature Group to become available if it is being created by another instance in our cluster...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        wait_for_feature_group_creation_complete(feature_group)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "        \u001b[34mpass\u001b[39;49;00m\r\n",
      "        \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        record_identifier_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        event_time_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        feature_group.create(\r\n",
      "            s3_uri=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{prefix}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            record_identifier_name=record_identifier_feature_name,\r\n",
      "            event_time_feature_name=event_time_feature_name,\r\n",
      "            role_arn=role,\r\n",
      "            enable_online_store=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "        )\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group. Completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for new Feature Group to become available...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        wait_for_feature_group_creation_complete(feature_group)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)        \r\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "        \u001b[34mpass\u001b[39;49;00m\r\n",
      "        \r\n",
      "    feature_group.describe()        \r\n",
      "        \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m feature_group\r\n",
      "\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id,\r\n",
      "               review_id,\r\n",
      "               date,\r\n",
      "               label):\r\n",
      "\u001b[37m#               review_body):\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\r\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\r\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "\u001b[37m#        self.review_body = review_body\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.review_id = review_id\r\n",
      "    \u001b[36mself\u001b[39;49;00m.date = date\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    tokens = tokenizer.tokenize(the_input.text)    \r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(the_input.text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=max_seq_length,\r\n",
      "\u001b[37m#                                               truncation=True\u001b[39;49;00m\r\n",
      "                                              )\r\n",
      "\r\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\r\n",
      "\r\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[the_input.label]\r\n",
      "\r\n",
      "    features = InputFeatures(\r\n",
      "        input_ids=input_ids,\r\n",
      "        input_mask=input_mask,\r\n",
      "        segment_ids=segment_ids,\r\n",
      "        label_id=label_id,\r\n",
      "        review_id=the_input.review_id,\r\n",
      "        date=the_input.date,\r\n",
      "        label=the_input.label)\r\n",
      "\u001b[37m#        review_body=the_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#     print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**review_id**\\n{}\\n'.format(features.review_id))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**date**\\n{}\\n'.format(features.date))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**label**\\n{}\\n'.format(features.label))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**review_body**\\n{}\\n'.format(features.review_body))\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs,\r\n",
      "                                 output_file,\r\n",
      "                                 max_seq_length):\r\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    records = []\r\n",
      "\r\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\r\n",
      "    \r\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\r\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\r\n",
      "\r\n",
      "        features = convert_input(the_input, max_seq_length)\r\n",
      "\r\n",
      "        all_features = collections.OrderedDict()\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\r\n",
      "\r\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\r\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\r\n",
      "\r\n",
      "        records.append({\u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_ids,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_mask,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.segment_ids,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label_id,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.review_id,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.date,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label,\r\n",
      "\u001b[37m#                        'review_body': features.review_body\u001b[39;49;00m\r\n",
      "                       })\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################            \u001b[39;49;00m\r\n",
      "        \u001b[37m# break\u001b[39;49;00m\r\n",
      "        \r\n",
      "    tf_record_writer.close()\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m records\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "    )  \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "    ) \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "    ) \r\n",
      "        \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, \r\n",
      "                               max_seq_length, \r\n",
      "                               balance_dataset,\r\n",
      "                               prefix,\r\n",
      "                               feature_group_name):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(file))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(balance_dataset))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(prefix)) \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group_name))    \r\n",
      "\r\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\r\n",
      "    feature_group = create_or_load_feature_group(prefix, feature_group_name)\r\n",
      "    \r\n",
      "    filename_without_extension = Path(Path(file).stem).stem\r\n",
      "\r\n",
      "    df = pd.read_csv(file, \r\n",
      "                     delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                     quoting=csv.QUOTE_NONE,\r\n",
      "                     compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    df.isna().values.any()\r\n",
      "    df = df.dropna()\r\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:  \r\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\r\n",
      "        five_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        four_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        three_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        two_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        one_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        minority_count = \u001b[36mmin\u001b[39;49;00m(five_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             four_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             three_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             two_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             one_star_df.shape[\u001b[34m0\u001b[39;49;00m]) \r\n",
      "\r\n",
      "        five_star_df = resample(five_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        four_star_df = resample(four_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        three_star_df = resample(three_star_df,\r\n",
      "                                 replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                 n_samples = minority_count,\r\n",
      "                                 random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        two_star_df = resample(two_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        one_star_df = resample(one_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\r\n",
      "\r\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_balanced.shape))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\r\n",
      "\r\n",
      "        df = df_balanced\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.train_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.validation_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.test_split_percentage))    \r\n",
      "    \r\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(holdout_percentage))\r\n",
      "    df_train, df_holdout = train_test_split(df, \r\n",
      "                                            test_size=holdout_percentage, \r\n",
      "                                            stratify=df[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_holdout_percentage))\r\n",
      "    df_validation, df_test = train_test_split(df_holdout, \r\n",
      "                                              test_size=test_holdout_percentage,\r\n",
      "                                              stratify=df_holdout[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_train.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_validation.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_test.shape))\r\n",
      "\r\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\r\n",
      "\r\n",
      "    train_inputs = df_train.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\r\n",
      "                                    label = x[LABEL_COLUMN],\r\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\r\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\r\n",
      "                                    date = timestamp\r\n",
      "                            ),\r\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    validation_inputs = df_validation.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\r\n",
      "                                    label = x[LABEL_COLUMN],\r\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\r\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\r\n",
      "                                    date = timestamp\r\n",
      "                            ),\r\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    test_inputs = df_test.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\r\n",
      "                                    label = x[LABEL_COLUMN],\r\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\r\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\r\n",
      "                                    date = timestamp\r\n",
      "                            ),\r\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "\r\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\r\n",
      "    train_records = transform_inputs_to_tfrecord(train_inputs, \r\n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension), \r\n",
      "                                                         max_seq_length)\r\n",
      "\r\n",
      "    validation_records = transform_inputs_to_tfrecord(validation_inputs, \r\n",
      "                                                              \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension), \r\n",
      "                                                              max_seq_length)\r\n",
      "\r\n",
      "    test_records = transform_inputs_to_tfrecord(test_inputs, \r\n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension), \r\n",
      "                                                        max_seq_length)    \r\n",
      "                \r\n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\r\n",
      "    df_train_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    df_train_records.head()   \r\n",
      "    \r\n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\r\n",
      "    df_validation_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \r\n",
      "    df_validation_records.head()   \r\n",
      "\r\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\r\n",
      "    df_test_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \r\n",
      "    df_test_records.head()   \r\n",
      "    \r\n",
      "    \u001b[37m# Add record to feature store    \u001b[39;49;00m\r\n",
      "    df_fs_train_records = cast_object_to_string(df_train_records)\r\n",
      "    df_fs_validation_records = cast_object_to_string(df_validation_records)\r\n",
      "    df_fs_test_records = cast_object_to_string(df_test_records)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mIngesting Features...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    feature_group.ingest(\r\n",
      "        data_frame=df_fs_train_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )        \r\n",
      "    feature_group.ingest(\r\n",
      "        data_frame=df_fs_validation_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )        \r\n",
      "    feature_group.ingest(\r\n",
      "        data_frame=df_fs_test_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )            \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature ingest completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\r\n",
      "    \r\n",
      "    feature_group = create_or_load_feature_group(prefix=args.feature_store_offline_prefix, \r\n",
      "                                                         feature_group_name=args.feature_group_name)\r\n",
      "\r\n",
      "    feature_group.describe()\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(feature_group.as_hive_ddl())\r\n",
      "    \r\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    \r\n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \r\n",
      "                                                  max_seq_length=args.max_seq_length,\r\n",
      "                                                  balance_dataset=args.balance_dataset,\r\n",
      "                                                  prefix=args.feature_store_offline_prefix,\r\n",
      "                                                  feature_group_name=args.feature_group_name)\r\n",
      "\r\n",
      "    input_files = glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\r\n",
      "\r\n",
      "    num_cpus = multiprocessing.cpu_count()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_cpus))\r\n",
      "\r\n",
      "    p = multiprocessing.Pool(num_cpus)\r\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\r\n",
      "    dirs_output = os.listdir(args.output_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\r\n",
      "    dirs_output = os.listdir(train_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\r\n",
      "    dirs_output = os.listdir(validation_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))\r\n",
      "    dirs_output = os.listdir(test_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "        \r\n",
      "    offline_store_contents = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    \u001b[34mwhile\u001b[39;49;00m (offline_store_contents \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m):\r\n",
      "        objects_in_bucket = s3.list_objects(Bucket=bucket,\r\n",
      "                                            Prefix=args.feature_store_offline_prefix)\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m objects_in_bucket \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) > \u001b[34m1\u001b[39;49;00m):\r\n",
      "            offline_store_contents = objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for data in offline store...\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            sleep(\u001b[34m60\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mData available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\r\n",
      "\r\n",
      "    process(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of an `SKLearnProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "We also specify the `framework_version` we will use throughout.\n",
    "\n",
    "Note the `processing_instance_type` and `processing_instance_count` parameters that used by the processor instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(framework_version='0.23-1',\n",
    "                             role=role,\n",
    "                             instance_type=processing_instance_type,\n",
    "                             instance_count=processing_instance_count,\n",
    "                             env={'AWS_DEFAULT_REGION': region},                             \n",
    "                             max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33manaconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.3.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker==2.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_group\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m FeatureGroup\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_definition\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\r\n",
      "    FeatureDefinition,\r\n",
      "    FeatureTypeEnum,\r\n",
      ")\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m############################\u001b[39;49;00m\r\n",
      "region = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRegion: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(region))\r\n",
      "\u001b[37m############################\u001b[39;49;00m\r\n",
      "\r\n",
      "sm = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\r\n",
      "\r\n",
      "featurestore_runtime = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-featurestore-runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\r\n",
      "\r\n",
      "s3 = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\r\n",
      "\r\n",
      "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name=region), sagemaker_client=sm, sagemaker_featurestore_runtime_client=featurestore_runtime)\r\n",
      "\r\n",
      "role = sagemaker.get_execution_role()\r\n",
      "bucket = sagemaker_session.default_bucket()\r\n",
      "\r\n",
      "\u001b[37m############################\u001b[39;49;00m\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "REVIEW_BODY_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "REVIEW_ID_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\r\n",
      "\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "    \r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\r\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\r\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m'\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\r\n",
      "            \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mwait_for_feature_group_creation_complete\u001b[39;49;00m(feature_group):\r\n",
      "    status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwhile\u001b[39;49;00m status == \u001b[33m\"\u001b[39;49;00m\u001b[33mCreating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for Feature Group Creation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        time.sleep(\u001b[34m5\u001b[39;49;00m)\r\n",
      "        status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m status != \u001b[33m\"\u001b[39;49;00m\u001b[33mCreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFailed to create feature group \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroup \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m successfully created.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "            \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_or_load_feature_group\u001b[39;49;00m(prefix, feature_group_name):\r\n",
      "\r\n",
      "    \u001b[37m# Feature Definitions for our records\u001b[39;49;00m\r\n",
      "    feature_definitions= [\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\r\n",
      "\u001b[37m#        FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\r\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING)            \r\n",
      "    ]\r\n",
      "    \r\n",
      "    feature_group = FeatureGroup(\r\n",
      "        name=feature_group_name,\r\n",
      "        feature_definitions=feature_definitions,\r\n",
      "        sagemaker_session=sagemaker_session)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group))\r\n",
      "    \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:                \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for existing Feature Group to become available if it is being created by another instance in our cluster...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        wait_for_feature_group_creation_complete(feature_group)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "        \u001b[34mpass\u001b[39;49;00m\r\n",
      "        \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        record_identifier_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        event_time_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        feature_group.create(\r\n",
      "            s3_uri=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{prefix}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            record_identifier_name=record_identifier_feature_name,\r\n",
      "            event_time_feature_name=event_time_feature_name,\r\n",
      "            role_arn=role,\r\n",
      "            enable_online_store=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "        )\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group. Completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for new Feature Group to become available...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        wait_for_feature_group_creation_complete(feature_group)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)        \r\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "        \u001b[34mpass\u001b[39;49;00m\r\n",
      "        \r\n",
      "    feature_group.describe()        \r\n",
      "        \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m feature_group\r\n",
      "\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id,\r\n",
      "               review_id,\r\n",
      "               date,\r\n",
      "               label):\r\n",
      "\u001b[37m#               review_body):\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\r\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\r\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "\u001b[37m#        self.review_body = review_body\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.review_id = review_id\r\n",
      "    \u001b[36mself\u001b[39;49;00m.date = date\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    tokens = tokenizer.tokenize(the_input.text)    \r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(the_input.text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=max_seq_length,\r\n",
      "\u001b[37m#                                               truncation=True\u001b[39;49;00m\r\n",
      "                                              )\r\n",
      "\r\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\r\n",
      "\r\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[the_input.label]\r\n",
      "\r\n",
      "    features = InputFeatures(\r\n",
      "        input_ids=input_ids,\r\n",
      "        input_mask=input_mask,\r\n",
      "        segment_ids=segment_ids,\r\n",
      "        label_id=label_id,\r\n",
      "        review_id=the_input.review_id,\r\n",
      "        date=the_input.date,\r\n",
      "        label=the_input.label)\r\n",
      "\u001b[37m#        review_body=the_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#     print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**review_id**\\n{}\\n'.format(features.review_id))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**date**\\n{}\\n'.format(features.date))\u001b[39;49;00m\r\n",
      "\u001b[37m#     print('**label**\\n{}\\n'.format(features.label))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**review_body**\\n{}\\n'.format(features.review_body))\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs,\r\n",
      "                                 output_file,\r\n",
      "                                 max_seq_length):\r\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    records = []\r\n",
      "\r\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\r\n",
      "    \r\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\r\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\r\n",
      "\r\n",
      "        features = convert_input(the_input, max_seq_length)\r\n",
      "\r\n",
      "        all_features = collections.OrderedDict()\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\r\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\r\n",
      "\r\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\r\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\r\n",
      "\r\n",
      "        records.append({\u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_ids,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_mask,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.segment_ids,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label_id,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.review_id,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.date,\r\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label,\r\n",
      "\u001b[37m#                        'review_body': features.review_body\u001b[39;49;00m\r\n",
      "                       })\r\n",
      "\r\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\r\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\r\n",
      "        \u001b[37m#####################################            \u001b[39;49;00m\r\n",
      "        \u001b[37m# break\u001b[39;49;00m\r\n",
      "        \r\n",
      "    tf_record_writer.close()\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m records\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "    )  \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "    ) \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "    ) \r\n",
      "        \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, \r\n",
      "                               max_seq_length, \r\n",
      "                               balance_dataset,\r\n",
      "                               prefix,\r\n",
      "                               feature_group_name):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(file))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(balance_dataset))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(prefix)) \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group_name))    \r\n",
      "\r\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\r\n",
      "    feature_group = create_or_load_feature_group(prefix, feature_group_name)\r\n",
      "    \r\n",
      "    filename_without_extension = Path(Path(file).stem).stem\r\n",
      "\r\n",
      "    df = pd.read_csv(file, \r\n",
      "                     delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                     quoting=csv.QUOTE_NONE,\r\n",
      "                     compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    df.isna().values.any()\r\n",
      "    df = df.dropna()\r\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:  \r\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\r\n",
      "        five_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        four_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        three_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        two_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        one_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        minority_count = \u001b[36mmin\u001b[39;49;00m(five_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             four_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             three_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             two_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             one_star_df.shape[\u001b[34m0\u001b[39;49;00m]) \r\n",
      "\r\n",
      "        five_star_df = resample(five_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        four_star_df = resample(four_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        three_star_df = resample(three_star_df,\r\n",
      "                                 replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                 n_samples = minority_count,\r\n",
      "                                 random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        two_star_df = resample(two_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        one_star_df = resample(one_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\r\n",
      "\r\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_balanced.shape))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\r\n",
      "\r\n",
      "        df = df_balanced\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.train_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.validation_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.test_split_percentage))    \r\n",
      "    \r\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(holdout_percentage))\r\n",
      "    df_train, df_holdout = train_test_split(df, \r\n",
      "                                            test_size=holdout_percentage, \r\n",
      "                                            stratify=df[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_holdout_percentage))\r\n",
      "    df_validation, df_test = train_test_split(df_holdout, \r\n",
      "                                              test_size=test_holdout_percentage,\r\n",
      "                                              stratify=df_holdout[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_train.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_validation.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_test.shape))\r\n",
      "\r\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\r\n",
      "\r\n",
      "    train_inputs = df_train.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\r\n",
      "                                    label = x[LABEL_COLUMN],\r\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\r\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\r\n",
      "                                    date = timestamp\r\n",
      "                            ),\r\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    validation_inputs = df_validation.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\r\n",
      "                                    label = x[LABEL_COLUMN],\r\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\r\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\r\n",
      "                                    date = timestamp\r\n",
      "                            ),\r\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    test_inputs = df_test.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\r\n",
      "                                    label = x[LABEL_COLUMN],\r\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\r\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\r\n",
      "                                    date = timestamp\r\n",
      "                            ),\r\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "\r\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\r\n",
      "    train_records = transform_inputs_to_tfrecord(train_inputs, \r\n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension), \r\n",
      "                                                         max_seq_length)\r\n",
      "\r\n",
      "    validation_records = transform_inputs_to_tfrecord(validation_inputs, \r\n",
      "                                                              \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension), \r\n",
      "                                                              max_seq_length)\r\n",
      "\r\n",
      "    test_records = transform_inputs_to_tfrecord(test_inputs, \r\n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension), \r\n",
      "                                                        max_seq_length)    \r\n",
      "                \r\n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\r\n",
      "    df_train_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    df_train_records.head()   \r\n",
      "    \r\n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\r\n",
      "    df_validation_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \r\n",
      "    df_validation_records.head()   \r\n",
      "\r\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\r\n",
      "    df_test_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \r\n",
      "    df_test_records.head()   \r\n",
      "    \r\n",
      "    \u001b[37m# Add record to feature store    \u001b[39;49;00m\r\n",
      "    df_fs_train_records = cast_object_to_string(df_train_records)\r\n",
      "    df_fs_validation_records = cast_object_to_string(df_validation_records)\r\n",
      "    df_fs_test_records = cast_object_to_string(df_test_records)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mIngesting Features...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    feature_group.ingest(\r\n",
      "        data_frame=df_fs_train_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )        \r\n",
      "    feature_group.ingest(\r\n",
      "        data_frame=df_fs_validation_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )        \r\n",
      "    feature_group.ingest(\r\n",
      "        data_frame=df_fs_test_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )            \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature ingest completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\r\n",
      "    \r\n",
      "    feature_group = create_or_load_feature_group(prefix=args.feature_store_offline_prefix, \r\n",
      "                                                         feature_group_name=args.feature_group_name)\r\n",
      "\r\n",
      "    feature_group.describe()\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(feature_group.as_hive_ddl())\r\n",
      "    \r\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    \r\n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \r\n",
      "                                                  max_seq_length=args.max_seq_length,\r\n",
      "                                                  balance_dataset=args.balance_dataset,\r\n",
      "                                                  prefix=args.feature_store_offline_prefix,\r\n",
      "                                                  feature_group_name=args.feature_group_name)\r\n",
      "\r\n",
      "    input_files = glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\r\n",
      "\r\n",
      "    num_cpus = multiprocessing.cpu_count()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_cpus))\r\n",
      "\r\n",
      "    p = multiprocessing.Pool(num_cpus)\r\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\r\n",
      "    dirs_output = os.listdir(args.output_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\r\n",
      "    dirs_output = os.listdir(train_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\r\n",
      "    dirs_output = os.listdir(validation_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))\r\n",
      "    dirs_output = os.listdir(test_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "        \r\n",
      "    offline_store_contents = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    \u001b[34mwhile\u001b[39;49;00m (offline_store_contents \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m):\r\n",
      "        objects_in_bucket = s3.list_objects(Bucket=bucket,\r\n",
      "                                            Prefix=args.feature_store_offline_prefix)\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m objects_in_bucket \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) > \u001b[34m1\u001b[39;49;00m):\r\n",
      "            offline_store_contents = objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for data in offline store...\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            sleep(\u001b[34m60\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mData available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\r\n",
      "\r\n",
      "    process(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='Processing', step_type=<StepTypeEnum.PROCESSING: 'Processing'>)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name='raw-input-data',\n",
    "            source=raw_input_data_s3_uri,\n",
    "            destination='/opt/ml/processing/input/data/',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "]\n",
    "\n",
    "processing_outputs=[\n",
    "        ProcessingOutput(output_name='bert-train',\n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/bert/train',\n",
    "                        ),\n",
    "        ProcessingOutput(output_name='bert-validation',\n",
    "                         s3_upload_mode='EndOfJob',                         \n",
    "                         source='/opt/ml/processing/output/bert/validation',\n",
    "                        ),\n",
    "        ProcessingOutput(output_name='bert-test',\n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/bert/test',\n",
    "                        ),\n",
    "]        \n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Processing', \n",
    "    code='preprocess-scikit-text-to-bert-feature-store.py',\n",
    "    processor=processor,\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=['--train-split-percentage', str(train_split_percentage.default_value),                   \n",
    "                   '--validation-split-percentage', str(validation_split_percentage.default_value),\n",
    "                   '--test-split-percentage', str(test_split_percentage.default_value),\n",
    "                   '--max-seq-length', str(max_seq_length.default_value),\n",
    "                   '--balance-dataset', str(balance_dataset.default_value),\n",
    "                   '--feature-store-offline-prefix', str(feature_store_offline_prefix.default_value),\n",
    "                   '--feature-group-name', str(feature_group_name.default_value)\n",
    "                  ]\n",
    ")        \n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Processing Step for Feature Engineering](img/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "Note the `input_data` parameters passed into `ProcessingStep` as the input data of the step itself. This input data will be used by the processor instance when it is run.\n",
    "\n",
    "Also, take note the `\"train_data\"` and `\"test_data\"` named channels specified in the output configuration for the processing job. Such step `Properties` can be used in subsequent steps and will resolve to their runtime values at execution. In particular, we'll call out this usage when we define our training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.9.3'])\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertModel\r\n",
      "\u001b[37m#from transformers import TFBertForSequenceClassification\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcallbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\r\n",
      "\u001b[37m#from tensorflow.keras.mixed_precision import experimental as mixed_precision\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\r\n",
      "    x = {\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder,\r\n",
      "                                     batch_size,\r\n",
      "                                     epochs,\r\n",
      "                                     steps_per_epoch,\r\n",
      "                                     max_seq_length):\r\n",
      "\r\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\r\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format=\u001b[33m'\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using input_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * \u001b[34m100\u001b[39;49;00m)\r\n",
      "\u001b[37m#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49;00m\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\r\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\r\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\r\n",
      "        \u001b[37m# TODO:  wip/bert/bert_attention_head_view/train.py\u001b[39;49;00m\r\n",
      "        \u001b[37m# Convert input_ids into input_tokens with DistilBert vocabulary \u001b[39;49;00m\r\n",
      "        \u001b[37m#  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\u001b[39;49;00m\r\n",
      "        \u001b[37m#    hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\r\n",
      "    \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=batch_size,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "\u001b[37m#    dataset.cache()\u001b[39;49;00m\r\n",
      "\r\n",
      "    dataset = dataset.shuffle(buffer_size=\u001b[34m1000\u001b[39;49;00m,\r\n",
      "                              reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    row_count = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**************** \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****************\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\r\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m dataset.as_numpy_iterator():\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(row)\r\n",
      "        \u001b[34mif\u001b[39;49;00m row_count == \u001b[34m5\u001b[39;49;00m:\r\n",
      "            \u001b[34mbreak\u001b[39;49;00m\r\n",
      "        row_count = row_count + \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_checkpoint_model\u001b[39;49;00m(checkpoint_path):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "    \r\n",
      "    glob_pattern = os.path.join(checkpoint_path, \u001b[33m'\u001b[39;49;00m\u001b[33m*.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mglob pattern \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(glob_pattern))\r\n",
      "\r\n",
      "    list_of_checkpoint_files = glob.glob(glob_pattern)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mList of checkpoint files \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(list_of_checkpoint_files))\r\n",
      "    \r\n",
      "    latest_checkpoint_file = \u001b[36mmax\u001b[39;49;00m(list_of_checkpoint_files)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLatest checkpoint file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(latest_checkpoint_file))\r\n",
      "\r\n",
      "    initial_epoch_number_str = latest_checkpoint_file.rsplit(\u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m].split(\u001b[33m'\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "    initial_epoch_number = \u001b[36mint\u001b[39;49;00m(initial_epoch_number_str)\r\n",
      "\r\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\r\n",
      "                                               latest_checkpoint_file,\r\n",
      "                                               config=config)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mloaded_model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(loaded_model))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minitial_epoch_number \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(initial_epoch_number))\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaded_model, initial_epoch_number\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \r\n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_base_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m64\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m2\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.00003\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.00000001\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_sagemaker_debugger\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)        \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_checkpointing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# This is unused\u001b[39;49;00m\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[37m# This points to the S3 location - this should not be used by our code\u001b[39;49;00m\r\n",
      "    \u001b[37m# We should use /opt/ml/model/ instead\u001b[39;49;00m\r\n",
      "    \u001b[37m# parser.add_argument('--model_dir', \u001b[39;49;00m\r\n",
      "    \u001b[37m#                     type=str, \u001b[39;49;00m\r\n",
      "    \u001b[37m#                     default=os.environ['SM_MODEL_DIR'])\u001b[39;49;00m\r\n",
      "     \r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    env_var = os.environ \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(env_var[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    sm_training_env_json = json.loads(env_var[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    is_master = sm_training_env_json[\u001b[33m'\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mis_master \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(is_master))\r\n",
      "    \r\n",
      "    train_data = args.train_data\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\r\n",
      "    validation_data = args.validation_data\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\r\n",
      "    test_data = args.test_data\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))    \r\n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    output_dir = args.output_dir\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \r\n",
      "    hosts = args.hosts\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \r\n",
      "    current_host = args.current_host\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \r\n",
      "    num_gpus = args.num_gpus\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\r\n",
      "    job_name = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSAGEMAKER_JOB_NAME\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mjob_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(job_name))    \r\n",
      "    use_xla = args.use_xla\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_xla \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_xla))    \r\n",
      "    use_amp = args.use_amp\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))    \r\n",
      "    max_seq_length = args.max_seq_length\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))    \r\n",
      "    train_batch_size = args.train_batch_size\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_batch_size))    \r\n",
      "    validation_batch_size = args.validation_batch_size\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_batch_size))    \r\n",
      "    test_batch_size = args.test_batch_size\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_batch_size))    \r\n",
      "    epochs = args.epochs\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))    \r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))    \r\n",
      "    epsilon = args.epsilon\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepsilon \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epsilon))    \r\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_steps_per_epoch))    \r\n",
      "    validation_steps = args.validation_steps\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_steps))    \r\n",
      "    test_steps = args.test_steps\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_steps))    \r\n",
      "    freeze_bert_layer = args.freeze_bert_layer\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfreeze_bert_layer \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(freeze_bert_layer))    \r\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))    \r\n",
      "    run_validation = args.run_validation\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_validation \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_validation))    \r\n",
      "    run_test = args.run_test\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_test \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_test))    \r\n",
      "    run_sample_predictions = args.run_sample_predictions\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_sample_predictions \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_sample_predictions))\r\n",
      "    enable_tensorboard = args.enable_tensorboard\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_tensorboard))       \r\n",
      "    enable_checkpointing = args.enable_checkpointing\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_checkpointing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_checkpointing))    \r\n",
      "\r\n",
      "    checkpoint_base_path = args.checkpoint_base_path\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcheckpoint_base_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_base_path))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_master:\r\n",
      "        checkpoint_path = checkpoint_base_path\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        checkpoint_path = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcheckpoint_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_path))\r\n",
      "    \r\n",
      "    \u001b[37m# Determine if PipeMode is enabled \u001b[39;49;00m\r\n",
      "    pipe_mode_str = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    pipe_mode = (pipe_mode_str.find(\u001b[33m'\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing pipe_mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(pipe_mode))\r\n",
      " \r\n",
      "    \u001b[37m# Model Output \u001b[39;49;00m\r\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\r\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Tensorboard Logs \u001b[39;49;00m\r\n",
      "    tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Commented out due to incompatibility with transformers library (possibly)\u001b[39;49;00m\r\n",
      "    \u001b[37m# Set the global precision mixed_precision policy to \"mixed_float16\"    \u001b[39;49;00m\r\n",
      "\u001b[37m#    mixed_precision_policy = 'mixed_float16'\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('Mixed precision policy {}'.format(mixed_precision_policy))\u001b[39;49;00m\r\n",
      "\u001b[37m#    policy = mixed_precision.Policy(mixed_precision_policy)\u001b[39;49;00m\r\n",
      "\u001b[37m#    mixed_precision.set_policy(policy)    \u001b[39;49;00m\r\n",
      "    \r\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\r\n",
      "    \u001b[37m# Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\u001b[39;49;00m\r\n",
      "    \u001b[37m#distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\r\n",
      "        tf.config.optimizer.set_jit(use_xla)\r\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\r\n",
      "\r\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data_filenames))\r\n",
      "        train_dataset = file_based_input_dataset_builder(\r\n",
      "            channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            input_filenames=train_data_filenames,\r\n",
      "            pipe_mode=pipe_mode,\r\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "            drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "            batch_size=train_batch_size,\r\n",
      "            epochs=epochs,\r\n",
      "            steps_per_epoch=train_steps_per_epoch,\r\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "        tokenizer = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        config = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        model = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        transformer_model = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# This is required when launching many instances at once...  the urllib request seems to get denied periodically\u001b[39;49;00m\r\n",
      "        successful_download = \u001b[34mFalse\u001b[39;49;00m\r\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m (retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download):\r\n",
      "            \u001b[34mtry\u001b[39;49;00m:\r\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\r\n",
      "                                                          id2label={\r\n",
      "                                                            \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\r\n",
      "                                                          },\r\n",
      "                                                          label2id={\r\n",
      "                                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "                                                            \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\r\n",
      "                                                          })\r\n",
      "\r\n",
      "                transformer_model = TFDistilBertModel.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                                                                      config=config)\r\n",
      "\r\n",
      "                input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \r\n",
      "\r\n",
      "                embedding_layer = transformer_model.distilbert(input_ids, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "                X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\u001b[34m50\u001b[39;49;00m, return_sequences=\u001b[34mTrue\u001b[39;49;00m, dropout=\u001b[34m0.1\u001b[39;49;00m, recurrent_dropout=\u001b[34m0.1\u001b[39;49;00m))(embedding_layer)\r\n",
      "                X = tf.keras.layers.GlobalMaxPool1D()(X)\r\n",
      "                X = tf.keras.layers.Dense(\u001b[34m50\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\r\n",
      "                X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\r\n",
      "                X = tf.keras.layers.Dense(\u001b[36mlen\u001b[39;49;00m(CLASSES), activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\r\n",
      "\r\n",
      "                model = tf.keras.Model(inputs=[input_ids, input_mask], outputs = X)\r\n",
      "\r\n",
      "                \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\r\n",
      "                    layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\r\n",
      "\r\n",
      "                successful_download = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSucessfully downloaded after \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m retries.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries))\r\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\r\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRetry #\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.  Sleeping for \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m seconds\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries, random_sleep))\r\n",
      "                time.sleep(random_sleep)\r\n",
      "\r\n",
      "        callbacks = []\r\n",
      "\r\n",
      "        initial_epoch_number = \u001b[34m0\u001b[39;49;00m \r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m enable_checkpointing:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Checkpoint enabled *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \r\n",
      "            os.makedirs(checkpoint_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)        \r\n",
      "            \u001b[34mif\u001b[39;49;00m os.listdir(checkpoint_path):\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Found checkpoint *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(checkpoint_path)\r\n",
      "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using checkpoint model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))\r\n",
      "                \r\n",
      "            checkpoint_callback = ModelCheckpoint(\r\n",
      "                    filepath=os.path.join(checkpoint_path, \u001b[33m'\u001b[39;49;00m\u001b[33mtf_model_\u001b[39;49;00m\u001b[33m{epoch:05d}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "                    save_weights_only=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                    verbose=\u001b[34m1\u001b[39;49;00m,\r\n",
      "                    monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mval_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** CHECKPOINT CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_callback))\r\n",
      "            callbacks.append(checkpoint_callback)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m** use_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))        \r\n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\r\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\r\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m'\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))\r\n",
      "        \u001b[34mif\u001b[39;49;00m enable_sagemaker_debugger:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGING ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdebug\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msmd\u001b[39;49;00m\r\n",
      "            \u001b[37m# This assumes that we specified debugger_hook_config\u001b[39;49;00m\r\n",
      "            debugger_callback = smd.KerasHook.create_from_json_file()\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGER CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(debugger_callback))            \r\n",
      "            callbacks.append(debugger_callback)\r\n",
      "            optimizer = debugger_callback.wrap_optimizer(optimizer)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m enable_tensorboard:            \r\n",
      "            tensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n",
      "                                                        log_dir=tensorboard_logs_path)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** TENSORBOARD CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorboard_callback))\r\n",
      "            callbacks.append(tensorboard_callback)\r\n",
      "  \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** OPTIMIZER \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\r\n",
      "        \r\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCompiled model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))          \r\n",
      "\u001b[37m#        model.layers[0].trainable = not freeze_bert_layer\u001b[39;49;00m\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(model.summary())\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\r\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data_filenames))\r\n",
      "            validation_dataset = file_based_input_dataset_builder(\r\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                input_filenames=validation_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                batch_size=validation_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=validation_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "            \r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training and Validation...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\r\n",
      "            train_and_validation_history = model.fit(train_dataset,\r\n",
      "                                                     shuffle=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                     epochs=epochs,\r\n",
      "                                                     initial_epoch=initial_epoch_number,\r\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                                     validation_data=validation_dataset,\r\n",
      "                                                     validation_steps=validation_steps,\r\n",
      "                                                     callbacks=callbacks)                                \r\n",
      "            \u001b[36mprint\u001b[39;49;00m(train_and_validation_history)\r\n",
      "        \u001b[34melse\u001b[39;49;00m: \u001b[37m# Not running validation\u001b[39;49;00m\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training (Without Validation)...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            train_history = model.fit(train_dataset,\r\n",
      "                                      shuffle=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                      epochs=epochs,\r\n",
      "                                      initial_epoch=initial_epoch_number,\r\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                      callbacks=callbacks)                \r\n",
      "            \u001b[36mprint\u001b[39;49;00m(train_history)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\r\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_filenames))\r\n",
      "            test_dataset = file_based_input_dataset_builder(\r\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                input_filenames=test_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                batch_size=test_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=test_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting test...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            test_history = model.evaluate(test_dataset,\r\n",
      "                                          steps=test_steps,\r\n",
      "                                          callbacks=callbacks)\r\n",
      "                                 \r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest history \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_history))\r\n",
      "            \r\n",
      "        \u001b[37m# Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\u001b[39;49;00m\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(transformer_fine_tuned_model_path))   \r\n",
      "        transformer_model.save_pretrained(transformer_fine_tuned_model_path)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel inputs after save_pretrained: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model.inputs))\r\n",
      "            \r\n",
      "        \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))   \r\n",
      "        model.save(tensorflow_saved_model_path,\r\n",
      "                   include_optimizer=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                   overwrite=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                   save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\r\n",
      "        \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\r\n",
      "        \u001b[37m#         This appears to be hard-coded and must be called code/\u001b[39;49;00m\r\n",
      "        inference_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference source files to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\r\n",
      "        os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)               \r\n",
      "        os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(glob(inference_path))        \r\n",
      "\u001b[37m#        os.system('cp requirements.txt {}/code'.format(inference_path))\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# Copy test data for the evaluation step\u001b[39;49;00m\r\n",
      "        os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp -R ./test_data/ \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(local_model_dir))\r\n",
      "        \r\n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\r\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text):\r\n",
      "            encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                                       pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                       max_length=max_seq_length,\r\n",
      "                                                       truncation=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                       return_tensors=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\r\n",
      "            input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "            \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\r\n",
      "            input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "            outputs = model.predict(x=(input_ids, input_mask))\r\n",
      "\r\n",
      "            scores = np.exp(outputs) / np.exp(outputs).sum(-\u001b[34m1\u001b[39;49;00m, keepdims=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "            prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m scores]\r\n",
      "\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "            \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "        df_test_reviews = pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33m./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                                        delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                                        quoting=csv.QUOTE_NONE,\r\n",
      "                                        compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\r\n",
      "\r\n",
      "        df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\r\n",
      "        df_test_reviews.shape\r\n",
      "        df_test_reviews.head()\r\n",
      "        \r\n",
      "        y_test = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].map(predict)\r\n",
      "        y_test\r\n",
      "        \r\n",
      "        y_actual = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        y_actual\r\n",
      "\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\r\n",
      "        \r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\r\n",
      "        accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, accuracy)\r\n",
      "        \r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap = plt.cm.Greens):\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(cm)\r\n",
      "            plt.imshow(cm, interpolation=\u001b[33m'\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cmap=cmap)\r\n",
      "            plt.title(title)\r\n",
      "            plt.colorbar()\r\n",
      "            tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\r\n",
      "            plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\r\n",
      "            plt.yticks(tick_marks, classes)\r\n",
      "\r\n",
      "            fmt = \u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "            thresh = cm.max() / \u001b[34m2.\u001b[39;49;00m\r\n",
      "            \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\r\n",
      "                plt.text(j, i, \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\r\n",
      "                horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "                color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "                plt.tight_layout()\r\n",
      "                plt.ylabel(\u001b[33m'\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                plt.xlabel(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                \r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\r\n",
      "\r\n",
      "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\r\n",
      "\r\n",
      "        plt.figure()\r\n",
      "        fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m,\u001b[34m5\u001b[39;49;00m))\r\n",
      "        plot_conf_mat(cm, \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      classes=[\u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \r\n",
      "                      title=\u001b[33m'\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# Save the confusion matrix        \u001b[39;49;00m\r\n",
      "        plt.show()\r\n",
      "        \r\n",
      "        \u001b[37m# Model Output         \u001b[39;49;00m\r\n",
      "        metrics_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        plt.savefig(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\r\n",
      "        \r\n",
      "        report_dict = {\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\r\n",
      "                },\r\n",
      "            },\r\n",
      "        }\r\n",
      "\r\n",
      "        evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "            f.write(json.dumps(report_dict))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.c5.9xlarge\"\n",
    ")\n",
    "\n",
    "train_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Hyper-Parameters\n",
    "Note that `max_seq_length` is re-used from the processing hyper-parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "learning_rate=0.00001\n",
    "epsilon=0.00000001\n",
    "train_batch_size=128\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "train_steps_per_epoch=50\n",
    "validation_steps=50\n",
    "test_steps=50\n",
    "#train_instance_count=1\n",
    "#train_instance_type='ml.c5.9xlarge'\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "freeze_bert_layer=False\n",
    "enable_sagemaker_debugger=False\n",
    "enable_checkpointing=False\n",
    "enable_tensorboard=False\n",
    "input_mode='File'\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True\n",
    "deploy_instance_count=1\n",
    "deploy_instance_type='ml.m5.4xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Training Step to Train a Model\n",
    "\n",
    "We configure an Estimator and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later.\n",
    "\n",
    "We also specify the model path where the models from training will be saved.\n",
    "\n",
    "Note the `training_instance_type` parameter passed may be also used and passed into other places in the pipeline. In this case, the `training_instance_type` is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.utils import name_from_base\n",
    "# training_job_name = name_from_base('bert-train')\n",
    "# print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 's3://{}/{}/model'.format(default_bucket,training_job_name)\n",
    "# print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       instance_type=train_instance_type,\n",
    "                       volume_size=train_volume_size,                       \n",
    "                       py_version='py37',\n",
    "                       framework_version='2.3.1',\n",
    "                       hyperparameters={'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'train_batch_size': train_batch_size,\n",
    "                                        'validation_batch_size': validation_batch_size,\n",
    "                                        'test_batch_size': test_batch_size,                                             \n",
    "                                        'train_steps_per_epoch': train_steps_per_epoch,\n",
    "                                        'validation_steps': validation_steps,\n",
    "                                        'test_steps': test_steps,\n",
    "                                        'use_xla': use_xla,\n",
    "                                        'use_amp': use_amp,                                             \n",
    "                                        'max_seq_length': max_seq_length,\n",
    "                                        'freeze_bert_layer': freeze_bert_layer,\n",
    "                                        'enable_sagemaker_debugger': enable_sagemaker_debugger,\n",
    "                                        'enable_checkpointing': enable_checkpointing,\n",
    "                                        'enable_tensorboard': enable_tensorboard,                                        \n",
    "                                        'run_validation': run_validation,\n",
    "                                        'run_test': run_test,\n",
    "                                        'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "#                       max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the estimator instance to construct a `TrainingStep` as well as the `Properties` of the prior `ProcessingStep` used as input in the `TrainingStep` inputs and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to an estimator's `fit` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3Uri` of the `\"train_data\"` output channel to the `TrainingStep`. We will also use the other `\"test_data\"` output channel for model evaluation in the pipeline. The `properties` attribute of a Workflow step match the object model of the corresponding response of a describe call. These properties can be referenced as placeholder values and are resolved, or filled in, at runtime. For example, the `ProcessingStep` `properties` attribute matches the object model of the [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Train',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'bert-train'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'bert-validation'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'test': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'bert-test'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )        \n",
    "    },\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Training Step to Train a Model](img/pipeline-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Step\n",
    "\n",
    "First, we develop an evaluation script that will be specified in a Processing step that will perform the model evaluation.\n",
    "\n",
    "The evaluation script `evaluation.py` takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model.\n",
    "\n",
    "After pipeline execution, we will examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script:\n",
    "\n",
    "* loads in the model\n",
    "* reads in the test data\n",
    "* issues a bunch o' predictions against the test data\n",
    "* builds a classification report, including accuracy and roc\n",
    "* saves the evaluation report to the evaluation directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance of a `ScriptProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "Note the `processing_instance_type` parameter passed into the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(framework_version='0.23-1',\n",
    "                                      role=role,\n",
    "                                      instance_type=processing_instance_type,\n",
    "                                      instance_count=processing_instance_count,\n",
    "                                      env={'AWS_DEFAULT_REGION': region},\n",
    "                                      max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33manaconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.3.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\r\n",
      "                                          id2label={\r\n",
      "                                            \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\r\n",
      "                                          },\r\n",
      "                                          label2id={\r\n",
      "                                            \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "                                            \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\r\n",
      "                                          })\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "    )  \r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "        \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minput_data: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minput_model: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model))          \r\n",
      "          \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of input model dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model))\r\n",
      "    input_files = os.listdir(args.input_model)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m input_files:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "    model_tar_path = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/model.tar.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model)                \r\n",
      "    model_tar = tarfile.open(model_tar_path)\r\n",
      "    model_tar.extractall(args.input_model)\r\n",
      "    model_tar.close()                                \r\n",
      "\r\n",
      "    model = keras.models.load_model(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/tensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_model))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(model)\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text):\r\n",
      "        encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                                   pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                   max_length=args.max_seq_length,\r\n",
      "                                                   truncation=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                   return_tensors=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\r\n",
      "        input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\r\n",
      "        input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "        outputs = model.predict(x=(input_ids, input_mask))\r\n",
      "\r\n",
      "        scores = np.exp(outputs) / np.exp(outputs).sum(-\u001b[34m1\u001b[39;49;00m, keepdims=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "        prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m scores]\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m###########################################################################################\u001b[39;49;00m\r\n",
      "    \u001b[37m# TODO:  Replace this with glob for all files and remove test_data/ from the model.tar.gz #\u001b[39;49;00m\r\n",
      "    \u001b[37m###########################################################################################    \u001b[39;49;00m\r\n",
      "\u001b[37m#    evaluation_data_path = '/opt/ml/processing/input/data/'\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of input data dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\r\n",
      "    input_files = os.listdir(args.input_data)\r\n",
      "\r\n",
      "    test_data_path = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing only \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m to evaluate.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_path))\r\n",
      "    df_test_reviews = pd.read_csv(test_data_path, \r\n",
      "                                  delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                                  quoting=csv.QUOTE_NONE,\r\n",
      "                                  compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\r\n",
      "\r\n",
      "    df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\r\n",
      "    df_test_reviews.shape\r\n",
      "    df_test_reviews.head()\r\n",
      "\r\n",
      "    y_test = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].map(predict)\r\n",
      "    y_test\r\n",
      "\r\n",
      "    y_actual = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    y_actual\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\r\n",
      "\r\n",
      "    accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, accuracy)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(cm)\r\n",
      "        plt.imshow(cm, interpolation=\u001b[33m'\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cmap=cmap)\r\n",
      "        plt.title(title)\r\n",
      "        plt.colorbar()\r\n",
      "        tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\r\n",
      "        plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\r\n",
      "        plt.yticks(tick_marks, classes)\r\n",
      "\r\n",
      "        fmt = \u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        thresh = cm.max() / \u001b[34m2.\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\r\n",
      "            plt.text(j, i, \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\r\n",
      "            horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "            plt.tight_layout()\r\n",
      "            plt.ylabel(\u001b[33m'\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            plt.xlabel(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\r\n",
      "\r\n",
      "    plt.figure()\r\n",
      "    fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m,\u001b[34m5\u001b[39;49;00m))\r\n",
      "    plot_conf_mat(cm, \r\n",
      "                  classes=CLASSES, \r\n",
      "                  title=\u001b[33m'\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  cmap=plt.cm.Greens)\r\n",
      "\r\n",
      "    \u001b[37m# Save the confusion matrix        \u001b[39;49;00m\r\n",
      "    plt.show()\r\n",
      "\r\n",
      "    \u001b[37m# Model Output         \u001b[39;49;00m\r\n",
      "    metrics_path = os.path.join(args.output_data, \u001b[33m'\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    plt.savefig(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\r\n",
      "\r\n",
      "    report_dict = {\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\r\n",
      "            },\r\n",
      "        },\r\n",
      "    }\r\n",
      "\r\n",
      "    evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        f.write(json.dumps(report_dict))\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of output dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\r\n",
      "    output_files = os.listdir(args.output_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m output_files:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of output/metrics dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\r\n",
      "    output_files = os.listdir(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m output_files:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\r\n",
      "\r\n",
      "    process(args)    \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize evaluate_model_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3ModelArtifacts` from the `TrainingStep`, `step_train` properties as well as the `S3Uri` of the `\"test_data\"` output channel of the first `ProcessingStep`, `step_process`.\n",
    "\n",
    "The `TrainingStep` and `ProcessingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) and  [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# NOTE:\n",
    "# property files cause deserialization failure on listing pipeline executions\n",
    "# therefore jsonget and robust conditions won't work\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='metrics',\n",
    "    path='evaluation.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_step = ProcessingStep(\n",
    "    name='Evaluation',\n",
    "    processor=evaluation_processor,\n",
    "    code='evaluate_model_metrics.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/input/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=raw_input_data_s3_uri,\n",
    "            #processing_step.properties.ProcessingInputConfig.Inputs['raw-input-data'].S3Input.S3Uri,\n",
    "            destination='/opt/ml/processing/input/data'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='metrics', \n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/metrics/'),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "                   '--max-seq-length', str(max_seq_length.default_value),\n",
    "                  ],\n",
    "    property_files=[evaluation_report],  # these cause deserialization issues\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Model Evaluation Step to Evaluate the Trained Model](img/pipeline-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(evaluation_step.properties.ProcessingOutputConfig.Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(training_step.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_config = processing_job_description['ProcessingOutputConfig']\n",
    "# for output in output_config['Outputs']:\n",
    "#     if output['OutputName'] == 'metrics':\n",
    "#         evaluation_metrics_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "# print(evaluation_metrics_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model_metrics.ModelMetrics object at 0x7f1e993042e8>\n"
     ]
    }
   ],
   "source": [
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Model Step\n",
    "\n",
    "We use the estimator instance that was used for the training step to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a Model Package. A Model Package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A Model Package Group is a collection of Model Packages. You can create a Model Package Group for a specific ML business problem, and you can keep adding versions/model packages into it. Typically, we expect customers to create a ModelPackageGroup for a SageMaker Workflow Pipeline so that they can keep adding versions/model packages to the group for every Workflow Pipeline run.\n",
    "\n",
    "The construction of `RegisterModel` is very similar to an estimator instance's `register` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3ModelArtifacts` from the `TrainingStep`, `step_train` properties. The `TrainingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) response object.\n",
    "\n",
    "Of note, we provided a specific model package group name which we will use in the Model Registry and CI/CD work later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Reviews-1609640615\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"BERT-Reviews-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)\n",
    "\n",
    "# # # NOTE: in the future, the model package group will be created automatically if it doesn't exist\n",
    "# sm.create_model_package_group(\n",
    "#     ModelPackageGroupName=model_package_group_name,\n",
    "#     ModelPackageGroupDescription=\"BERT-Reviews\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "# NOTE: model_approval_status is not available as arg in service dsl currently\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=estimator,\n",
    "    image_uri=inference_image_uri, # we have to specify, by default it's using training image\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.4xlarge\"],\n",
    "    transform_instances=[\"ml.c5.18xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "# Note:  this isn't being picked up, so we're commenting it out\n",
    "#    model_approval_status=model_approval_status  \n",
    "    model_metrics=model_metrics\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Condition Step to Check Accuracy and Conditionally Register Model\n",
    "\n",
    "Finally, we'd like to only register this model if the accuracy of the model, as determined by our evaluation step `step_eval`, exceeded some value. A `ConditionStep` allows for pipelines to support conditional execution in the pipeline DAG based on conditions of step properties. \n",
    "\n",
    "Below, we:\n",
    "\n",
    "* define a `ConditionGreaterThan` on the accuracy value found in the output of the evaluation step, `step_eval`.\n",
    "* use the condition in the list of conditions in a `ConditionStep`\n",
    "* pass the `RegisterModel` step collection into the `if_steps` of the `ConditionStep`\n",
    "* use the `FailStep` in the `else_steps` of the `ConditionStep` to fail the pipeline if the accuracy condition was not met\n",
    "\n",
    "NOTE: there are a few things that are planned to be implemented in the Workflow service that are currently unavailable to us:\n",
    "\n",
    "* JsonGet - a function to allow getting json files from S3 and using their values in conditions\n",
    "* FailStep - a step that terminates the pipeline in failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=evaluation_step,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=0 # accuracy percent\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[minimum_accuracy_condition],\n",
    "    if_steps=[register_step], # success, continue with model registration\n",
    "    else_steps=[], # fail, end the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Condition Step to Check Accuracy and Conditionally Register Model](img/pipeline-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline of Parameters, Steps, and Conditions\n",
    "\n",
    "Let's tie it all up into a workflow pipeline so we can execute it, and even schedule it.\n",
    "\n",
    "A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair so we tack on the timestamp to the name.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline need not be in the order of execution. The SageMaker Workflow service will resolve the _data dependency_ DAG as steps the execution complete.\n",
    "* Steps must be unique to either pipeline step list or a single condition step if/else list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Pipeline of Parameters, Steps, and Conditions](img/pipeline-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        max_seq_length,\n",
    "        balance_dataset,\n",
    "        train_split_percentage,\n",
    "        validation_split_percentage,\n",
    "        test_split_percentage,\n",
    "        feature_store_offline_prefix,\n",
    "        feature_group_name,        \n",
    "        train_instance_type,\n",
    "        train_instance_count,\n",
    "        model_approval_status,\n",
    "        deploy_instance_type,\n",
    "        deploy_instance_count        \n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step], # register_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the Json of the pipeline definition that meets the SageMaker Workflow Pipeline DSL specification.\n",
    "\n",
    "By examining the definition, we're also confirming that the pipeline was well-defined, and that the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [{'DefaultValue': 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/',\n",
      "                 'Name': 'InputData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'ProcessingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'ml.c5.2xlarge',\n",
      "                 'Name': 'ProcessingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 64, 'Name': 'MaxSeqLength', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'True',\n",
      "                 'Name': 'BalanceDataset',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.9,\n",
      "                 'Name': 'TrainSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'ValidationSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'TestSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'reviews-feature-store-1609640615',\n",
      "                 'Name': 'FeatureStoreOfflinePrefix',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'reviews-feature-group-1609640615',\n",
      "                 'Name': 'FeatureGroupName',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'ml.c5.9xlarge',\n",
      "                 'Name': 'TrainingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'TrainingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'PendingManualApproval',\n",
      "                 'Name': 'ModelApprovalStatus',\n",
      "                 'Type': 'String'}],\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--train-split-percentage',\n",
      "                                                                      '0.9',\n",
      "                                                                      '--validation-split-percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--test-split-percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--max-seq-length',\n",
      "                                                                      '64',\n",
      "                                                                      '--balance-dataset',\n",
      "                                                                      'True',\n",
      "                                                                      '--feature-store-offline-prefix',\n",
      "                                                                      'reviews-feature-store-1609640615',\n",
      "                                                                      '--feature-group-name',\n",
      "                                                                      'reviews-feature-group-1609640615'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'raw-input-data',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data/',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/'}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-40-305/input/code/preprocess-scikit-text-to-bert-feature-store.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-train',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/train',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-40-305/output/bert-train'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-validation',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/validation',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-40-305/output/bert-validation'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-test',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/test',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-40-305/output/bert-test'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      "            'Name': 'Processing',\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'AlgorithmSpecification': {'EnableSageMakerMetricsTimeSeries': True,\n",
      "                                                     'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                            'Regex': 'loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'train:accuracy',\n",
      "                                                                            'Regex': 'accuracy: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:loss',\n",
      "                                                                            'Regex': 'val_loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:accuracy',\n",
      "                                                                            'Regex': 'val_accuracy: '\n",
      "                                                                                     '([0-9\\\\.]+)'}],\n",
      "                                                     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37',\n",
      "                                                     'TrainingInputMode': 'File'},\n",
      "                          'DebugHookConfig': {'CollectionConfigurations': [],\n",
      "                                              'S3OutputPath': 's3://sagemaker-us-east-1-835319576252/'},\n",
      "                          'HyperParameters': {'enable_checkpointing': 'false',\n",
      "                                              'enable_sagemaker_debugger': 'false',\n",
      "                                              'enable_tensorboard': 'false',\n",
      "                                              'epochs': '1',\n",
      "                                              'epsilon': '1e-08',\n",
      "                                              'freeze_bert_layer': 'false',\n",
      "                                              'learning_rate': '1e-05',\n",
      "                                              'max_seq_length': '64',\n",
      "                                              'model_dir': '\"s3://sagemaker-us-east-1-835319576252/tensorflow-training-2021-01-03-02-23-40-535/model\"',\n",
      "                                              'run_sample_predictions': 'true',\n",
      "                                              'run_test': 'true',\n",
      "                                              'run_validation': 'true',\n",
      "                                              'sagemaker_container_log_level': '20',\n",
      "                                              'sagemaker_job_name': '\"tensorflow-training-2021-01-03-02-23-40-535\"',\n",
      "                                              'sagemaker_program': '\"tf_bert_reviews.py\"',\n",
      "                                              'sagemaker_region': '\"us-east-1\"',\n",
      "                                              'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-835319576252/tensorflow-training-2021-01-03-02-23-40-535/source/sourcedir.tar.gz\"',\n",
      "                                              'test_batch_size': '128',\n",
      "                                              'test_steps': '50',\n",
      "                                              'train_batch_size': '128',\n",
      "                                              'train_steps_per_epoch': '50',\n",
      "                                              'use_amp': 'true',\n",
      "                                              'use_xla': 'true',\n",
      "                                              'validation_batch_size': '128',\n",
      "                                              'validation_steps': '50'},\n",
      "                          'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-train'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'validation',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-validation'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'test',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-test'].S3Output.S3Uri\"}}}}],\n",
      "                          'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-835319576252/'},\n",
      "                          'ResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'},\n",
      "                                             'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
      "                                             'VolumeSizeInGB': 1024},\n",
      "                          'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 86400}},\n",
      "            'Name': 'Train',\n",
      "            'Type': 'Training'},\n",
      "           {'Arguments': {'AppSpecification': {'ContainerArguments': ['--max-seq-length',\n",
      "                                                                      '64'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/evaluate_model_metrics.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'input-1',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/model',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-2',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/'}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-41-641/input/code/evaluate_model_metrics.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'metrics',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/metrics/',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-39-966/output/metrics'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      "            'Name': 'Evaluation',\n",
      "            'PropertyFiles': [{'FilePath': 'evaluation.json',\n",
      "                               'OutputName': 'metrics',\n",
      "                               'PropertyFileName': 'EvaluationReport'}],\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'Conditions': [{'LeftValue': {'Std:JsonGet': {'Path': 'metrics.accuracy.value',\n",
      "                                                                        'PropertyFile': {'Get': 'Steps.Evaluation.PropertyFiles.EvaluationReport'}}},\n",
      "                                          'RightValue': 0,\n",
      "                                          'Type': 'GreaterThanOrEqualTo'}],\n",
      "                          'ElseSteps': [],\n",
      "                          'IfSteps': [{'Arguments': {'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu',\n",
      "                                                                                                'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}],\n",
      "                                                                                'SupportedContentTypes': ['application/json'],\n",
      "                                                                                'SupportedRealtimeInferenceInstanceTypes': ['ml.m5.4xlarge'],\n",
      "                                                                                'SupportedResponseMIMETypes': ['application/json'],\n",
      "                                                                                'SupportedTransformInstanceTypes': ['ml.c5.18xlarge']},\n",
      "                                                     'ModelApprovalStatus': 'PendingManualApproval',\n",
      "                                                     'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
      "                                                                                                      'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2021-01-03-02-23-39-966/output/metrics/evaluation.json'}}},\n",
      "                                                     'ModelPackageGroupName': 'BERT-Reviews-1609640615'},\n",
      "                                       'Name': 'RegisterModel',\n",
      "                                       'Type': 'RegisterModel'}]},\n",
      "            'Name': 'AccuracyCondition',\n",
      "            'Type': 'Condition'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Let's submit our pipeline definition to the workflow service. The role passed in will be used by the workflow service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-pipeline-16096406156145956\n"
     ]
    }
   ],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the `WARNING` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:835319576252:pipeline/bert-pipeline-16096406156145956\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.create(role_arn=role\n",
    "#                           experiment_name=experiment_name # Experiments not natively supported by pipeline\n",
    "                          )\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the pipeline, accepting all the default parameters.\n",
    "\n",
    "Values can also be passed into these pipeline parameters on starting of the pipeline, and will be covered later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:835319576252:pipeline/bert-pipeline-16096406156145956/execution/kdcvcnkx1smp\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start(\n",
    "# TODO:  ADD THE PARAMETERS - SAME AS 2nd RUN BELOW\n",
    "#         InputDataUrl=raw_input_data_s3_uri,\n",
    "#         ProcessingInstanceCount=1,\n",
    "#         ProcessingInstanceType='ml.c5.2xlarge',\n",
    "#         MaxSeqLength=64,\n",
    "#         BalanceDataset='True',\n",
    "#         TrainSplitPercentage=0.9,\n",
    "#         ValidationSplitPercentage=0.05,\n",
    "#         TestSplitPercentage=0.05,\n",
    "# #        FeatureStoreOfflinePrefix='', # TODO:  pass this in or leave empty for default?\n",
    "# #        FeatureGroupName='', # TODO:  pass this in or leave empty for default?\n",
    "#         TrainingInstanceType='ml.c5.9xlarge',\n",
    "#         TrainingInstanceCount=1,\n",
    "#         ModelApprovalStatus=\"Approved\", # TODO:  Figure out why this is not working\n",
    "#         DeployInstanceType='ml.m5.4xlarge',\n",
    "#         DeployInstanceCount=1    \n",
    ")\n",
    "\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Now we describe execution instance and list the steps in the execution to find out more about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreatedBy': {},\n",
      " 'CreationTime': datetime.datetime(2021, 1, 3, 2, 23, 43, 321000, tzinfo=tzlocal()),\n",
      " 'LastModifiedBy': {},\n",
      " 'LastModifiedTime': datetime.datetime(2021, 1, 3, 2, 23, 43, 321000, tzinfo=tzlocal()),\n",
      " 'PipelineArn': 'arn:aws:sagemaker:us-east-1:835319576252:pipeline/bert-pipeline-16096406156145956',\n",
      " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:835319576252:pipeline/bert-pipeline-16096406156145956/execution/kdcvcnkx1smp',\n",
      " 'PipelineExecutionDisplayName': 'execution-1609640623425',\n",
      " 'PipelineExecutionStatus': 'Executing',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '427',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sun, 03 Jan 2021 02:23:42 GMT',\n",
      "                                      'x-amzn-requestid': 'd0a9dadf-7a96-4105-ad01-5b9817d8a8ab'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'd0a9dadf-7a96-4105-ad01-5b9817d8a8ab',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "execution_run = execution.describe()\n",
    "pprint(execution_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Execution Run as Trial to Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution-1609640623425\n"
     ]
    }
   ],
   "source": [
    "execution_run_name = execution_run['PipelineExecutionDisplayName']\n",
    "print(execution_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'Processing',\n",
       "  'StartTime': datetime.datetime(2021, 1, 3, 2, 23, 44, 8000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Executing',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:835319576252:processing-job/pipelines-kdcvcnkx1smp-processing-xbtw8fwfaf'}}}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Giving the first step time to start up\n",
    "time.sleep(30)\n",
    "\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wait for the execution by invoking `wait()` on the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the Evalution\n",
    "\n",
    "Examine the resulting model evaluation after the pipeline completes. Download the resulting evaluation.json file from S3 and print the report.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_step.properties.ProcessingOutputConfig.Outputs['metrics'].S3Output.S3Uri)\n",
    "\n",
    "\n",
    "# arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "\n",
    "# .__dict__ #  .ModelArtifacts.S3ModelArtifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(evaluation_metrics_s3_uri)\n",
    "\n",
    "# TODO:  Change this to ProcessingOutput when we switch to PropertyFile per https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\"{}/evaluation.json\".format(\n",
    "    evaluation_metrics_s3_uri\n",
    "))\n",
    "\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# evaluation_report = PropertyFile(\n",
    "#     name=\"EvaluationReport\",\n",
    "#     output_name=\"evaluation\",\n",
    "#     path=\"evaluation.json\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Artifacts Generated By The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name=None\n",
    "training_job_name=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step['StepName'] == 'Processing':\n",
    "        processing_job_name=execution_step['Metadata']['ProcessingJob']['Arn'].split('/')[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step['StepName'] == 'Train':\n",
    "        training_job_name=execution_step['Metadata']['TrainingJob']['Arn'].split('/')[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Trial Compontents To Experiment Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tc = '{}-aws-processing-job'.format(processing_job_name)\n",
    "print(processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(\n",
    "    # -aws-processing-job is the default name assigned by ProcessingJob\n",
    "    TrialComponentName=processing_job_tc,\n",
    "    TrialName=trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_tc = '{}-aws-training-job'.format(training_job_name)\n",
    "print(training_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(\n",
    "    # -aws-training-job is the default name assigned by TrainingJob\n",
    "    TrialComponentName=training_job_tc,\n",
    "    TrialName=trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Additional Parameters within Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments import tracker\n",
    "\n",
    "processing_job_tracker = tracker.Tracker.load(trial_component_name=processing_job_tc)\n",
    "print(processing_job_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters({\n",
    "    \"balance_dataset\": str(balance_dataset), \n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the evalution\n",
    "\n",
    "We can examine the resulting model evaluation after the pipeline completes.\n",
    "\n",
    "We download the resulting `evaluation.json` file from S3 and print the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.s3.S3Downloader.read_file(\"{}/evaluation.json\".format(\n",
    "#     step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ Run another Pipeline Execution with Different Parameters ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# THIS IS RUNNING A 2nd PIPELINE\n",
    "###\n",
    "execution_parametrized = pipeline.start(\n",
    "    parameters=dict(\n",
    "#         InputData='',\n",
    "#         ProcessingInstanceCount=1,\n",
    "#         ProcessingInstanceType='',\n",
    "#         MaxSeqLength=64,\n",
    "#         TrainSplitPercentage=0.9,\n",
    "#         ValidationSplitPercentage=0.05,\n",
    "#         TestSplitPercentage=0.05,\n",
    "#         TrainingInstanceType='',\n",
    "#         TrainingInstanceCount=1,\n",
    "        ModelApprovalStatus='Approved'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "#pd.set_option(\"max_rows\", 100)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List All Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.analytics import ArtifactAnalytics\n",
    "# analytics = ArtifactAnalytics()\n",
    "# artifact_analytics_df = analytics.dataframe()\n",
    "# artifact_analytics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Registry and CI/CD\n",
    "\n",
    "\n",
    "The pipeline that was executed created a Model Package version within the specified Model Package Group. Of particular note, the registration of the model/creation of the Model Package was done so with approval status as `PendingManualApproval`.\n",
    "\n",
    "Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for execution_step in execution.list_steps():\n",
    "    if execution_step[\"StepName\"] == \"RegisterModel\":\n",
    "        model_package_arn = execution_step[\"Metadata\"][\"RegisterModel\"][\"Arn\"]\n",
    "        break\n",
    "print(model_package_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Model Package Approval Status\n",
    "\n",
    "As noted above, the model has been registered with `\"PendingManualApproval\"` status. As part of Yosemite, data scientists can register the model with approved/pending manual approval as part of the Tioga workflow. Here we are demonstrating how they can approve the generated model manually. In GA (Nov 2020), we will have UX in SageMaker Studio so that datascients can approve the model, which will inturn trigger the Ci/CD system. For now, here is a way to approve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")\n",
    "\n",
    "print(model_package_update_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model From Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'bert-model-{}'.format(timestamp)\n",
    "# print(\"Model name : {}\".format(model_name))\n",
    "# primary_container = {\n",
    "#     'ModelPackageName': model_package_arn,\n",
    "# }\n",
    "# create_model_response = sm.create_model(\n",
    "#     ModelName = model_name,\n",
    "#     ExecutionRoleArn = role,\n",
    "#     PrimaryContainer = primary_container\n",
    "# )\n",
    "# print(\"Model arn : {}\".format(create_model_response[\"ModelArn\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.model import Model\n",
    "\n",
    "# model = Model(\n",
    "#     image_uri=image_uri,\n",
    "#     model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#     sagemaker_session=sagemaker_session,\n",
    "#     role=role,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.inputs import CreateModelInput\n",
    "# from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "# inputs = CreateModelInput(\n",
    "#     instance_type=\"ml.m5.large\",\n",
    "#     accelerator_type=\"ml.eia1.medium\",\n",
    "# )\n",
    "# step_create_model = CreateModelStep(\n",
    "#     name=\"AbaloneCreateModel\",\n",
    "#     model=model,\n",
    "#     inputs=inputs,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'bert-model-epc-{}'.format(timestamp)\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m5.4xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'bert-model-ep-{}'.format(timestamp)\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(region, endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the Endpoint is Deployed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "waiter = sm.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(endpoint_name=endpoint_name,\n",
    "                                sagemaker_session=sess,\n",
    "                                model_name='saved_model',\n",
    "                                model_version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the `star_rating` with Ad Hoc `review_body` Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"This is great!\"]\n",
    "\n",
    "predicted_classes = predictor.predict(reviews)\n",
    "\n",
    "for predicted_class, review in zip(predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
