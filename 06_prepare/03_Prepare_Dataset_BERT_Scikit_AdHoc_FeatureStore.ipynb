{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Features into a SageMaker Feature Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "s3 = boto3.Session().client(service_name='s3', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas                             1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Raw Text to BERT Features using Hugging Face and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "REVIEW_BODY_COLUMN = 'review_body'\n",
    "REVIEW_ID_COLUMN = 'review_id'\n",
    "# DATE_COLUMN = 'date'\n",
    "\n",
    "LABEL_COLUMN = 'star_rating'\n",
    "LABEL_VALUES = [1, 2, 3, 4, 5]\n",
    "\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "    \n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               review_id,\n",
    "               date,\n",
    "               label,\n",
    "               review_body):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n",
    "        self.review_body = review_body\n",
    "\n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, review_id, date, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.review_id = review_id\n",
    "    self.date = date\n",
    "    self.label = label\n",
    "    \n",
    "\n",
    "def convert_input(the_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "\n",
    "    tokens = tokenizer.tokenize(the_input.text)\n",
    "    print('**tokens**\\n{}\\n'.format(tokens))\n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(the_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length,\n",
    "#                                               truncation=True\n",
    "                                              )\n",
    "\n",
    "    # The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    \n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "\n",
    "    # Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for each training row (`star_rating` 1 through 5)\n",
    "    label_id = label_map[the_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        review_id=the_input.review_id,\n",
    "        date=the_input.date,\n",
    "        label=the_input.label,\n",
    "        review_body=the_input.text)\n",
    "\n",
    "    print('**input_ids**\\n{}\\n'.format(features.input_ids))\n",
    "    print('**input_mask**\\n{}\\n'.format(features.input_mask))\n",
    "    print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\n",
    "    print('**label_id**\\n{}\\n'.format(features.label_id))\n",
    "    print('**review_id**\\n{}\\n'.format(features.review_id))\n",
    "    print('**date**\\n{}\\n'.format(features.date))\n",
    "    print('**label**\\n{}\\n'.format(features.label))\n",
    "    print('**review_body**\\n{}\\n'.format(features.review_body))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# We'll need to transform our data into a format that BERT understands.\n",
    "# - `text` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "# - `label` is the star_rating label (1, 2, 3, 4, 5) for our training input data\n",
    "def transform_inputs_to_tfrecord(inputs, output_file, max_seq_length):\n",
    "    records = []\n",
    "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, the_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print('Writing input {} of {}\\n'.format(input_idx, len(inputs)))\n",
    "\n",
    "        features = convert_input(the_input, max_seq_length)\n",
    "\n",
    "        all_features = collections.OrderedDict()\n",
    "        all_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
    "        all_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
    "        all_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
    "        all_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
    "\n",
    "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
    "        tf_record_writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        records.append({'tf_record': tf_record.SerializeToString(),\n",
    "                        'input_ids': features.input_ids,\n",
    "                        'input_mask': features.input_mask,\n",
    "                        'segment_ids': features.segment_ids,\n",
    "                        'label_id': features.label_id,\n",
    "                        'review_id': the_input.review_id,\n",
    "                        'date': the_input.date,\n",
    "                        'label': features.label,\n",
    "                        'review_body': features.review_body\n",
    "                       })\n",
    "        \n",
    "    tf_record_writer.close()\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three(3) feature vectors are created from each raw review (`review_body`) during the feature engineering phase to prepare for BERT processing:\n",
    "\n",
    "* **`input_ids`**:  The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\n",
    "    \n",
    "* **`input_mask`**:  Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\n",
    "\n",
    "* **`segment_ids`**:  Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\n",
    "\n",
    "And one(1) label is created from each raw review (`star_rating`)  :\n",
    "\n",
    "* **`label_id`**:  Label for each training row (`star_rating` 1 through 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate the BERT-specific Feature Engineering Step\n",
    "While we are demonstrating this code with a small amount of data here in the notebook, we will soon scale this to much more data on a powerful SageMaker cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Event time date feature type provided Integral. Event time type should be either Fractional(Unix timestamp in seconds) or String (ISO-8601 format) type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-23T17:08:18Z\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "#timestamp = datetime.now().replace(microsecond=0).isoformat()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "        [5, 'ABCD12345', \"\"\"I needed an \"antivirus\" application and know the quality of Norton products.  This was a no brainer for me and I am glad it was so simple to get.\"\"\"],\n",
    "        [3, 'EFGH12345', \"\"\"The problem with ElephantDrive is that it requires the use of Java. Since Java is notorious for security problems I haveit removed from all of my computers. What files I do have stored are photos.\"\"\"],\n",
    "        [1, 'IJKL2345', \"\"\"Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\"\"\"]\n",
    "       ]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['star_rating', 'review_id',  'review_body'])\n",
    "\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "inputs = df.apply(lambda x: Input(\n",
    "                                label = x[LABEL_COLUMN],\n",
    "                                text = x[REVIEW_BODY_COLUMN],\n",
    "                                review_id = x[REVIEW_ID_COLUMN],\n",
    "                                date = timestamp\n",
    "                            ),\n",
    "                  axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-23T17:08:18Z\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_embeddings = convert_features_to_tfrecord(train_inputs,\n",
    "#                                                    '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension),\n",
    "#                                                    max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file='./data-tfrecord-featurestore/data.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 0 of 3\n",
      "\n",
      "**tokens**\n",
      "['i', 'needed', 'an', '\"', 'anti', '##virus', '\"', 'application', 'and', 'know', 'the', 'quality', 'of', 'norton', 'products', '.', 'this', 'was', 'a', 'no', 'brain', '##er', 'for', 'me', 'and', 'i', 'am', 'glad', 'it', 'was', 'so', 'simple', 'to', 'get', '.']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1045, 2734, 2019, 1000, 3424, 23350, 1000, 4646, 1998, 2113, 1996, 3737, 1997, 10770, 3688, 1012, 2023, 2001, 1037, 2053, 4167, 2121, 2005, 2033, 1998, 1045, 2572, 5580, 2009, 2001, 2061, 3722, 2000, 2131, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "4\n",
      "\n",
      "**review_id**\n",
      "ABCD12345\n",
      "\n",
      "**date**\n",
      "2020-12-23T17:08:18Z\n",
      "\n",
      "**label**\n",
      "5\n",
      "\n",
      "**review_body**\n",
      "I needed an \"antivirus\" application and know the quality of Norton products.  This was a no brainer for me and I am glad it was so simple to get.\n",
      "\n",
      "**tokens**\n",
      "['the', 'problem', 'with', 'elephant', '##drive', 'is', 'that', 'it', 'requires', 'the', 'use', 'of', 'java', '.', 'since', 'java', 'is', 'notorious', 'for', 'security', 'problems', 'i', 'have', '##it', 'removed', 'from', 'all', 'of', 'my', 'computers', '.', 'what', 'files', 'i', 'do', 'have', 'stored', 'are', 'photos', '.']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1996, 3291, 2007, 10777, 23663, 2003, 2008, 2009, 5942, 1996, 2224, 1997, 9262, 1012, 2144, 9262, 2003, 12536, 2005, 3036, 3471, 1045, 2031, 4183, 3718, 2013, 2035, 1997, 2026, 7588, 1012, 2054, 6764, 1045, 2079, 2031, 8250, 2024, 7760, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "2\n",
      "\n",
      "**review_id**\n",
      "EFGH12345\n",
      "\n",
      "**date**\n",
      "2020-12-23T17:08:18Z\n",
      "\n",
      "**label**\n",
      "3\n",
      "\n",
      "**review_body**\n",
      "The problem with ElephantDrive is that it requires the use of Java. Since Java is notorious for security problems I haveit removed from all of my computers. What files I do have stored are photos.\n",
      "\n",
      "**tokens**\n",
      "['terrible', ',', 'none', 'of', 'my', 'codes', 'worked', ',', 'and', 'i', 'can', \"'\", 't', 'un', '##ins', '##tal', '##l', 'it', '.', 'i', 'think', 'this', 'product', 'is', 'mal', '##ware', 'and', 'viruses']\n",
      "\n",
      "**input_ids**\n",
      "[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499, 1010, 1998, 1045, 2064, 1005, 1056, 4895, 7076, 9080, 2140, 2009, 1012, 1045, 2228, 2023, 4031, 2003, 15451, 8059, 1998, 18191, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "0\n",
      "\n",
      "**review_id**\n",
      "IJKL2345\n",
      "\n",
      "**date**\n",
      "2020-12-23T17:08:18Z\n",
      "\n",
      "**label**\n",
      "1\n",
      "\n",
      "**review_body**\n",
      "Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 64\n",
    "records = transform_inputs_to_tfrecord(inputs, output_file, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three(3) features vectors and one(1) label are converted into a list of `TFRecord` instances (1 per each row of training data):\n",
    "* **`tf_records`**:  Binary representation of each row of training data (3 features + 1 label)\n",
    "\n",
    "These `TFRecord`s are the engineered features that we will use throughout the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**records**\n",
      "b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nu\\n\\tinput_ids\\x12h\\x1af\\nde\\x95\\x08\\xae\\x15\\xe3\\x0f\\xe8\\x07\\xe0\\x1a\\xb6\\xb6\\x01\\xe8\\x07\\xa6$\\xce\\x0f\\xc1\\x10\\xcc\\x0f\\x99\\x1d\\xcd\\x0f\\x92T\\xe8\\x1c\\xf4\\x07\\xe7\\x0f\\xd1\\x0f\\x8d\\x08\\x85\\x10\\xc7 \\xc9\\x10\\xd5\\x0f\\xf1\\x0f\\xce\\x0f\\x95\\x08\\x8c\\x14\\xcc+\\xd9\\x0f\\xd1\\x0f\\x8d\\x10\\x8a\\x1d\\xd0\\x0f\\xd3\\x10\\xf4\\x07f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nR\\n\\ninput_mask\\x12D\\x1aB\\n@\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
      "b'\\n\\xb9\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nz\\n\\tinput_ids\\x12m\\x1ak\\nie\\xcc\\x0f\\xdb\\x19\\xd7\\x0f\\x99T\\xef\\xb8\\x01\\xd3\\x0f\\xd8\\x0f\\xd9\\x0f\\xb6.\\xcc\\x0f\\xb0\\x11\\xcd\\x0f\\xaeH\\xf4\\x07\\xe0\\x10\\xaeH\\xd3\\x0f\\xf8a\\xd5\\x0f\\xdc\\x17\\x8f\\x1b\\x95\\x08\\xef\\x0f\\xd7 \\x86\\x1d\\xdd\\x0f\\xf3\\x0f\\xcd\\x0f\\xea\\x0f\\xa4;\\xf4\\x07\\x86\\x10\\xec4\\x95\\x08\\x9f\\x10\\xef\\x0f\\xba@\\xe8\\x0f\\xd0<\\xf4\\x07f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nR\\n\\ninput_mask\\x12D\\x1aB\\n@\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
      "b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nn\\n\\tinput_ids\\x12a\\x1a_\\n]e\\x834\\xf2\\x07\\xc0\\x1e\\xcd\\x0f\\xea\\x0f\\xc1J\\xc3\\x13\\xf2\\x07\\xce\\x0f\\x95\\x08\\x90\\x10\\xed\\x07\\xa0\\x08\\x9f&\\xa47\\xf8F\\xdc\\x10\\xd9\\x0f\\xf4\\x07\\x95\\x08\\xb4\\x11\\xe7\\x0f\\xbf\\x1f\\xd3\\x0f\\xdbx\\xfb>\\xce\\x0f\\x8f\\x8e\\x01f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nR\\n\\ninput_mask\\x12D\\x1aB\\n@\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x00'\n"
     ]
    }
   ],
   "source": [
    "print('**records**')\n",
    "\n",
    "for record in records:\n",
    "    print(record['tf_record'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nu\\n\\tinput_ids\\x12h\\x1af\\nde\\x95\\x08\\xae\\x15\\xe3\\x0f\\xe8\\x07\\xe0\\x1a\\xb6\\xb6\\x01\\xe8\\x07\\xa6$\\xce\\x0f\\xc1\\x10\\xcc\\x0f\\x99\\x1d\\xcd\\x0f\\x92T\\xe8\\x1c\\xf4\\x07\\xe7\\x0f\\xd1\\x0f\\x8d\\x08\\x85\\x10\\xc7 \\xc9\\x10\\xd5\\x0f\\xf1\\x0f\\xce\\x0f\\x95\\x08\\x8c\\x14\\xcc+\\xd9\\x0f\\xd1\\x0f\\x8d\\x10\\x8a\\x1d\\xd0\\x0f\\xd3\\x10\\xf4\\x07f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nR\\n\\ninput_mask\\x12D\\x1aB\\n@\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "my_tf_record = records[0]['tf_record']\n",
    "print(my_tf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.core.example.example_pb2.Example'>\n",
      "features {\n",
      "  feature {\n",
      "    key: \"input_ids\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 101\n",
      "        value: 1045\n",
      "        value: 2734\n",
      "        value: 2019\n",
      "        value: 1000\n",
      "        value: 3424\n",
      "        value: 23350\n",
      "        value: 1000\n",
      "        value: 4646\n",
      "        value: 1998\n",
      "        value: 2113\n",
      "        value: 1996\n",
      "        value: 3737\n",
      "        value: 1997\n",
      "        value: 10770\n",
      "        value: 3688\n",
      "        value: 1012\n",
      "        value: 2023\n",
      "        value: 2001\n",
      "        value: 1037\n",
      "        value: 2053\n",
      "        value: 4167\n",
      "        value: 2121\n",
      "        value: 2005\n",
      "        value: 2033\n",
      "        value: 1998\n",
      "        value: 1045\n",
      "        value: 2572\n",
      "        value: 5580\n",
      "        value: 2009\n",
      "        value: 2001\n",
      "        value: 2061\n",
      "        value: 3722\n",
      "        value: 2000\n",
      "        value: 2131\n",
      "        value: 1012\n",
      "        value: 102\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"input_mask\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 1\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label_ids\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"segment_ids\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "        value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deserialized_tf_record_example = tf.train.Example.FromString(my_tf_record)\n",
    "print(type(deserialized_tf_record_example))\n",
    "print(deserialized_tf_record_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.io.parse_single_example(deserialized_tf_record_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.data.Dataset.from_tensor_slices([deserialized_tf_record_example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add BERT Embeddings to Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto3.Session().client(service_name='sagemaker-featurestore-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define FeatureGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-23-17-11-04\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "reviews_feature_group_name = 'reviews-feature-group-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "print(reviews_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-23-17-11-04', sagemaker_session=<sagemaker.session.Session object at 0x7f5bcdc87fd0>, feature_definitions=[])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "reviews_feature_group = FeatureGroup(name=reviews_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "print(reviews_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"review_id\"\n",
    "event_time_feature_name = \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_record</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...</td>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>2020-12-23T17:08:18Z</td>\n",
       "      <td>5</td>\n",
       "      <td>I needed an \"antivirus\" application and know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\n\\xb9\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...</td>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>2020-12-23T17:08:18Z</td>\n",
       "      <td>3</td>\n",
       "      <td>The problem with ElephantDrive is that it requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\...</td>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>2020-12-23T17:08:18Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible, none of my codes worked, and I can't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tf_record  \\\n",
       "0  b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...   \n",
       "1  b'\\n\\xb9\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...   \n",
       "2  b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label  \\\n",
       "0  2020-12-23T17:08:18Z      5   \n",
       "1  2020-12-23T17:08:18Z      3   \n",
       "2  2020-12-23T17:08:18Z      1   \n",
       "\n",
       "                                         review_body  \n",
       "0  I needed an \"antivirus\" application and know t...  \n",
       "1  The problem with ElephantDrive is that it requ...  \n",
       "2  Terrible, none of my codes worked, and I can't...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records = pd.DataFrame.from_dict(records)\n",
    "df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_store = df_records.rename(columns={\"label\": \"star_rating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_store[['review_id', 'date', 'review_body', 'star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast to Supported Feature Store Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == 'object':\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_object_to_string(df_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_record</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...</td>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>2020-12-23T17:08:18Z</td>\n",
       "      <td>5</td>\n",
       "      <td>I needed an \"antivirus\" application and know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\n\\xb9\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...</td>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>2020-12-23T17:08:18Z</td>\n",
       "      <td>3</td>\n",
       "      <td>The problem with ElephantDrive is that it requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\...</td>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>2020-12-23T17:08:18Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible, none of my codes worked, and I can't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tf_record  \\\n",
       "0  b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...   \n",
       "1  b'\\n\\xb9\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...   \n",
       "2  b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label  \\\n",
       "0  2020-12-23T17:08:18Z      5   \n",
       "1  2020-12-23T17:08:18Z      3   \n",
       "2  2020-12-23T17:08:18Z      1   \n",
       "\n",
       "                                         review_body  \n",
       "0  I needed an \"antivirus\" application and know t...  \n",
       "1  The problem with ElephantDrive is that it requ...  \n",
       "2  Terrible, none of my codes worked, and I can't...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='tf_record', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='input_mask', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='segment_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='review_body', feature_type=<FeatureTypeEnum.STRING: 'String'>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "reviews_feature_group.load_feature_definitions(data_frame=df_records) # output is suppressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-store-2020-12-23T17:08:18Z\n"
     ]
    }
   ],
   "source": [
    "prefix = 'reviews-feature-store-' + timestamp\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-23-17-11-04',\n",
       " 'ResponseMetadata': {'RequestId': 'bc864abf-64c6-4611-8fd5-b65855c5990a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bc864abf-64c6-4611-8fd5-b65855c5990a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '110',\n",
       "   'date': 'Wed, 23 Dec 2020 17:11:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-23-17-11-04',\n",
       " 'FeatureGroupName': 'reviews-feature-group-23-17-11-04',\n",
       " 'RecordIdentifierFeatureName': 'review_id',\n",
       " 'EventTimeFeatureName': 'date',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'tf_record', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'input_ids', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'input_mask', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'segment_ids', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'label_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'review_id', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'date', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'label', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'review_body', 'FeatureType': 'String'}],\n",
       " 'CreationTime': datetime.datetime(2020, 12, 23, 17, 11, 29, 282000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/reviews-feature-store-2020-12-23T17:08:18Z'},\n",
       "  'DisableGlueTableCreation': False},\n",
       " 'RoleArn': 'arn:aws:iam::231218423789:role/TeamRole',\n",
       " 'FeatureGroupStatus': 'Creating',\n",
       " 'ResponseMetadata': {'RequestId': '41052b1e-981b-4088-ba0c-561b8a294f83',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '41052b1e-981b-4088-ba0c-561b8a294f83',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1048',\n",
       "   'date': 'Wed, 23 Dec 2020 17:11:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupSummaries': [{'FeatureGroupName': 'reviews-feature-group-23-17-11-04',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-23-17-11-04',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 23, 17, 11, 29, 282000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Creating'}],\n",
       " 'ResponseMetadata': {'RequestId': '018b2946-277b-4321-9e18-64035bb47ce4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '018b2946-277b-4321-9e18-64035bb47ce4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '257',\n",
       "   'date': 'Wed, 23 Dec 2020 17:11:33 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.list_feature_groups() # use boto client to list FeatureGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PutRecords into FeatureGroup\n",
    "\n",
    "After the FeatureGroups have been created, we can put data into the FeatureGroups by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to S3 in chunks. The files will be written to the offline store within a few minutes of ingestion. For this example, to accelerate the ingestion process, we are specifying multiple workers to do the job simultaneously. It will take ~1min to ingest data to the 2 FeatureGroups, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group Creation\n",
      "FeatureGroup reviews-feature-group-23-17-11-04 successfully created.\n"
     ]
    }
   ],
   "source": [
    "wait_for_feature_group_creation_complete(feature_group=reviews_feature_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='reviews-feature-group-23-17-11-04', sagemaker_session=<sagemaker.session.Session object at 0x7f5bcdc87fd0>, data_frame=                                           tf_record  \\\n",
       "0  b'\\n\\xb4\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...   \n",
       "1  b'\\n\\xb9\\x02\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x0...   \n",
       "2  b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label  \\\n",
       "0  2020-12-23T17:08:18Z      5   \n",
       "1  2020-12-23T17:08:18Z      3   \n",
       "2  2020-12-23T17:08:18Z      1   \n",
       "\n",
       "                                         review_body  \n",
       "0  I needed an \"antivirus\" application and know t...  \n",
       "1  The problem with ElephantDrive is that it requ...  \n",
       "2  Terrible, none of my codes worked, and I can't...  , max_workers=3, _futures={<Future at 0x7f5b505d8d68 state=finished returned NoneType>: (0, 1), <Future at 0x7f5b306aff28 state=finished returned NoneType>: (1, 2), <Future at 0x7f5b306aff98 state=finished returned NoneType>: (2, 3)})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_group.ingest(\n",
    "    data_frame=df_records, max_workers=3, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '92c99498-e469-4e9d-88eb-1fcd326fba8d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '92c99498-e469-4e9d-88eb-1fcd326fba8d',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '2482',\n",
       "   'date': 'Wed, 23 Dec 2020 17:12:11 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Record': [{'FeatureName': 'tf_record',\n",
       "   'ValueAsString': \"b'\\\\n\\\\xad\\\\x02\\\\nS\\\\n\\\\x0bsegment_ids\\\\x12D\\\\x1aB\\\\n@\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\nn\\\\n\\\\tinput_ids\\\\x12a\\\\x1a_\\\\n]e\\\\x834\\\\xf2\\\\x07\\\\xc0\\\\x1e\\\\xcd\\\\x0f\\\\xea\\\\x0f\\\\xc1J\\\\xc3\\\\x13\\\\xf2\\\\x07\\\\xce\\\\x0f\\\\x95\\\\x08\\\\x90\\\\x10\\\\xed\\\\x07\\\\xa0\\\\x08\\\\x9f&\\\\xa47\\\\xf8F\\\\xdc\\\\x10\\\\xd9\\\\x0f\\\\xf4\\\\x07\\\\x95\\\\x08\\\\xb4\\\\x11\\\\xe7\\\\x0f\\\\xbf\\\\x1f\\\\xd3\\\\x0f\\\\xdbx\\\\xfb>\\\\xce\\\\x0f\\\\x8f\\\\x8e\\\\x01f\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\nR\\\\n\\\\ninput_mask\\\\x12D\\\\x1aB\\\\n@\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x01\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\n\\\\x12\\\\n\\\\tlabel_ids\\\\x12\\\\x05\\\\x1a\\\\x03\\\\n\\\\x01\\\\x00'\"},\n",
       "  {'FeatureName': 'input_ids',\n",
       "   'ValueAsString': '[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499, 1010, 1998, 1045, 2064, 1005, 1056, 4895, 7076, 9080, 2140, 2009, 1012, 1045, 2228, 2023, 4031, 2003, 15451, 8059, 1998, 18191, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'},\n",
       "  {'FeatureName': 'input_mask',\n",
       "   'ValueAsString': '[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'},\n",
       "  {'FeatureName': 'segment_ids',\n",
       "   'ValueAsString': '[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'},\n",
       "  {'FeatureName': 'label_id', 'ValueAsString': '0'},\n",
       "  {'FeatureName': 'review_id', 'ValueAsString': 'IJKL2345'},\n",
       "  {'FeatureName': 'date', 'ValueAsString': '2020-12-23T17:08:18Z'},\n",
       "  {'FeatureName': 'label', 'ValueAsString': '1'},\n",
       "  {'FeatureName': 'review_body',\n",
       "   'ValueAsString': \"Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\"}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_identifier_value = 'IJKL2345'\n",
    "\n",
    "featurestore_runtime.get_record(FeatureGroupName=reviews_feature_group_name, RecordIdentifierValueAsString=record_identifier_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTERNAL TABLE IF NOT EXISTS sagemaker_featurestore.reviews-feature-group-23-17-11-04 (\n",
      "  tf_record STRING\n",
      "  input_ids STRING\n",
      "  input_mask STRING\n",
      "  segment_ids STRING\n",
      "  label_id INT\n",
      "  review_id STRING\n",
      "  date STRING\n",
      "  label INT\n",
      "  review_body STRING\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "  STORED AS\n",
      "  INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'\n",
      "  OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'\n",
      "LOCATION 's3://sagemaker-us-east-1-231218423789/reviews-feature-store-2020-12-23T17:08:18Z/231218423789/sagemaker/us-east-1/offline-store/reviews-feature-group-23-17-11-04'\n"
     ]
    }
   ],
   "source": [
    "print(reviews_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-23-17-11-04\n"
     ]
    }
   ],
   "source": [
    "print(reviews_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Data available.\n"
     ]
    }
   ],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "reviews_feature_group_s3_prefix = prefix + '/' + account_id + '/sagemaker/' + region + '/offline-store/' + reviews_feature_group_name + '/data'\n",
    "\n",
    "offline_store_contents = None\n",
    "while (offline_store_contents is None):\n",
    "    objects_in_bucket = s3.list_objects(Bucket=bucket,\n",
    "                                        Prefix=prefix)\n",
    "    if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "        offline_store_contents = objects_in_bucket['Contents']\n",
    "    else:\n",
    "        print('Waiting for data in offline store...\\n')\n",
    "        sleep(60)\n",
    "    \n",
    "print('Data available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (you can optionally turn it on/off while creating the FeatureGroup). In this example, we want to create one training dataset with FeatureValues from both identity and transaction FeatureGroups. This is done by utilizing the auto-built Catalog. We run an Athena query that joins the data stored in the offline store in S3 from the 2 FeatureGroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SELECT tf_record FROM \"reviews-feature-group-23-17-11-04-1608743489\" LIMIT 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tf_record\n",
       "0  b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_query = reviews_feature_group.athena_query()\n",
    "\n",
    "reviews_table = reviews_query.table_name\n",
    "\n",
    "#query_string = 'SELECT input_ids, input_mask, segment_ids, label_id FROM \"'+reviews_table+'\" LIMIT 1'\n",
    "query_string = 'SELECT tf_record FROM \"'+reviews_table+'\" LIMIT 1'\n",
    "\n",
    "print('Running ' + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "dataset = pd.DataFrame()\n",
    "reviews_query.run(query_string=query_string, output_location='s3://'+bucket+'/'+prefix+'/query_results/')\n",
    "reviews_query.wait()\n",
    "dataset = reviews_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare query results for training.\n",
    "# query_execution = reviews_query.get_query_execution()\n",
    "# query_result = 's3://'+bucket+'/'+prefix+'/query_results/'+query_execution['QueryExecution']['QueryExecutionId']+'.csv'\n",
    "# print(query_result)\n",
    "\n",
    "# # Select useful columns for training with target column as the first.\n",
    "# dataset = dataset[[\"embedding\"]]\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data-tfrecord-featurestore/reviews-embeddings.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_tfrecord_dataset = tf.data.TFRecordDataset(file_name)\n",
    "restored_tfrecord_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x7f5af47f65f8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_tfrecord_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataLossError",
     "evalue": "corrupted record at 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2101\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2102\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2103\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2610\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: corrupted record at 0 [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-67741c52f0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tfrecord_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3783\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2103\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: corrupted record at 0"
     ]
    }
   ],
   "source": [
    "list(restored_tfrecord_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xad\\x02\\nS\\n\\x0bsegment_ids\\x12D\\x1aB\\n@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nn\\n\\tinput_ids\\x12a\\x1a_\\n]e\\x834\\xf2\\x07\\xc0\\x1e\\xcd\\x0f\\xea\\x0f\\xc1J\\xc3\\x13\\xf2\\x07\\xce\\x0f\\x95\\x08\\x90\\x10\\xed\\x07\\xa0\\x08\\x9f&\\xa47\\xf8F\\xdc\\x10\\xd9\\x0f\\xf4\\x07\\x95\\x08\\xb4\\x11\\xe7\\x0f\\xbf\\x1f\\xd3\\x0f\\xdbx\\xfb>\\xce\\x0f\\x8f\\x8e\\x01f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\nR\\n\\ninput_mask\\x12D\\x1aB\\n@\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x00'\n"
     ]
    }
   ],
   "source": [
    "!head $file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
