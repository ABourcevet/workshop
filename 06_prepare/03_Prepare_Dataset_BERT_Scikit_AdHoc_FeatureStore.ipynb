{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Features into a SageMaker Feature Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "s3 = boto3.Session().client(service_name='s3', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas                             1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Raw Text to BERT Features using Hugging Face and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "REVIEW_BODY_COLUMN = 'review_body'\n",
    "REVIEW_ID_COLUMN = 'review_id'\n",
    "# DATE_COLUMN = 'date'\n",
    "\n",
    "LABEL_COLUMN = 'star_rating'\n",
    "LABEL_VALUES = [1, 2, 3, 4, 5]\n",
    "\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "    \n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               review_id,\n",
    "               date,\n",
    "               label,\n",
    "               review_body):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n",
    "        self.review_body = review_body\n",
    "\n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, review_id, date, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.review_id = review_id\n",
    "    self.date = date\n",
    "    self.label = label\n",
    "    \n",
    "\n",
    "def convert_input(the_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "\n",
    "    tokens = tokenizer.tokenize(the_input.text)\n",
    "    print('**tokens**\\n{}\\n'.format(tokens))\n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(the_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length,\n",
    "#                                               truncation=True\n",
    "                                              )\n",
    "\n",
    "    # The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    \n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "\n",
    "    # Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for each training row (`star_rating` 1 through 5)\n",
    "    label_id = label_map[the_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        review_id=the_input.review_id,\n",
    "        date=the_input.date,\n",
    "        label=the_input.label,\n",
    "        review_body=the_input.text)\n",
    "\n",
    "    print('**input_ids**\\n{}\\n'.format(features.input_ids))\n",
    "    print('**input_mask**\\n{}\\n'.format(features.input_mask))\n",
    "    print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\n",
    "    print('**label_id**\\n{}\\n'.format(features.label_id))\n",
    "    print('**review_id**\\n{}\\n'.format(features.review_id))\n",
    "    print('**date**\\n{}\\n'.format(features.date))\n",
    "    print('**label**\\n{}\\n'.format(features.label))\n",
    "    print('**review_body**\\n{}\\n'.format(features.review_body))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# We'll need to transform our data into a format that BERT understands.\n",
    "# - `text` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "# - `label` is the star_rating label (1, 2, 3, 4, 5) for our training input data\n",
    "def transform_inputs_to_tfrecord(inputs, output_file, max_seq_length):\n",
    "    records = []\n",
    "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, the_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print('Writing input {} of {}\\n'.format(input_idx, len(inputs)))\n",
    "\n",
    "        features = convert_input(the_input, max_seq_length)\n",
    "\n",
    "        all_features = collections.OrderedDict()\n",
    "        all_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
    "        all_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
    "        all_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
    "        all_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
    "\n",
    "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
    "        tf_record_writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        records.append({#'tf_record': tf_record.SerializeToString(),\n",
    "                        'input_ids': features.input_ids,\n",
    "                        'input_mask': features.input_mask,\n",
    "                        'segment_ids': features.segment_ids,\n",
    "                        'label_id': features.label_id,\n",
    "                        'review_id': the_input.review_id,\n",
    "                        'date': the_input.date,\n",
    "                        'label': features.label,\n",
    "                        'review_body': features.review_body\n",
    "                       })\n",
    "        \n",
    "    tf_record_writer.close()\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three(3) feature vectors are created from each raw review (`review_body`) during the feature engineering phase to prepare for BERT processing:\n",
    "\n",
    "* **`input_ids`**:  The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\n",
    "    \n",
    "* **`input_mask`**:  Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\n",
    "\n",
    "* **`segment_ids`**:  Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\n",
    "\n",
    "And one(1) label is created from each raw review (`star_rating`)  :\n",
    "\n",
    "* **`label_id`**:  Label for each training row (`star_rating` 1 through 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate the BERT-specific Feature Engineering Step\n",
    "While we are demonstrating this code with a small amount of data here in the notebook, we will soon scale this to much more data on a powerful SageMaker cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Event time date feature type provided Integral. Event time type should be either Fractional(Unix timestamp in seconds) or String (ISO-8601 format) type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27T01:22:25Z\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "#timestamp = datetime.now().replace(microsecond=0).isoformat()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "        [5, 'ABCD12345', \"\"\"I needed an \"antivirus\" application and know the quality of Norton products.  This was a no brainer for me and I am glad it was so simple to get.\"\"\"],\n",
    "        [3, 'EFGH12345', \"\"\"The problem with ElephantDrive is that it requires the use of Java. Since Java is notorious for security problems I haveit removed from all of my computers. What files I do have stored are photos.\"\"\"],\n",
    "        [1, 'IJKL2345', \"\"\"Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\"\"\"]\n",
    "       ]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['star_rating', 'review_id',  'review_body'])\n",
    "\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "inputs = df.apply(lambda x: Input(\n",
    "                                label = x[LABEL_COLUMN],\n",
    "                                text = x[REVIEW_BODY_COLUMN],\n",
    "                                review_id = x[REVIEW_ID_COLUMN],\n",
    "                                date = timestamp\n",
    "                            ),\n",
    "                  axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27T01:22:25Z\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_embeddings = convert_features_to_tfrecord(train_inputs,\n",
    "#                                                    '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension),\n",
    "#                                                    max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file='./data-tfrecord-featurestore/data.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 0 of 3\n",
      "\n",
      "**tokens**\n",
      "['i', 'needed', 'an', '\"', 'anti', '##virus', '\"', 'application', 'and', 'know', 'the', 'quality', 'of', 'norton', 'products', '.', 'this', 'was', 'a', 'no', 'brain', '##er', 'for', 'me', 'and', 'i', 'am', 'glad', 'it', 'was', 'so', 'simple', 'to', 'get', '.']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1045, 2734, 2019, 1000, 3424, 23350, 1000, 4646, 1998, 2113, 1996, 3737, 1997, 10770, 3688, 1012, 2023, 2001, 1037, 2053, 4167, 2121, 2005, 2033, 1998, 1045, 2572, 5580, 2009, 2001, 2061, 3722, 2000, 2131, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "4\n",
      "\n",
      "**review_id**\n",
      "ABCD12345\n",
      "\n",
      "**date**\n",
      "2020-12-27T01:22:25Z\n",
      "\n",
      "**label**\n",
      "5\n",
      "\n",
      "**review_body**\n",
      "I needed an \"antivirus\" application and know the quality of Norton products.  This was a no brainer for me and I am glad it was so simple to get.\n",
      "\n",
      "**tokens**\n",
      "['the', 'problem', 'with', 'elephant', '##drive', 'is', 'that', 'it', 'requires', 'the', 'use', 'of', 'java', '.', 'since', 'java', 'is', 'notorious', 'for', 'security', 'problems', 'i', 'have', '##it', 'removed', 'from', 'all', 'of', 'my', 'computers', '.', 'what', 'files', 'i', 'do', 'have', 'stored', 'are', 'photos', '.']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1996, 3291, 2007, 10777, 23663, 2003, 2008, 2009, 5942, 1996, 2224, 1997, 9262, 1012, 2144, 9262, 2003, 12536, 2005, 3036, 3471, 1045, 2031, 4183, 3718, 2013, 2035, 1997, 2026, 7588, 1012, 2054, 6764, 1045, 2079, 2031, 8250, 2024, 7760, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "2\n",
      "\n",
      "**review_id**\n",
      "EFGH12345\n",
      "\n",
      "**date**\n",
      "2020-12-27T01:22:25Z\n",
      "\n",
      "**label**\n",
      "3\n",
      "\n",
      "**review_body**\n",
      "The problem with ElephantDrive is that it requires the use of Java. Since Java is notorious for security problems I haveit removed from all of my computers. What files I do have stored are photos.\n",
      "\n",
      "**tokens**\n",
      "['terrible', ',', 'none', 'of', 'my', 'codes', 'worked', ',', 'and', 'i', 'can', \"'\", 't', 'un', '##ins', '##tal', '##l', 'it', '.', 'i', 'think', 'this', 'product', 'is', 'mal', '##ware', 'and', 'viruses']\n",
      "\n",
      "**input_ids**\n",
      "[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499, 1010, 1998, 1045, 2064, 1005, 1056, 4895, 7076, 9080, 2140, 2009, 1012, 1045, 2228, 2023, 4031, 2003, 15451, 8059, 1998, 18191, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "0\n",
      "\n",
      "**review_id**\n",
      "IJKL2345\n",
      "\n",
      "**date**\n",
      "2020-12-27T01:22:25Z\n",
      "\n",
      "**label**\n",
      "1\n",
      "\n",
      "**review_body**\n",
      "Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 64\n",
    "records = transform_inputs_to_tfrecord(inputs, output_file, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three(3) features vectors and one(1) label are converted into a list of `TFRecord` instances (1 per each row of training data):\n",
    "* **`tf_records`**:  Binary representation of each row of training data (3 features + 1 label)\n",
    "\n",
    "These `TFRecord`s are the engineered features that we will use throughout the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('**records**')\n",
    "\n",
    "# for record in records:\n",
    "#     print(record['tf_record'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_tf_record = records[0]['tf_record']\n",
    "# print(my_tf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deserialized_tf_record_example = tf.train.Example.FromString(my_tf_record)\n",
    "# print(type(deserialized_tf_record_example))\n",
    "# print(deserialized_tf_record_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'input_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.io.parse_single_example(deserialized_tf_record_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.data.Dataset.from_tensor_slices([deserialized_tf_record_example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add BERT Embeddings to Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto3.Session().client(service_name='sagemaker-featurestore-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define FeatureGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-27-01-23-16\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "reviews_feature_group_name = 'reviews-feature-group-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "print(reviews_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-27-01-23-16', sagemaker_session=<sagemaker.session.Session object at 0x7fd5b2a3da20>, feature_definitions=[])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "reviews_feature_group = FeatureGroup(name=reviews_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "print(reviews_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"review_id\"\n",
    "event_time_feature_name = \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>2020-12-27T01:22:25Z</td>\n",
       "      <td>5</td>\n",
       "      <td>I needed an \"antivirus\" application and know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>2020-12-27T01:22:25Z</td>\n",
       "      <td>3</td>\n",
       "      <td>The problem with ElephantDrive is that it requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>2020-12-27T01:22:25Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible, none of my codes worked, and I can't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label  \\\n",
       "0  2020-12-27T01:22:25Z      5   \n",
       "1  2020-12-27T01:22:25Z      3   \n",
       "2  2020-12-27T01:22:25Z      1   \n",
       "\n",
       "                                         review_body  \n",
       "0  I needed an \"antivirus\" application and know t...  \n",
       "1  The problem with ElephantDrive is that it requ...  \n",
       "2  Terrible, none of my codes worked, and I can't...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records = pd.DataFrame.from_dict(records)\n",
    "df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_store = df_records.rename(columns={\"label\": \"star_rating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature_store[['review_id', 'date', 'review_body', 'star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast to Supported Feature Store Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == 'object':\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_object_to_string(df_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>2020-12-27T01:22:25Z</td>\n",
       "      <td>5</td>\n",
       "      <td>I needed an \"antivirus\" application and know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>2020-12-27T01:22:25Z</td>\n",
       "      <td>3</td>\n",
       "      <td>The problem with ElephantDrive is that it requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>2020-12-27T01:22:25Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible, none of my codes worked, and I can't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label  \\\n",
       "0  2020-12-27T01:22:25Z      5   \n",
       "1  2020-12-27T01:22:25Z      3   \n",
       "2  2020-12-27T01:22:25Z      1   \n",
       "\n",
       "                                         review_body  \n",
       "0  I needed an \"antivirus\" application and know t...  \n",
       "1  The problem with ElephantDrive is that it requ...  \n",
       "2  Terrible, none of my codes worked, and I can't...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='input_mask', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='segment_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='review_body', feature_type=<FeatureTypeEnum.STRING: 'String'>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "reviews_feature_group.load_feature_definitions(data_frame=df_records) # output is suppressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-store-2020-12-27T01:22:25Z\n"
     ]
    }
   ],
   "source": [
    "prefix = 'reviews-feature-store-' + timestamp\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-27-01-23-16',\n",
       " 'ResponseMetadata': {'RequestId': '30fc21b1-9d35-4afc-bb41-a83fb40f45ce',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '30fc21b1-9d35-4afc-bb41-a83fb40f45ce',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '110',\n",
       "   'date': 'Sun, 27 Dec 2020 01:23:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-27-01-23-16',\n",
       " 'FeatureGroupName': 'reviews-feature-group-27-01-23-16',\n",
       " 'RecordIdentifierFeatureName': 'review_id',\n",
       " 'EventTimeFeatureName': 'date',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'input_ids', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'input_mask', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'segment_ids', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'label_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'review_id', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'date', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'label', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'review_body', 'FeatureType': 'String'}],\n",
       " 'CreationTime': datetime.datetime(2020, 12, 27, 1, 23, 16, 397000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/reviews-feature-store-2020-12-27T01:22:25Z'},\n",
       "  'DisableGlueTableCreation': False},\n",
       " 'RoleArn': 'arn:aws:iam::231218423789:role/TeamRole',\n",
       " 'FeatureGroupStatus': 'Creating',\n",
       " 'ResponseMetadata': {'RequestId': '2a910cb5-c8ec-4324-8934-a52995ea5013',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2a910cb5-c8ec-4324-8934-a52995ea5013',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '997',\n",
       "   'date': 'Sun, 27 Dec 2020 01:23:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupSummaries': [{'FeatureGroupName': 'reviews-feature-group-27-01-23-16',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-27-01-23-16',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 27, 1, 23, 16, 397000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Creating'},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-26-02-37-47',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-26-02-37-47',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 26, 2, 37, 47, 501000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-47-38',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-47-38',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 47, 41, 815000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-47-36',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-47-36',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 47, 38, 176000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-36-09',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-36-09',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 38, 10, 596000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-36-08',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-36-08',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 37, 47, 401000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-31-59',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-31-59',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 32, 1, 852000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-23-57',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-23-57',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 25, 37, 417000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-23-55',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-23-55',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 25, 58, 145000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'reviews-feature-group-25-00-09-53',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:231218423789:feature-group/reviews-feature-group-25-00-09-53',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 25, 0, 9, 55, 556000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}}],\n",
       " 'NextToken': 'cIws2QhTXUIa8bi8X9H6fuXNjs2a/qx+SXiisTJIESEmS230UH3Epud59scY4BOFtLn0ScyuwIxwVScdBpUEBQNz+0eY+RCRO/FXm0191acLze5Dz7ivH7FsZeyCvPrfU7glbXC8oaFXMoJh5QsrnY+OOGuVwe999xWtvEPFzR4V7htt5SasY8p97itsOucD7A+do/h5jfB1UtrhCAWqPbcsxWjnMmBd52TjoMK+RrNJZVEavgcBSBLNWFIB7IZqwIaAnJui8CN3izEoum5ziTldt+P1qHCFdGsjAT/i5giZZxg3tAqgORw+eTBV0sp/dLNLSP7RJu2VL953s3bEnUZMHRbZ+tLSoqO32ZwY77tMe1DN6ppGAvlTDUkzpAIMrcdVduTy8WE5fUw0QCIkUeuu038QfH41dNvNDzv/v/F5nwuKKmiK1tZfMWe6992z5zv2HxNws/0Jdkoh+tWje2OcDbCY09L0EpVbQMqsjqRlvuNRtBsWfc4quo5I1op5rGDK2A9f3Mq8aHDWFlcrCpmra43QuuRx5tR39kKXzHrtcjnwCz+8gT+vOg917PnQ0Pe8',\n",
       " 'ResponseMetadata': {'RequestId': 'b27a9104-61c8-44d5-8a6d-44d37cdd4b08',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b27a9104-61c8-44d5-8a6d-44d37cdd4b08',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3282',\n",
       "   'date': 'Sun, 27 Dec 2020 01:23:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.list_feature_groups() # use boto client to list FeatureGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PutRecords into FeatureGroup\n",
    "\n",
    "After the FeatureGroups have been created, we can put data into the FeatureGroups by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to S3 in chunks. The files will be written to the offline store within a few minutes of ingestion. For this example, to accelerate the ingestion process, we are specifying multiple workers to do the job simultaneously. It will take ~1min to ingest data to the 2 FeatureGroups, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup reviews-feature-group-27-01-23-16 successfully created.\n"
     ]
    }
   ],
   "source": [
    "wait_for_feature_group_creation_complete(feature_group=reviews_feature_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='reviews-feature-group-27-01-23-16', sagemaker_session=<sagemaker.session.Session object at 0x7fd5b2a3da20>, data_frame=                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label  \\\n",
       "0  2020-12-27T01:22:25Z      5   \n",
       "1  2020-12-27T01:22:25Z      3   \n",
       "2  2020-12-27T01:22:25Z      1   \n",
       "\n",
       "                                         review_body  \n",
       "0  I needed an \"antivirus\" application and know t...  \n",
       "1  The problem with ElephantDrive is that it requ...  \n",
       "2  Terrible, none of my codes worked, and I can't...  , max_workers=3, _futures={<Future at 0x7fd51323e518 state=finished returned NoneType>: (0, 1), <Future at 0x7fd51324f208 state=finished returned NoneType>: (1, 2), <Future at 0x7fd51324fc18 state=finished returned NoneType>: (2, 3)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_group.ingest(\n",
    "    data_frame=df_records, max_workers=3, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c3bf18e4-4951-4d53-ab1b-2d92f75edad5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c3bf18e4-4951-4d53-ab1b-2d92f75edad5',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '1184',\n",
       "   'date': 'Sun, 27 Dec 2020 01:23:27 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Record': [{'FeatureName': 'input_ids',\n",
       "   'ValueAsString': '[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499, 1010, 1998, 1045, 2064, 1005, 1056, 4895, 7076, 9080, 2140, 2009, 1012, 1045, 2228, 2023, 4031, 2003, 15451, 8059, 1998, 18191, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'},\n",
       "  {'FeatureName': 'input_mask',\n",
       "   'ValueAsString': '[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'},\n",
       "  {'FeatureName': 'segment_ids',\n",
       "   'ValueAsString': '[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]'},\n",
       "  {'FeatureName': 'label_id', 'ValueAsString': '0'},\n",
       "  {'FeatureName': 'review_id', 'ValueAsString': 'IJKL2345'},\n",
       "  {'FeatureName': 'date', 'ValueAsString': '2020-12-27T01:22:25Z'},\n",
       "  {'FeatureName': 'label', 'ValueAsString': '1'},\n",
       "  {'FeatureName': 'review_body',\n",
       "   'ValueAsString': \"Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\"}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_identifier_value = 'IJKL2345'\n",
    "\n",
    "featurestore_runtime.get_record(FeatureGroupName=reviews_feature_group_name, RecordIdentifierValueAsString=record_identifier_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE EXTERNAL TABLE IF NOT EXISTS sagemaker_featurestore.reviews-feature-group-27-01-23-16 (\n",
      "  input_ids STRING\n",
      "  input_mask STRING\n",
      "  segment_ids STRING\n",
      "  label_id INT\n",
      "  review_id STRING\n",
      "  date STRING\n",
      "  label INT\n",
      "  review_body STRING\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "  STORED AS\n",
      "  INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'\n",
      "  OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'\n",
      "LOCATION 's3://sagemaker-us-east-1-231218423789/reviews-feature-store-2020-12-27T01:22:25Z/231218423789/sagemaker/us-east-1/offline-store/reviews-feature-group-27-01-23-16'\n"
     ]
    }
   ],
   "source": [
    "print(reviews_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-27-01-23-16\n"
     ]
    }
   ],
   "source": [
    "print(reviews_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Waiting for data in offline store...\n",
      "\n",
      "Data available.\n"
     ]
    }
   ],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "reviews_feature_group_s3_prefix = prefix + '/' + account_id + '/sagemaker/' + region + '/offline-store/' + reviews_feature_group_name + '/data'\n",
    "\n",
    "offline_store_contents = None\n",
    "while (offline_store_contents is None):\n",
    "    objects_in_bucket = s3.list_objects(Bucket=bucket,\n",
    "                                        Prefix=prefix)\n",
    "    if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "        offline_store_contents = objects_in_bucket['Contents']\n",
    "    else:\n",
    "        print('Waiting for data in offline store...\\n')\n",
    "        sleep(60)\n",
    "    \n",
    "print('Data available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (you can optionally turn it on/off while creating the FeatureGroup). In this example, we want to create one training dataset with FeatureValues from both identity and transaction FeatureGroups. This is done by utilizing the auto-built Catalog. We run an Athena query that joins the data stored in the offline store in S3 from the 2 FeatureGroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SELECT review_body, input_ids, input_mask, segment_ids, label_id FROM \"reviews-feature-group-27-01-23-16-1609032196\" LIMIT 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The problem with ElephantDrive is that it requ...</td>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible, none of my codes worked, and I can't...</td>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I needed an \"antivirus\" application and know t...</td>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  \\\n",
       "0  The problem with ElephantDrive is that it requ...   \n",
       "1  Terrible, none of my codes worked, and I can't...   \n",
       "2  I needed an \"antivirus\" application and know t...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "1  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "2  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_feature_store_query = reviews_feature_group.athena_query()\n",
    "\n",
    "reviews_feature_store_table = reviews_feature_store_query.table_name\n",
    "\n",
    "#query_string = 'SELECT review_body, input_ids, input_mask, segment_ids, label_id FROM \"'+reviews_table+'\" LIMIT 5'\n",
    "\n",
    "query_string = 'SELECT review_body, input_ids, input_mask, segment_ids, label_id FROM \"{}\" LIMIT 5'.format(reviews_feature_store_table)\n",
    "\n",
    "print('Running ' + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "\n",
    "reviews_feature_store_query.run(query_string=query_string, output_location='s3://'+bucket+'/'+prefix+'/query_results/')\n",
    "\n",
    "#reviews_query.run(query_string=query_string, output_location='s3://'+bucket+'/'+prefix+'/query_results/')\n",
    "\n",
    "reviews_feature_store_query.wait()\n",
    "dataset = reviews_feature_store_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare query results for training.\n",
    "# query_execution = reviews_query.get_query_execution()\n",
    "# query_result = 's3://'+bucket+'/'+prefix+'/query_results/'+query_execution['QueryExecution']['QueryExecutionId']+'.csv'\n",
    "# print(query_result)\n",
    "\n",
    "# # Select useful columns for training with target column as the first.\n",
    "# dataset = dataset[[\"embedding\"]]\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data-tfrecord-featurestore/reviews-embeddings.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_tfrecord_dataset = tf.data.TFRecordDataset(file_name)\n",
    "restored_tfrecord_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x7fd511711198>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_tfrecord_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(restored_tfrecord_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head $file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
